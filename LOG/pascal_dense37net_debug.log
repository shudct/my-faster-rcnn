Called with args:
Namespace(cfg_file=None, gpu_id=1, imdb_name='voc_2007_trainval', net_name='VGG16', pretrained_model=None, set_cfgs=None)
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0211 13:04:07.022014 15724 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/pascal_voc/VGG16/dense37net/train_densenet.prototxt"
base_lr: 0.001
display: 20
max_iter: 240000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 50000
snapshot_prefix: "pascal_densenet-37L-lr1e-3"
solver_mode: GPU
random_seed: 831486
stepvalue: 120000
stepvalue: 180000
type: "Nesterov"
I0211 13:04:07.022161 15724 solver.cpp:81] Creating training net from train_net file: models/pascal_voc/VGG16/dense37net/train_densenet.prototxt
I0211 13:04:07.026679 15724 net.cpp:49] Initializing net from parameters: 
name: "Densenet_37_layers"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 21"
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "BatchNorm1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Convolution1"
  bottom: "Dropout1"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Concat1"
  top: "BatchNorm2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Concat1"
  bottom: "Dropout2"
  top: "Concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Concat2"
  top: "BatchNorm3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "BatchNorm3"
  top: "Convolution4"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution4"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Concat2"
  bottom: "Dropout3"
  top: "Concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Concat3"
  top: "BatchNorm4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "BatchNorm4"
  top: "Convolution5"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution5"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat4"
  type: "Concat"
  bottom: "Concat3"
  bottom: "Dropout4"
  top: "Concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Concat4"
  top: "BatchNorm5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "BatchNorm5"
  top: "Convolution6"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution6"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat5"
  type: "Concat"
  bottom: "Concat4"
  bottom: "Dropout5"
  top: "Concat5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Concat5"
  top: "BatchNorm6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "BatchNorm6"
  top: "Convolution7"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution7"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat6"
  type: "Concat"
  bottom: "Concat5"
  bottom: "Dropout6"
  top: "Concat6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Concat6"
  top: "BatchNorm7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "BatchNorm7"
  top: "Convolution8"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution8"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat7"
  type: "Concat"
  bottom: "Concat6"
  bottom: "Dropout7"
  top: "Concat7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Concat7"
  top: "BatchNorm8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "BatchNorm8"
  top: "Convolution9"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution9"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat8"
  type: "Concat"
  bottom: "Concat7"
  bottom: "Dropout8"
  top: "Concat8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Concat8"
  top: "BatchNorm9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "BatchNorm9"
  top: "Convolution10"
  convolution_param {
    num_output: 112
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution10"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Dropout9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Pooling1"
  top: "BatchNorm10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "BatchNorm10"
  top: "Convolution11"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution11"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat9"
  type: "Concat"
  bottom: "Pooling1"
  bottom: "Dropout10"
  top: "Concat9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Concat9"
  top: "BatchNorm11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "BatchNorm11"
  top: "Convolution12"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution12"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat10"
  type: "Concat"
  bottom: "Concat9"
  bottom: "Dropout11"
  top: "Concat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Concat10"
  top: "BatchNorm12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "BatchNorm12"
  top: "Convolution13"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout12"
  type: "Dropout"
  bottom: "Convolution13"
  top: "Dropout12"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat11"
  type: "Concat"
  bottom: "Concat10"
  bottom: "Dropout12"
  top: "Concat11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Concat11"
  top: "BatchNorm13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "BatchNorm13"
  top: "Convolution14"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout13"
  type: "Dropout"
  bottom: "Convolution14"
  top: "Dropout13"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat12"
  type: "Concat"
  bottom: "Concat11"
  bottom: "Dropout13"
  top: "Concat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Concat12"
  top: "BatchNorm14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "BatchNorm14"
  top: "Convolution15"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout14"
  type: "Dropout"
  bottom: "Convolution15"
  top: "Dropout14"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat13"
  type: "Concat"
  bottom: "Concat12"
  bottom: "Dropout14"
  top: "Concat13"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Concat13"
  top: "BatchNorm15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "BatchNorm15"
  top: "Convolution16"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout15"
  type: "Dropout"
  bottom: "Convolution16"
  top: "Dropout15"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat14"
  type: "Concat"
  bottom: "Concat13"
  bottom: "Dropout15"
  top: "Concat14"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Concat14"
  top: "BatchNorm16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "BatchNorm16"
  top: "Convolution17"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout16"
  type: "Dropout"
  bottom: "Convolution17"
  top: "Dropout16"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat15"
  type: "Concat"
  bottom: "Concat14"
  bottom: "Dropout16"
  top: "Concat15"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Concat15"
  top: "BatchNorm17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "BatchNorm17"
  top: "Convolution18"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout17"
  type: "Dropout"
  bottom: "Convolution18"
  top: "Dropout17"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat16"
  type: "Concat"
  bottom: "Concat15"
  bottom: "Dropout17"
  top: "Concat16"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Concat16"
  top: "BatchNorm18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "BatchNorm18"
  top: "Convolution19"
  convolution_param {
    num_output: 208
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout18"
  type: "Dropout"
  bottom: "Convolution19"
  top: "Dropout18"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Dropout18"
  top: "Pooling2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Pooling2"
  top: "BatchNorm19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "BatchNorm19"
  top: "Convolution20"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout19"
  type: "Dropout"
  bottom: "Convolution20"
  top: "Dropout19"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat17"
  type: "Concat"
  bottom: "Pooling2"
  bottom: "Dropout19"
  top: "Concat17"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Concat17"
  top: "BatchNorm20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "BatchNorm20"
  top: "Convolution21"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout20"
  type: "Dropout"
  bottom: "Convolution21"
  top: "Dropout20"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat18"
  type: "Concat"
  bottom: "Concat17"
  bottom: "Dropout20"
  top: "Concat18"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Concat18"
  top: "BatchNorm21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "BatchNorm21"
  top: "Convolution22"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout21"
  type: "Dropout"
  bottom: "Convolution22"
  top: "Dropout21"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat19"
  type: "Concat"
  bottom: "Concat18"
  bottom: "Dropout21"
  top: "Concat19"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Concat19"
  top: "BatchNorm22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "BatchNorm22"
  top: "Convolution23"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout22"
  type: "Dropout"
  bottom: "Convolution23"
  top: "Dropout22"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat20"
  type: "Concat"
  bottom: "Concat19"
  bottom: "Dropout22"
  top: "Concat20"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Concat20"
  top: "BatchNorm23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "BatchNorm23"
  top: "Convolution24"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout23"
  type: "Dropout"
  bottom: "Convolution24"
  top: "Dropout23"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat21"
  type: "Concat"
  bottom: "Concat20"
  bottom: "Dropout23"
  top: "Concat21"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Concat21"
  top: "BatchNorm24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "BatchNorm24"
  top: "Convolution25"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout24"
  type: "Dropout"
  bottom: "Convolution25"
  top: "Dropout24"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat22"
  type: "Concat"
  bottom: "Concat21"
  bottom: "Dropout24"
  top: "Concat22"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Concat22"
  top: "BatchNorm25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "BatchNorm25"
  top: "Convolution26"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout25"
  type: "Dropout"
  bottom: "Convolution26"
  top: "Dropout25"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat23"
  type: "Concat"
  bottom: "Concat22"
  bottom: "Dropout25"
  top: "Concat23"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Concat23"
  top: "BatchNorm26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "BatchNorm26"
  top: "Convolution27"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout26"
  type: "Dropout"
  bottom: "Convolution27"
  top: "Dropout26"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat24"
  type: "Concat"
  bottom: "Concat23"
  bottom: "Dropout26"
  top: "Concat24"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Concat24"
  top: "BatchNorm27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "BatchNorm27"
  top: "BatchNorm27"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "BatchNorm27"
  top: "BatchNorm27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "BatchNorm27"
  top: "Convolution28"
  convolution_param {
    num_output: 304
    bias_term: false
    pad: 0
   
I0211 13:04:07.027598 15724 layer_factory.hpp:77] Creating layer input-data
I0211 13:04:07.046627 15724 net.cpp:106] Creating Layer input-data
I0211 13:04:07.046651 15724 net.cpp:411] input-data -> data
I0211 13:04:07.046667 15724 net.cpp:411] input-data -> im_info
I0211 13:04:07.046677 15724 net.cpp:411] input-data -> gt_boxes
I0211 13:04:07.094004 15724 net.cpp:150] Setting up input-data
I0211 13:04:07.094040 15724 net.cpp:157] Top shape: 1 3 300 500 (450000)
I0211 13:04:07.094050 15724 net.cpp:157] Top shape: 1 3 (3)
I0211 13:04:07.094059 15724 net.cpp:157] Top shape: 1 4 (4)
I0211 13:04:07.094065 15724 net.cpp:165] Memory required for data: 1800028
I0211 13:04:07.094089 15724 layer_factory.hpp:77] Creating layer data_input-data_0_split
I0211 13:04:07.094105 15724 net.cpp:106] Creating Layer data_input-data_0_split
I0211 13:04:07.094113 15724 net.cpp:454] data_input-data_0_split <- data
I0211 13:04:07.094125 15724 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_0
I0211 13:04:07.094136 15724 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_1
I0211 13:04:07.094195 15724 net.cpp:150] Setting up data_input-data_0_split
I0211 13:04:07.094207 15724 net.cpp:157] Top shape: 1 3 300 500 (450000)
I0211 13:04:07.094213 15724 net.cpp:157] Top shape: 1 3 300 500 (450000)
I0211 13:04:07.094219 15724 net.cpp:165] Memory required for data: 5400028
I0211 13:04:07.094224 15724 layer_factory.hpp:77] Creating layer Convolution1
I0211 13:04:07.094244 15724 net.cpp:106] Creating Layer Convolution1
I0211 13:04:07.094251 15724 net.cpp:454] Convolution1 <- data_input-data_0_split_0
I0211 13:04:07.094259 15724 net.cpp:411] Convolution1 -> Convolution1
I0211 13:04:07.095183 15724 net.cpp:150] Setting up Convolution1
I0211 13:04:07.095199 15724 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0211 13:04:07.095206 15724 net.cpp:165] Memory required for data: 15000028
I0211 13:04:07.095218 15724 layer_factory.hpp:77] Creating layer Convolution1_Convolution1_0_split
I0211 13:04:07.095227 15724 net.cpp:106] Creating Layer Convolution1_Convolution1_0_split
I0211 13:04:07.095232 15724 net.cpp:454] Convolution1_Convolution1_0_split <- Convolution1
I0211 13:04:07.095239 15724 net.cpp:411] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_0
I0211 13:04:07.095249 15724 net.cpp:411] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_1
I0211 13:04:07.095304 15724 net.cpp:150] Setting up Convolution1_Convolution1_0_split
I0211 13:04:07.095315 15724 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0211 13:04:07.095322 15724 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0211 13:04:07.095329 15724 net.cpp:165] Memory required for data: 34200028
I0211 13:04:07.095332 15724 layer_factory.hpp:77] Creating layer BatchNorm1
I0211 13:04:07.095350 15724 net.cpp:106] Creating Layer BatchNorm1
I0211 13:04:07.095357 15724 net.cpp:454] BatchNorm1 <- Convolution1_Convolution1_0_split_0
I0211 13:04:07.095365 15724 net.cpp:411] BatchNorm1 -> BatchNorm1
I0211 13:04:07.096361 15724 net.cpp:150] Setting up BatchNorm1
I0211 13:04:07.096377 15724 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0211 13:04:07.096384 15724 net.cpp:165] Memory required for data: 43800028
I0211 13:04:07.096400 15724 layer_factory.hpp:77] Creating layer Scale1
I0211 13:04:07.096412 15724 net.cpp:106] Creating Layer Scale1
I0211 13:04:07.096420 15724 net.cpp:454] Scale1 <- BatchNorm1
I0211 13:04:07.096428 15724 net.cpp:397] Scale1 -> BatchNorm1 (in-place)
I0211 13:04:07.096496 15724 layer_factory.hpp:77] Creating layer Scale1
I0211 13:04:07.098175 15724 net.cpp:150] Setting up Scale1
I0211 13:04:07.098194 15724 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0211 13:04:07.098201 15724 net.cpp:165] Memory required for data: 53400028
I0211 13:04:07.098215 15724 layer_factory.hpp:77] Creating layer ReLU1
I0211 13:04:07.098229 15724 net.cpp:106] Creating Layer ReLU1
I0211 13:04:07.098238 15724 net.cpp:454] ReLU1 <- BatchNorm1
I0211 13:04:07.098247 15724 net.cpp:397] ReLU1 -> BatchNorm1 (in-place)
I0211 13:04:07.098258 15724 net.cpp:150] Setting up ReLU1
I0211 13:04:07.098279 15724 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0211 13:04:07.098284 15724 net.cpp:165] Memory required for data: 63000028
I0211 13:04:07.098289 15724 layer_factory.hpp:77] Creating layer Convolution2
I0211 13:04:07.098302 15724 net.cpp:106] Creating Layer Convolution2
I0211 13:04:07.098309 15724 net.cpp:454] Convolution2 <- BatchNorm1
I0211 13:04:07.098315 15724 net.cpp:411] Convolution2 -> Convolution2
I0211 13:04:07.099261 15724 net.cpp:150] Setting up Convolution2
I0211 13:04:07.099277 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.099282 15724 net.cpp:165] Memory required for data: 70200028
I0211 13:04:07.099288 15724 layer_factory.hpp:77] Creating layer Dropout1
I0211 13:04:07.099308 15724 net.cpp:106] Creating Layer Dropout1
I0211 13:04:07.099313 15724 net.cpp:454] Dropout1 <- Convolution2
I0211 13:04:07.099323 15724 net.cpp:411] Dropout1 -> Dropout1
I0211 13:04:07.099398 15724 net.cpp:150] Setting up Dropout1
I0211 13:04:07.099409 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.099416 15724 net.cpp:165] Memory required for data: 77400028
I0211 13:04:07.099421 15724 layer_factory.hpp:77] Creating layer Concat1
I0211 13:04:07.099431 15724 net.cpp:106] Creating Layer Concat1
I0211 13:04:07.099436 15724 net.cpp:454] Concat1 <- Convolution1_Convolution1_0_split_1
I0211 13:04:07.099442 15724 net.cpp:454] Concat1 <- Dropout1
I0211 13:04:07.099452 15724 net.cpp:411] Concat1 -> Concat1
I0211 13:04:07.099488 15724 net.cpp:150] Setting up Concat1
I0211 13:04:07.099498 15724 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0211 13:04:07.099503 15724 net.cpp:165] Memory required for data: 94200028
I0211 13:04:07.099509 15724 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I0211 13:04:07.099519 15724 net.cpp:106] Creating Layer Concat1_Concat1_0_split
I0211 13:04:07.099524 15724 net.cpp:454] Concat1_Concat1_0_split <- Concat1
I0211 13:04:07.099530 15724 net.cpp:411] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I0211 13:04:07.099539 15724 net.cpp:411] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I0211 13:04:07.099587 15724 net.cpp:150] Setting up Concat1_Concat1_0_split
I0211 13:04:07.099597 15724 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0211 13:04:07.099604 15724 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0211 13:04:07.099609 15724 net.cpp:165] Memory required for data: 127800028
I0211 13:04:07.099614 15724 layer_factory.hpp:77] Creating layer BatchNorm2
I0211 13:04:07.099624 15724 net.cpp:106] Creating Layer BatchNorm2
I0211 13:04:07.099632 15724 net.cpp:454] BatchNorm2 <- Concat1_Concat1_0_split_0
I0211 13:04:07.099639 15724 net.cpp:411] BatchNorm2 -> BatchNorm2
I0211 13:04:07.100584 15724 net.cpp:150] Setting up BatchNorm2
I0211 13:04:07.100599 15724 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0211 13:04:07.100605 15724 net.cpp:165] Memory required for data: 144600028
I0211 13:04:07.100620 15724 layer_factory.hpp:77] Creating layer Scale2
I0211 13:04:07.100631 15724 net.cpp:106] Creating Layer Scale2
I0211 13:04:07.100637 15724 net.cpp:454] Scale2 <- BatchNorm2
I0211 13:04:07.100646 15724 net.cpp:397] Scale2 -> BatchNorm2 (in-place)
I0211 13:04:07.100703 15724 layer_factory.hpp:77] Creating layer Scale2
I0211 13:04:07.102200 15724 net.cpp:150] Setting up Scale2
I0211 13:04:07.102216 15724 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0211 13:04:07.102221 15724 net.cpp:165] Memory required for data: 161400028
I0211 13:04:07.102231 15724 layer_factory.hpp:77] Creating layer ReLU2
I0211 13:04:07.102244 15724 net.cpp:106] Creating Layer ReLU2
I0211 13:04:07.102250 15724 net.cpp:454] ReLU2 <- BatchNorm2
I0211 13:04:07.102257 15724 net.cpp:397] ReLU2 -> BatchNorm2 (in-place)
I0211 13:04:07.102265 15724 net.cpp:150] Setting up ReLU2
I0211 13:04:07.102273 15724 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0211 13:04:07.102278 15724 net.cpp:165] Memory required for data: 178200028
I0211 13:04:07.102283 15724 layer_factory.hpp:77] Creating layer Convolution3
I0211 13:04:07.102296 15724 net.cpp:106] Creating Layer Convolution3
I0211 13:04:07.102303 15724 net.cpp:454] Convolution3 <- BatchNorm2
I0211 13:04:07.102310 15724 net.cpp:411] Convolution3 -> Convolution3
I0211 13:04:07.102676 15724 net.cpp:150] Setting up Convolution3
I0211 13:04:07.102690 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.102695 15724 net.cpp:165] Memory required for data: 185400028
I0211 13:04:07.102702 15724 layer_factory.hpp:77] Creating layer Dropout2
I0211 13:04:07.102711 15724 net.cpp:106] Creating Layer Dropout2
I0211 13:04:07.102717 15724 net.cpp:454] Dropout2 <- Convolution3
I0211 13:04:07.102726 15724 net.cpp:411] Dropout2 -> Dropout2
I0211 13:04:07.102782 15724 net.cpp:150] Setting up Dropout2
I0211 13:04:07.102793 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.102799 15724 net.cpp:165] Memory required for data: 192600028
I0211 13:04:07.102804 15724 layer_factory.hpp:77] Creating layer Concat2
I0211 13:04:07.102815 15724 net.cpp:106] Creating Layer Concat2
I0211 13:04:07.102823 15724 net.cpp:454] Concat2 <- Concat1_Concat1_0_split_1
I0211 13:04:07.102828 15724 net.cpp:454] Concat2 <- Dropout2
I0211 13:04:07.102835 15724 net.cpp:411] Concat2 -> Concat2
I0211 13:04:07.102869 15724 net.cpp:150] Setting up Concat2
I0211 13:04:07.102880 15724 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0211 13:04:07.102887 15724 net.cpp:165] Memory required for data: 216600028
I0211 13:04:07.102892 15724 layer_factory.hpp:77] Creating layer Concat2_Concat2_0_split
I0211 13:04:07.102900 15724 net.cpp:106] Creating Layer Concat2_Concat2_0_split
I0211 13:04:07.102906 15724 net.cpp:454] Concat2_Concat2_0_split <- Concat2
I0211 13:04:07.102912 15724 net.cpp:411] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_0
I0211 13:04:07.102921 15724 net.cpp:411] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_1
I0211 13:04:07.102970 15724 net.cpp:150] Setting up Concat2_Concat2_0_split
I0211 13:04:07.102980 15724 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0211 13:04:07.102988 15724 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0211 13:04:07.102993 15724 net.cpp:165] Memory required for data: 264600028
I0211 13:04:07.102998 15724 layer_factory.hpp:77] Creating layer BatchNorm3
I0211 13:04:07.103006 15724 net.cpp:106] Creating Layer BatchNorm3
I0211 13:04:07.103013 15724 net.cpp:454] BatchNorm3 <- Concat2_Concat2_0_split_0
I0211 13:04:07.103019 15724 net.cpp:411] BatchNorm3 -> BatchNorm3
I0211 13:04:07.103979 15724 net.cpp:150] Setting up BatchNorm3
I0211 13:04:07.103994 15724 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0211 13:04:07.104001 15724 net.cpp:165] Memory required for data: 288600028
I0211 13:04:07.104010 15724 layer_factory.hpp:77] Creating layer Scale3
I0211 13:04:07.104019 15724 net.cpp:106] Creating Layer Scale3
I0211 13:04:07.104027 15724 net.cpp:454] Scale3 <- BatchNorm3
I0211 13:04:07.104035 15724 net.cpp:397] Scale3 -> BatchNorm3 (in-place)
I0211 13:04:07.104094 15724 layer_factory.hpp:77] Creating layer Scale3
I0211 13:04:07.105587 15724 net.cpp:150] Setting up Scale3
I0211 13:04:07.105602 15724 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0211 13:04:07.105607 15724 net.cpp:165] Memory required for data: 312600028
I0211 13:04:07.105620 15724 layer_factory.hpp:77] Creating layer ReLU3
I0211 13:04:07.105630 15724 net.cpp:106] Creating Layer ReLU3
I0211 13:04:07.105636 15724 net.cpp:454] ReLU3 <- BatchNorm3
I0211 13:04:07.105643 15724 net.cpp:397] ReLU3 -> BatchNorm3 (in-place)
I0211 13:04:07.105653 15724 net.cpp:150] Setting up ReLU3
I0211 13:04:07.105659 15724 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0211 13:04:07.105664 15724 net.cpp:165] Memory required for data: 336600028
I0211 13:04:07.105669 15724 layer_factory.hpp:77] Creating layer Convolution4
I0211 13:04:07.105684 15724 net.cpp:106] Creating Layer Convolution4
I0211 13:04:07.105690 15724 net.cpp:454] Convolution4 <- BatchNorm3
I0211 13:04:07.105698 15724 net.cpp:411] Convolution4 -> Convolution4
I0211 13:04:07.106712 15724 net.cpp:150] Setting up Convolution4
I0211 13:04:07.106727 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.106732 15724 net.cpp:165] Memory required for data: 343800028
I0211 13:04:07.106740 15724 layer_factory.hpp:77] Creating layer Dropout3
I0211 13:04:07.106750 15724 net.cpp:106] Creating Layer Dropout3
I0211 13:04:07.106755 15724 net.cpp:454] Dropout3 <- Convolution4
I0211 13:04:07.106761 15724 net.cpp:411] Dropout3 -> Dropout3
I0211 13:04:07.106817 15724 net.cpp:150] Setting up Dropout3
I0211 13:04:07.106829 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.106835 15724 net.cpp:165] Memory required for data: 351000028
I0211 13:04:07.106840 15724 layer_factory.hpp:77] Creating layer Concat3
I0211 13:04:07.106848 15724 net.cpp:106] Creating Layer Concat3
I0211 13:04:07.106854 15724 net.cpp:454] Concat3 <- Concat2_Concat2_0_split_1
I0211 13:04:07.106860 15724 net.cpp:454] Concat3 <- Dropout3
I0211 13:04:07.106868 15724 net.cpp:411] Concat3 -> Concat3
I0211 13:04:07.106904 15724 net.cpp:150] Setting up Concat3
I0211 13:04:07.106914 15724 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0211 13:04:07.106920 15724 net.cpp:165] Memory required for data: 382200028
I0211 13:04:07.106925 15724 layer_factory.hpp:77] Creating layer Concat3_Concat3_0_split
I0211 13:04:07.106931 15724 net.cpp:106] Creating Layer Concat3_Concat3_0_split
I0211 13:04:07.106937 15724 net.cpp:454] Concat3_Concat3_0_split <- Concat3
I0211 13:04:07.106945 15724 net.cpp:411] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_0
I0211 13:04:07.106952 15724 net.cpp:411] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_1
I0211 13:04:07.107005 15724 net.cpp:150] Setting up Concat3_Concat3_0_split
I0211 13:04:07.107018 15724 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0211 13:04:07.107025 15724 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0211 13:04:07.107030 15724 net.cpp:165] Memory required for data: 444600028
I0211 13:04:07.107035 15724 layer_factory.hpp:77] Creating layer BatchNorm4
I0211 13:04:07.107043 15724 net.cpp:106] Creating Layer BatchNorm4
I0211 13:04:07.107049 15724 net.cpp:454] BatchNorm4 <- Concat3_Concat3_0_split_0
I0211 13:04:07.107059 15724 net.cpp:411] BatchNorm4 -> BatchNorm4
I0211 13:04:07.108114 15724 net.cpp:150] Setting up BatchNorm4
I0211 13:04:07.108129 15724 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0211 13:04:07.108135 15724 net.cpp:165] Memory required for data: 475800028
I0211 13:04:07.108145 15724 layer_factory.hpp:77] Creating layer Scale4
I0211 13:04:07.108157 15724 net.cpp:106] Creating Layer Scale4
I0211 13:04:07.108165 15724 net.cpp:454] Scale4 <- BatchNorm4
I0211 13:04:07.108170 15724 net.cpp:397] Scale4 -> BatchNorm4 (in-place)
I0211 13:04:07.108232 15724 layer_factory.hpp:77] Creating layer Scale4
I0211 13:04:07.109766 15724 net.cpp:150] Setting up Scale4
I0211 13:04:07.109781 15724 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0211 13:04:07.109786 15724 net.cpp:165] Memory required for data: 507000028
I0211 13:04:07.109796 15724 layer_factory.hpp:77] Creating layer ReLU4
I0211 13:04:07.109804 15724 net.cpp:106] Creating Layer ReLU4
I0211 13:04:07.109810 15724 net.cpp:454] ReLU4 <- BatchNorm4
I0211 13:04:07.109817 15724 net.cpp:397] ReLU4 -> BatchNorm4 (in-place)
I0211 13:04:07.109825 15724 net.cpp:150] Setting up ReLU4
I0211 13:04:07.109833 15724 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0211 13:04:07.109838 15724 net.cpp:165] Memory required for data: 538200028
I0211 13:04:07.109843 15724 layer_factory.hpp:77] Creating layer Convolution5
I0211 13:04:07.109855 15724 net.cpp:106] Creating Layer Convolution5
I0211 13:04:07.109861 15724 net.cpp:454] Convolution5 <- BatchNorm4
I0211 13:04:07.109871 15724 net.cpp:411] Convolution5 -> Convolution5
I0211 13:04:07.110313 15724 net.cpp:150] Setting up Convolution5
I0211 13:04:07.110327 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.110332 15724 net.cpp:165] Memory required for data: 545400028
I0211 13:04:07.110339 15724 layer_factory.hpp:77] Creating layer Dropout4
I0211 13:04:07.110347 15724 net.cpp:106] Creating Layer Dropout4
I0211 13:04:07.110353 15724 net.cpp:454] Dropout4 <- Convolution5
I0211 13:04:07.110363 15724 net.cpp:411] Dropout4 -> Dropout4
I0211 13:04:07.110419 15724 net.cpp:150] Setting up Dropout4
I0211 13:04:07.110430 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.110435 15724 net.cpp:165] Memory required for data: 552600028
I0211 13:04:07.110440 15724 layer_factory.hpp:77] Creating layer Concat4
I0211 13:04:07.110447 15724 net.cpp:106] Creating Layer Concat4
I0211 13:04:07.110453 15724 net.cpp:454] Concat4 <- Concat3_Concat3_0_split_1
I0211 13:04:07.110460 15724 net.cpp:454] Concat4 <- Dropout4
I0211 13:04:07.110466 15724 net.cpp:411] Concat4 -> Concat4
I0211 13:04:07.110503 15724 net.cpp:150] Setting up Concat4
I0211 13:04:07.110514 15724 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0211 13:04:07.110519 15724 net.cpp:165] Memory required for data: 591000028
I0211 13:04:07.110524 15724 layer_factory.hpp:77] Creating layer Concat4_Concat4_0_split
I0211 13:04:07.110532 15724 net.cpp:106] Creating Layer Concat4_Concat4_0_split
I0211 13:04:07.110538 15724 net.cpp:454] Concat4_Concat4_0_split <- Concat4
I0211 13:04:07.110546 15724 net.cpp:411] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_0
I0211 13:04:07.110554 15724 net.cpp:411] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_1
I0211 13:04:07.110620 15724 net.cpp:150] Setting up Concat4_Concat4_0_split
I0211 13:04:07.110632 15724 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0211 13:04:07.110640 15724 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0211 13:04:07.110644 15724 net.cpp:165] Memory required for data: 667800028
I0211 13:04:07.110649 15724 layer_factory.hpp:77] Creating layer BatchNorm5
I0211 13:04:07.110661 15724 net.cpp:106] Creating Layer BatchNorm5
I0211 13:04:07.110668 15724 net.cpp:454] BatchNorm5 <- Concat4_Concat4_0_split_0
I0211 13:04:07.110679 15724 net.cpp:411] BatchNorm5 -> BatchNorm5
I0211 13:04:07.111641 15724 net.cpp:150] Setting up BatchNorm5
I0211 13:04:07.111656 15724 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0211 13:04:07.111661 15724 net.cpp:165] Memory required for data: 706200028
I0211 13:04:07.111671 15724 layer_factory.hpp:77] Creating layer Scale5
I0211 13:04:07.111682 15724 net.cpp:106] Creating Layer Scale5
I0211 13:04:07.111690 15724 net.cpp:454] Scale5 <- BatchNorm5
I0211 13:04:07.111696 15724 net.cpp:397] Scale5 -> BatchNorm5 (in-place)
I0211 13:04:07.111755 15724 layer_factory.hpp:77] Creating layer Scale5
I0211 13:04:07.113286 15724 net.cpp:150] Setting up Scale5
I0211 13:04:07.113301 15724 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0211 13:04:07.113306 15724 net.cpp:165] Memory required for data: 744600028
I0211 13:04:07.113317 15724 layer_factory.hpp:77] Creating layer ReLU5
I0211 13:04:07.113325 15724 net.cpp:106] Creating Layer ReLU5
I0211 13:04:07.113332 15724 net.cpp:454] ReLU5 <- BatchNorm5
I0211 13:04:07.113337 15724 net.cpp:397] ReLU5 -> BatchNorm5 (in-place)
I0211 13:04:07.113349 15724 net.cpp:150] Setting up ReLU5
I0211 13:04:07.113356 15724 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0211 13:04:07.113361 15724 net.cpp:165] Memory required for data: 783000028
I0211 13:04:07.113366 15724 layer_factory.hpp:77] Creating layer Convolution6
I0211 13:04:07.113378 15724 net.cpp:106] Creating Layer Convolution6
I0211 13:04:07.113382 15724 net.cpp:454] Convolution6 <- BatchNorm5
I0211 13:04:07.113392 15724 net.cpp:411] Convolution6 -> Convolution6
I0211 13:04:07.113875 15724 net.cpp:150] Setting up Convolution6
I0211 13:04:07.113889 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.113894 15724 net.cpp:165] Memory required for data: 790200028
I0211 13:04:07.113900 15724 layer_factory.hpp:77] Creating layer Dropout5
I0211 13:04:07.113910 15724 net.cpp:106] Creating Layer Dropout5
I0211 13:04:07.113915 15724 net.cpp:454] Dropout5 <- Convolution6
I0211 13:04:07.113922 15724 net.cpp:411] Dropout5 -> Dropout5
I0211 13:04:07.113977 15724 net.cpp:150] Setting up Dropout5
I0211 13:04:07.113988 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.113994 15724 net.cpp:165] Memory required for data: 797400028
I0211 13:04:07.113999 15724 layer_factory.hpp:77] Creating layer Concat5
I0211 13:04:07.114007 15724 net.cpp:106] Creating Layer Concat5
I0211 13:04:07.114014 15724 net.cpp:454] Concat5 <- Concat4_Concat4_0_split_1
I0211 13:04:07.114019 15724 net.cpp:454] Concat5 <- Dropout5
I0211 13:04:07.114029 15724 net.cpp:411] Concat5 -> Concat5
I0211 13:04:07.114066 15724 net.cpp:150] Setting up Concat5
I0211 13:04:07.114078 15724 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0211 13:04:07.114083 15724 net.cpp:165] Memory required for data: 843000028
I0211 13:04:07.114087 15724 layer_factory.hpp:77] Creating layer Concat5_Concat5_0_split
I0211 13:04:07.114097 15724 net.cpp:106] Creating Layer Concat5_Concat5_0_split
I0211 13:04:07.114104 15724 net.cpp:454] Concat5_Concat5_0_split <- Concat5
I0211 13:04:07.114110 15724 net.cpp:411] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_0
I0211 13:04:07.114120 15724 net.cpp:411] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_1
I0211 13:04:07.114173 15724 net.cpp:150] Setting up Concat5_Concat5_0_split
I0211 13:04:07.114184 15724 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0211 13:04:07.114192 15724 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0211 13:04:07.114197 15724 net.cpp:165] Memory required for data: 934200028
I0211 13:04:07.114202 15724 layer_factory.hpp:77] Creating layer BatchNorm6
I0211 13:04:07.114212 15724 net.cpp:106] Creating Layer BatchNorm6
I0211 13:04:07.114217 15724 net.cpp:454] BatchNorm6 <- Concat5_Concat5_0_split_0
I0211 13:04:07.114224 15724 net.cpp:411] BatchNorm6 -> BatchNorm6
I0211 13:04:07.115226 15724 net.cpp:150] Setting up BatchNorm6
I0211 13:04:07.115241 15724 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0211 13:04:07.115247 15724 net.cpp:165] Memory required for data: 979800028
I0211 13:04:07.115262 15724 layer_factory.hpp:77] Creating layer Scale6
I0211 13:04:07.115273 15724 net.cpp:106] Creating Layer Scale6
I0211 13:04:07.115279 15724 net.cpp:454] Scale6 <- BatchNorm6
I0211 13:04:07.115288 15724 net.cpp:397] Scale6 -> BatchNorm6 (in-place)
I0211 13:04:07.115350 15724 layer_factory.hpp:77] Creating layer Scale6
I0211 13:04:07.116863 15724 net.cpp:150] Setting up Scale6
I0211 13:04:07.116878 15724 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0211 13:04:07.116884 15724 net.cpp:165] Memory required for data: 1025400028
I0211 13:04:07.116892 15724 layer_factory.hpp:77] Creating layer ReLU6
I0211 13:04:07.116901 15724 net.cpp:106] Creating Layer ReLU6
I0211 13:04:07.116909 15724 net.cpp:454] ReLU6 <- BatchNorm6
I0211 13:04:07.116917 15724 net.cpp:397] ReLU6 -> BatchNorm6 (in-place)
I0211 13:04:07.116926 15724 net.cpp:150] Setting up ReLU6
I0211 13:04:07.116935 15724 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0211 13:04:07.116940 15724 net.cpp:165] Memory required for data: 1071000028
I0211 13:04:07.116945 15724 layer_factory.hpp:77] Creating layer Convolution7
I0211 13:04:07.116957 15724 net.cpp:106] Creating Layer Convolution7
I0211 13:04:07.116963 15724 net.cpp:454] Convolution7 <- BatchNorm6
I0211 13:04:07.116971 15724 net.cpp:411] Convolution7 -> Convolution7
I0211 13:04:07.117492 15724 net.cpp:150] Setting up Convolution7
I0211 13:04:07.117506 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.117511 15724 net.cpp:165] Memory required for data: 1078200028
I0211 13:04:07.117517 15724 layer_factory.hpp:77] Creating layer Dropout6
I0211 13:04:07.117527 15724 net.cpp:106] Creating Layer Dropout6
I0211 13:04:07.117532 15724 net.cpp:454] Dropout6 <- Convolution7
I0211 13:04:07.117542 15724 net.cpp:411] Dropout6 -> Dropout6
I0211 13:04:07.117599 15724 net.cpp:150] Setting up Dropout6
I0211 13:04:07.117610 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.117616 15724 net.cpp:165] Memory required for data: 1085400028
I0211 13:04:07.117621 15724 layer_factory.hpp:77] Creating layer Concat6
I0211 13:04:07.117630 15724 net.cpp:106] Creating Layer Concat6
I0211 13:04:07.117635 15724 net.cpp:454] Concat6 <- Concat5_Concat5_0_split_1
I0211 13:04:07.117641 15724 net.cpp:454] Concat6 <- Dropout6
I0211 13:04:07.117648 15724 net.cpp:411] Concat6 -> Concat6
I0211 13:04:07.117689 15724 net.cpp:150] Setting up Concat6
I0211 13:04:07.117700 15724 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0211 13:04:07.117705 15724 net.cpp:165] Memory required for data: 1138200028
I0211 13:04:07.117710 15724 layer_factory.hpp:77] Creating layer Concat6_Concat6_0_split
I0211 13:04:07.117718 15724 net.cpp:106] Creating Layer Concat6_Concat6_0_split
I0211 13:04:07.117724 15724 net.cpp:454] Concat6_Concat6_0_split <- Concat6
I0211 13:04:07.117733 15724 net.cpp:411] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_0
I0211 13:04:07.117741 15724 net.cpp:411] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_1
I0211 13:04:07.117795 15724 net.cpp:150] Setting up Concat6_Concat6_0_split
I0211 13:04:07.117806 15724 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0211 13:04:07.117813 15724 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0211 13:04:07.117820 15724 net.cpp:165] Memory required for data: 1243800028
I0211 13:04:07.117823 15724 layer_factory.hpp:77] Creating layer BatchNorm7
I0211 13:04:07.117832 15724 net.cpp:106] Creating Layer BatchNorm7
I0211 13:04:07.117838 15724 net.cpp:454] BatchNorm7 <- Concat6_Concat6_0_split_0
I0211 13:04:07.117847 15724 net.cpp:411] BatchNorm7 -> BatchNorm7
I0211 13:04:07.118856 15724 net.cpp:150] Setting up BatchNorm7
I0211 13:04:07.118871 15724 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0211 13:04:07.118877 15724 net.cpp:165] Memory required for data: 1296600028
I0211 13:04:07.118888 15724 layer_factory.hpp:77] Creating layer Scale7
I0211 13:04:07.118901 15724 net.cpp:106] Creating Layer Scale7
I0211 13:04:07.118906 15724 net.cpp:454] Scale7 <- BatchNorm7
I0211 13:04:07.118916 15724 net.cpp:397] Scale7 -> BatchNorm7 (in-place)
I0211 13:04:07.118978 15724 layer_factory.hpp:77] Creating layer Scale7
I0211 13:04:07.120508 15724 net.cpp:150] Setting up Scale7
I0211 13:04:07.120524 15724 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0211 13:04:07.120529 15724 net.cpp:165] Memory required for data: 1349400028
I0211 13:04:07.120538 15724 layer_factory.hpp:77] Creating layer ReLU7
I0211 13:04:07.120546 15724 net.cpp:106] Creating Layer ReLU7
I0211 13:04:07.120553 15724 net.cpp:454] ReLU7 <- BatchNorm7
I0211 13:04:07.120559 15724 net.cpp:397] ReLU7 -> BatchNorm7 (in-place)
I0211 13:04:07.120568 15724 net.cpp:150] Setting up ReLU7
I0211 13:04:07.120575 15724 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0211 13:04:07.120580 15724 net.cpp:165] Memory required for data: 1402200028
I0211 13:04:07.120585 15724 layer_factory.hpp:77] Creating layer Convolution8
I0211 13:04:07.120599 15724 net.cpp:106] Creating Layer Convolution8
I0211 13:04:07.120605 15724 net.cpp:454] Convolution8 <- BatchNorm7
I0211 13:04:07.120614 15724 net.cpp:411] Convolution8 -> Convolution8
I0211 13:04:07.121191 15724 net.cpp:150] Setting up Convolution8
I0211 13:04:07.121204 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.121209 15724 net.cpp:165] Memory required for data: 1409400028
I0211 13:04:07.121217 15724 layer_factory.hpp:77] Creating layer Dropout7
I0211 13:04:07.121228 15724 net.cpp:106] Creating Layer Dropout7
I0211 13:04:07.121234 15724 net.cpp:454] Dropout7 <- Convolution8
I0211 13:04:07.121243 15724 net.cpp:411] Dropout7 -> Dropout7
I0211 13:04:07.121301 15724 net.cpp:150] Setting up Dropout7
I0211 13:04:07.121312 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.121318 15724 net.cpp:165] Memory required for data: 1416600028
I0211 13:04:07.121323 15724 layer_factory.hpp:77] Creating layer Concat7
I0211 13:04:07.121331 15724 net.cpp:106] Creating Layer Concat7
I0211 13:04:07.121337 15724 net.cpp:454] Concat7 <- Concat6_Concat6_0_split_1
I0211 13:04:07.121343 15724 net.cpp:454] Concat7 <- Dropout7
I0211 13:04:07.121356 15724 net.cpp:411] Concat7 -> Concat7
I0211 13:04:07.121392 15724 net.cpp:150] Setting up Concat7
I0211 13:04:07.121403 15724 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0211 13:04:07.121409 15724 net.cpp:165] Memory required for data: 1476600028
I0211 13:04:07.121414 15724 layer_factory.hpp:77] Creating layer Concat7_Concat7_0_split
I0211 13:04:07.121424 15724 net.cpp:106] Creating Layer Concat7_Concat7_0_split
I0211 13:04:07.121430 15724 net.cpp:454] Concat7_Concat7_0_split <- Concat7
I0211 13:04:07.121438 15724 net.cpp:411] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_0
I0211 13:04:07.121446 15724 net.cpp:411] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_1
I0211 13:04:07.121501 15724 net.cpp:150] Setting up Concat7_Concat7_0_split
I0211 13:04:07.121513 15724 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0211 13:04:07.121520 15724 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0211 13:04:07.121523 15724 net.cpp:165] Memory required for data: 1596600028
I0211 13:04:07.121529 15724 layer_factory.hpp:77] Creating layer BatchNorm8
I0211 13:04:07.121539 15724 net.cpp:106] Creating Layer BatchNorm8
I0211 13:04:07.121546 15724 net.cpp:454] BatchNorm8 <- Concat7_Concat7_0_split_0
I0211 13:04:07.121556 15724 net.cpp:411] BatchNorm8 -> BatchNorm8
I0211 13:04:07.122539 15724 net.cpp:150] Setting up BatchNorm8
I0211 13:04:07.122555 15724 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0211 13:04:07.122570 15724 net.cpp:165] Memory required for data: 1656600028
I0211 13:04:07.122581 15724 layer_factory.hpp:77] Creating layer Scale8
I0211 13:04:07.122613 15724 net.cpp:106] Creating Layer Scale8
I0211 13:04:07.122622 15724 net.cpp:454] Scale8 <- BatchNorm8
I0211 13:04:07.122630 15724 net.cpp:397] Scale8 -> BatchNorm8 (in-place)
I0211 13:04:07.122692 15724 layer_factory.hpp:77] Creating layer Scale8
I0211 13:04:07.124248 15724 net.cpp:150] Setting up Scale8
I0211 13:04:07.124263 15724 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0211 13:04:07.124267 15724 net.cpp:165] Memory required for data: 1716600028
I0211 13:04:07.124275 15724 layer_factory.hpp:77] Creating layer ReLU8
I0211 13:04:07.124282 15724 net.cpp:106] Creating Layer ReLU8
I0211 13:04:07.124287 15724 net.cpp:454] ReLU8 <- BatchNorm8
I0211 13:04:07.124296 15724 net.cpp:397] ReLU8 -> BatchNorm8 (in-place)
I0211 13:04:07.124305 15724 net.cpp:150] Setting up ReLU8
I0211 13:04:07.124311 15724 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0211 13:04:07.124315 15724 net.cpp:165] Memory required for data: 1776600028
I0211 13:04:07.124321 15724 layer_factory.hpp:77] Creating layer Convolution9
I0211 13:04:07.124333 15724 net.cpp:106] Creating Layer Convolution9
I0211 13:04:07.124339 15724 net.cpp:454] Convolution9 <- BatchNorm8
I0211 13:04:07.124346 15724 net.cpp:411] Convolution9 -> Convolution9
I0211 13:04:07.124948 15724 net.cpp:150] Setting up Convolution9
I0211 13:04:07.124960 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.124964 15724 net.cpp:165] Memory required for data: 1783800028
I0211 13:04:07.124971 15724 layer_factory.hpp:77] Creating layer Dropout8
I0211 13:04:07.124979 15724 net.cpp:106] Creating Layer Dropout8
I0211 13:04:07.124985 15724 net.cpp:454] Dropout8 <- Convolution9
I0211 13:04:07.124994 15724 net.cpp:411] Dropout8 -> Dropout8
I0211 13:04:07.125053 15724 net.cpp:150] Setting up Dropout8
I0211 13:04:07.125064 15724 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0211 13:04:07.125069 15724 net.cpp:165] Memory required for data: 1791000028
I0211 13:04:07.125074 15724 layer_factory.hpp:77] Creating layer Concat8
I0211 13:04:07.125084 15724 net.cpp:106] Creating Layer Concat8
I0211 13:04:07.125090 15724 net.cpp:454] Concat8 <- Concat7_Concat7_0_split_1
I0211 13:04:07.125097 15724 net.cpp:454] Concat8 <- Dropout8
I0211 13:04:07.125103 15724 net.cpp:411] Concat8 -> Concat8
I0211 13:04:07.125139 15724 net.cpp:150] Setting up Concat8
I0211 13:04:07.125151 15724 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0211 13:04:07.125157 15724 net.cpp:165] Memory required for data: 1858200028
I0211 13:04:07.125162 15724 layer_factory.hpp:77] Creating layer BatchNorm9
I0211 13:04:07.125170 15724 net.cpp:106] Creating Layer BatchNorm9
I0211 13:04:07.125176 15724 net.cpp:454] BatchNorm9 <- Concat8
I0211 13:04:07.125183 15724 net.cpp:411] BatchNorm9 -> BatchNorm9
I0211 13:04:07.126183 15724 net.cpp:150] Setting up BatchNorm9
I0211 13:04:07.126197 15724 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0211 13:04:07.126204 15724 net.cpp:165] Memory required for data: 1925400028
I0211 13:04:07.126215 15724 layer_factory.hpp:77] Creating layer Scale9
I0211 13:04:07.126226 15724 net.cpp:106] Creating Layer Scale9
I0211 13:04:07.126233 15724 net.cpp:454] Scale9 <- BatchNorm9
I0211 13:04:07.126240 15724 net.cpp:397] Scale9 -> BatchNorm9 (in-place)
I0211 13:04:07.126302 15724 layer_factory.hpp:77] Creating layer Scale9
I0211 13:04:07.127796 15724 net.cpp:150] Setting up Scale9
I0211 13:04:07.127811 15724 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0211 13:04:07.127816 15724 net.cpp:165] Memory required for data: 1992600028
I0211 13:04:07.127825 15724 layer_factory.hpp:77] Creating layer ReLU9
I0211 13:04:07.127832 15724 net.cpp:106] Creating Layer ReLU9
I0211 13:04:07.127837 15724 net.cpp:454] ReLU9 <- BatchNorm9
I0211 13:04:07.127846 15724 net.cpp:397] ReLU9 -> BatchNorm9 (in-place)
I0211 13:04:07.127853 15724 net.cpp:150] Setting up ReLU9
I0211 13:04:07.127861 15724 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0211 13:04:07.127866 15724 net.cpp:165] Memory required for data: 2059800028
I0211 13:04:07.127871 15724 layer_factory.hpp:77] Creating layer Convolution10
I0211 13:04:07.127882 15724 net.cpp:106] Creating Layer Convolution10
I0211 13:04:07.127887 15724 net.cpp:454] Convolution10 <- BatchNorm9
I0211 13:04:07.127897 15724 net.cpp:411] Convolution10 -> Convolution10
I0211 13:04:07.128547 15724 net.cpp:150] Setting up Convolution10
I0211 13:04:07.128561 15724 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0211 13:04:07.128564 15724 net.cpp:165] Memory required for data: 2127000028
I0211 13:04:07.128571 15724 layer_factory.hpp:77] Creating layer Dropout9
I0211 13:04:07.128581 15724 net.cpp:106] Creating Layer Dropout9
I0211 13:04:07.128585 15724 net.cpp:454] Dropout9 <- Convolution10
I0211 13:04:07.128592 15724 net.cpp:411] Dropout9 -> Dropout9
I0211 13:04:07.128648 15724 net.cpp:150] Setting up Dropout9
I0211 13:04:07.128659 15724 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0211 13:04:07.128664 15724 net.cpp:165] Memory required for data: 2194200028
I0211 13:04:07.128669 15724 layer_factory.hpp:77] Creating layer Pooling1
I0211 13:04:07.128684 15724 net.cpp:106] Creating Layer Pooling1
I0211 13:04:07.128690 15724 net.cpp:454] Pooling1 <- Dropout9
I0211 13:04:07.128700 15724 net.cpp:411] Pooling1 -> Pooling1
I0211 13:04:07.128749 15724 net.cpp:150] Setting up Pooling1
I0211 13:04:07.128759 15724 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0211 13:04:07.128765 15724 net.cpp:165] Memory required for data: 2211000028
I0211 13:04:07.128770 15724 layer_factory.hpp:77] Creating layer Pooling1_Pooling1_0_split
I0211 13:04:07.128778 15724 net.cpp:106] Creating Layer Pooling1_Pooling1_0_split
I0211 13:04:07.128783 15724 net.cpp:454] Pooling1_Pooling1_0_split <- Pooling1
I0211 13:04:07.128792 15724 net.cpp:411] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_0
I0211 13:04:07.128800 15724 net.cpp:411] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_1
I0211 13:04:07.128855 15724 net.cpp:150] Setting up Pooling1_Pooling1_0_split
I0211 13:04:07.128866 15724 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0211 13:04:07.128872 15724 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0211 13:04:07.128878 15724 net.cpp:165] Memory required for data: 2244600028
I0211 13:04:07.128882 15724 layer_factory.hpp:77] Creating layer BatchNorm10
I0211 13:04:07.128892 15724 net.cpp:106] Creating Layer BatchNorm10
I0211 13:04:07.128898 15724 net.cpp:454] BatchNorm10 <- Pooling1_Pooling1_0_split_0
I0211 13:04:07.128906 15724 net.cpp:411] BatchNorm10 -> BatchNorm10
I0211 13:04:07.129801 15724 net.cpp:150] Setting up BatchNorm10
I0211 13:04:07.129815 15724 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0211 13:04:07.129822 15724 net.cpp:165] Memory required for data: 2261400028
I0211 13:04:07.129832 15724 layer_factory.hpp:77] Creating layer Scale10
I0211 13:04:07.129843 15724 net.cpp:106] Creating Layer Scale10
I0211 13:04:07.129849 15724 net.cpp:454] Scale10 <- BatchNorm10
I0211 13:04:07.129860 15724 net.cpp:397] Scale10 -> BatchNorm10 (in-place)
I0211 13:04:07.129923 15724 layer_factory.hpp:77] Creating layer Scale10
I0211 13:04:07.130147 15724 net.cpp:150] Setting up Scale10
I0211 13:04:07.130159 15724 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0211 13:04:07.130164 15724 net.cpp:165] Memory required for data: 2278200028
I0211 13:04:07.130172 15724 layer_factory.hpp:77] Creating layer ReLU10
I0211 13:04:07.130182 15724 net.cpp:106] Creating Layer ReLU10
I0211 13:04:07.130187 15724 net.cpp:454] ReLU10 <- BatchNorm10
I0211 13:04:07.130192 15724 net.cpp:397] ReLU10 -> BatchNorm10 (in-place)
I0211 13:04:07.130200 15724 net.cpp:150] Setting up ReLU10
I0211 13:04:07.130208 15724 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0211 13:04:07.130213 15724 net.cpp:165] Memory required for data: 2295000028
I0211 13:04:07.130218 15724 layer_factory.hpp:77] Creating layer Convolution11
I0211 13:04:07.130230 15724 net.cpp:106] Creating Layer Convolution11
I0211 13:04:07.130236 15724 net.cpp:454] Convolution11 <- BatchNorm10
I0211 13:04:07.130244 15724 net.cpp:411] Convolution11 -> Convolution11
I0211 13:04:07.130880 15724 net.cpp:150] Setting up Convolution11
I0211 13:04:07.130893 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.130899 15724 net.cpp:165] Memory required for data: 2296800028
I0211 13:04:07.130906 15724 layer_factory.hpp:77] Creating layer Dropout10
I0211 13:04:07.130916 15724 net.cpp:106] Creating Layer Dropout10
I0211 13:04:07.130923 15724 net.cpp:454] Dropout10 <- Convolution11
I0211 13:04:07.130930 15724 net.cpp:411] Dropout10 -> Dropout10
I0211 13:04:07.130990 15724 net.cpp:150] Setting up Dropout10
I0211 13:04:07.131000 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.131006 15724 net.cpp:165] Memory required for data: 2298600028
I0211 13:04:07.131011 15724 layer_factory.hpp:77] Creating layer Concat9
I0211 13:04:07.131018 15724 net.cpp:106] Creating Layer Concat9
I0211 13:04:07.131023 15724 net.cpp:454] Concat9 <- Pooling1_Pooling1_0_split_1
I0211 13:04:07.131029 15724 net.cpp:454] Concat9 <- Dropout10
I0211 13:04:07.131037 15724 net.cpp:411] Concat9 -> Concat9
I0211 13:04:07.131074 15724 net.cpp:150] Setting up Concat9
I0211 13:04:07.131085 15724 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0211 13:04:07.131090 15724 net.cpp:165] Memory required for data: 2317200028
I0211 13:04:07.131095 15724 layer_factory.hpp:77] Creating layer Concat9_Concat9_0_split
I0211 13:04:07.131104 15724 net.cpp:106] Creating Layer Concat9_Concat9_0_split
I0211 13:04:07.131110 15724 net.cpp:454] Concat9_Concat9_0_split <- Concat9
I0211 13:04:07.131116 15724 net.cpp:411] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_0
I0211 13:04:07.131132 15724 net.cpp:411] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_1
I0211 13:04:07.131189 15724 net.cpp:150] Setting up Concat9_Concat9_0_split
I0211 13:04:07.131201 15724 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0211 13:04:07.131207 15724 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0211 13:04:07.131213 15724 net.cpp:165] Memory required for data: 2354400028
I0211 13:04:07.131217 15724 layer_factory.hpp:77] Creating layer BatchNorm11
I0211 13:04:07.131228 15724 net.cpp:106] Creating Layer BatchNorm11
I0211 13:04:07.131234 15724 net.cpp:454] BatchNorm11 <- Concat9_Concat9_0_split_0
I0211 13:04:07.131240 15724 net.cpp:411] BatchNorm11 -> BatchNorm11
I0211 13:04:07.131567 15724 net.cpp:150] Setting up BatchNorm11
I0211 13:04:07.131580 15724 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0211 13:04:07.131585 15724 net.cpp:165] Memory required for data: 2373000028
I0211 13:04:07.131594 15724 layer_factory.hpp:77] Creating layer Scale11
I0211 13:04:07.131605 15724 net.cpp:106] Creating Layer Scale11
I0211 13:04:07.131613 15724 net.cpp:454] Scale11 <- BatchNorm11
I0211 13:04:07.131618 15724 net.cpp:397] Scale11 -> BatchNorm11 (in-place)
I0211 13:04:07.131682 15724 layer_factory.hpp:77] Creating layer Scale11
I0211 13:04:07.131907 15724 net.cpp:150] Setting up Scale11
I0211 13:04:07.131920 15724 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0211 13:04:07.131927 15724 net.cpp:165] Memory required for data: 2391600028
I0211 13:04:07.131943 15724 layer_factory.hpp:77] Creating layer ReLU11
I0211 13:04:07.131953 15724 net.cpp:106] Creating Layer ReLU11
I0211 13:04:07.131959 15724 net.cpp:454] ReLU11 <- BatchNorm11
I0211 13:04:07.131965 15724 net.cpp:397] ReLU11 -> BatchNorm11 (in-place)
I0211 13:04:07.131973 15724 net.cpp:150] Setting up ReLU11
I0211 13:04:07.131980 15724 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0211 13:04:07.131986 15724 net.cpp:165] Memory required for data: 2410200028
I0211 13:04:07.131990 15724 layer_factory.hpp:77] Creating layer Convolution12
I0211 13:04:07.132004 15724 net.cpp:106] Creating Layer Convolution12
I0211 13:04:07.132009 15724 net.cpp:454] Convolution12 <- BatchNorm11
I0211 13:04:07.132019 15724 net.cpp:411] Convolution12 -> Convolution12
I0211 13:04:07.132695 15724 net.cpp:150] Setting up Convolution12
I0211 13:04:07.132707 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.132714 15724 net.cpp:165] Memory required for data: 2412000028
I0211 13:04:07.132719 15724 layer_factory.hpp:77] Creating layer Dropout11
I0211 13:04:07.132727 15724 net.cpp:106] Creating Layer Dropout11
I0211 13:04:07.132733 15724 net.cpp:454] Dropout11 <- Convolution12
I0211 13:04:07.132742 15724 net.cpp:411] Dropout11 -> Dropout11
I0211 13:04:07.132802 15724 net.cpp:150] Setting up Dropout11
I0211 13:04:07.132814 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.132819 15724 net.cpp:165] Memory required for data: 2413800028
I0211 13:04:07.132824 15724 layer_factory.hpp:77] Creating layer Concat10
I0211 13:04:07.132832 15724 net.cpp:106] Creating Layer Concat10
I0211 13:04:07.132838 15724 net.cpp:454] Concat10 <- Concat9_Concat9_0_split_1
I0211 13:04:07.132843 15724 net.cpp:454] Concat10 <- Dropout11
I0211 13:04:07.132851 15724 net.cpp:411] Concat10 -> Concat10
I0211 13:04:07.132890 15724 net.cpp:150] Setting up Concat10
I0211 13:04:07.132901 15724 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0211 13:04:07.132905 15724 net.cpp:165] Memory required for data: 2434200028
I0211 13:04:07.132910 15724 layer_factory.hpp:77] Creating layer Concat10_Concat10_0_split
I0211 13:04:07.132917 15724 net.cpp:106] Creating Layer Concat10_Concat10_0_split
I0211 13:04:07.132922 15724 net.cpp:454] Concat10_Concat10_0_split <- Concat10
I0211 13:04:07.132931 15724 net.cpp:411] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_0
I0211 13:04:07.132938 15724 net.cpp:411] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_1
I0211 13:04:07.132992 15724 net.cpp:150] Setting up Concat10_Concat10_0_split
I0211 13:04:07.133002 15724 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0211 13:04:07.133007 15724 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0211 13:04:07.133010 15724 net.cpp:165] Memory required for data: 2475000028
I0211 13:04:07.133015 15724 layer_factory.hpp:77] Creating layer BatchNorm12
I0211 13:04:07.133021 15724 net.cpp:106] Creating Layer BatchNorm12
I0211 13:04:07.133026 15724 net.cpp:454] BatchNorm12 <- Concat10_Concat10_0_split_0
I0211 13:04:07.133034 15724 net.cpp:411] BatchNorm12 -> BatchNorm12
I0211 13:04:07.133937 15724 net.cpp:150] Setting up BatchNorm12
I0211 13:04:07.133949 15724 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0211 13:04:07.133954 15724 net.cpp:165] Memory required for data: 2495400028
I0211 13:04:07.133963 15724 layer_factory.hpp:77] Creating layer Scale12
I0211 13:04:07.133973 15724 net.cpp:106] Creating Layer Scale12
I0211 13:04:07.133978 15724 net.cpp:454] Scale12 <- BatchNorm12
I0211 13:04:07.133986 15724 net.cpp:397] Scale12 -> BatchNorm12 (in-place)
I0211 13:04:07.134045 15724 layer_factory.hpp:77] Creating layer Scale12
I0211 13:04:07.134265 15724 net.cpp:150] Setting up Scale12
I0211 13:04:07.134276 15724 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0211 13:04:07.134280 15724 net.cpp:165] Memory required for data: 2515800028
I0211 13:04:07.134289 15724 layer_factory.hpp:77] Creating layer ReLU12
I0211 13:04:07.134295 15724 net.cpp:106] Creating Layer ReLU12
I0211 13:04:07.134300 15724 net.cpp:454] ReLU12 <- BatchNorm12
I0211 13:04:07.134305 15724 net.cpp:397] ReLU12 -> BatchNorm12 (in-place)
I0211 13:04:07.134312 15724 net.cpp:150] Setting up ReLU12
I0211 13:04:07.134318 15724 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0211 13:04:07.134322 15724 net.cpp:165] Memory required for data: 2536200028
I0211 13:04:07.134326 15724 layer_factory.hpp:77] Creating layer Convolution13
I0211 13:04:07.134335 15724 net.cpp:106] Creating Layer Convolution13
I0211 13:04:07.134341 15724 net.cpp:454] Convolution13 <- BatchNorm12
I0211 13:04:07.134349 15724 net.cpp:411] Convolution13 -> Convolution13
I0211 13:04:07.135118 15724 net.cpp:150] Setting up Convolution13
I0211 13:04:07.135131 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.135136 15724 net.cpp:165] Memory required for data: 2538000028
I0211 13:04:07.135143 15724 layer_factory.hpp:77] Creating layer Dropout12
I0211 13:04:07.135150 15724 net.cpp:106] Creating Layer Dropout12
I0211 13:04:07.135155 15724 net.cpp:454] Dropout12 <- Convolution13
I0211 13:04:07.135161 15724 net.cpp:411] Dropout12 -> Dropout12
I0211 13:04:07.135218 15724 net.cpp:150] Setting up Dropout12
I0211 13:04:07.135228 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.135232 15724 net.cpp:165] Memory required for data: 2539800028
I0211 13:04:07.135237 15724 layer_factory.hpp:77] Creating layer Concat11
I0211 13:04:07.135246 15724 net.cpp:106] Creating Layer Concat11
I0211 13:04:07.135251 15724 net.cpp:454] Concat11 <- Concat10_Concat10_0_split_1
I0211 13:04:07.135257 15724 net.cpp:454] Concat11 <- Dropout12
I0211 13:04:07.135262 15724 net.cpp:411] Concat11 -> Concat11
I0211 13:04:07.135298 15724 net.cpp:150] Setting up Concat11
I0211 13:04:07.135308 15724 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0211 13:04:07.135313 15724 net.cpp:165] Memory required for data: 2562000028
I0211 13:04:07.135318 15724 layer_factory.hpp:77] Creating layer Concat11_Concat11_0_split
I0211 13:04:07.135324 15724 net.cpp:106] Creating Layer Concat11_Concat11_0_split
I0211 13:04:07.135329 15724 net.cpp:454] Concat11_Concat11_0_split <- Concat11
I0211 13:04:07.135334 15724 net.cpp:411] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_0
I0211 13:04:07.135341 15724 net.cpp:411] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_1
I0211 13:04:07.135395 15724 net.cpp:150] Setting up Concat11_Concat11_0_split
I0211 13:04:07.135404 15724 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0211 13:04:07.135409 15724 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0211 13:04:07.135414 15724 net.cpp:165] Memory required for data: 2606400028
I0211 13:04:07.135418 15724 layer_factory.hpp:77] Creating layer BatchNorm13
I0211 13:04:07.135427 15724 net.cpp:106] Creating Layer BatchNorm13
I0211 13:04:07.135432 15724 net.cpp:454] BatchNorm13 <- Concat11_Concat11_0_split_0
I0211 13:04:07.135438 15724 net.cpp:411] BatchNorm13 -> BatchNorm13
I0211 13:04:07.135768 15724 net.cpp:150] Setting up BatchNorm13
I0211 13:04:07.135778 15724 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0211 13:04:07.135783 15724 net.cpp:165] Memory required for data: 2628600028
I0211 13:04:07.135792 15724 layer_factory.hpp:77] Creating layer Scale13
I0211 13:04:07.135802 15724 net.cpp:106] Creating Layer Scale13
I0211 13:04:07.135807 15724 net.cpp:454] Scale13 <- BatchNorm13
I0211 13:04:07.135815 15724 net.cpp:397] Scale13 -> BatchNorm13 (in-place)
I0211 13:04:07.135875 15724 layer_factory.hpp:77] Creating layer Scale13
I0211 13:04:07.136098 15724 net.cpp:150] Setting up Scale13
I0211 13:04:07.136108 15724 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0211 13:04:07.136113 15724 net.cpp:165] Memory required for data: 2650800028
I0211 13:04:07.136121 15724 layer_factory.hpp:77] Creating layer ReLU13
I0211 13:04:07.136128 15724 net.cpp:106] Creating Layer ReLU13
I0211 13:04:07.136132 15724 net.cpp:454] ReLU13 <- BatchNorm13
I0211 13:04:07.136138 15724 net.cpp:397] ReLU13 -> BatchNorm13 (in-place)
I0211 13:04:07.136145 15724 net.cpp:150] Setting up ReLU13
I0211 13:04:07.136152 15724 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0211 13:04:07.136155 15724 net.cpp:165] Memory required for data: 2673000028
I0211 13:04:07.136159 15724 layer_factory.hpp:77] Creating layer Convolution14
I0211 13:04:07.136173 15724 net.cpp:106] Creating Layer Convolution14
I0211 13:04:07.136178 15724 net.cpp:454] Convolution14 <- BatchNorm13
I0211 13:04:07.136185 15724 net.cpp:411] Convolution14 -> Convolution14
I0211 13:04:07.136942 15724 net.cpp:150] Setting up Convolution14
I0211 13:04:07.136952 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.136957 15724 net.cpp:165] Memory required for data: 2674800028
I0211 13:04:07.136963 15724 layer_factory.hpp:77] Creating layer Dropout13
I0211 13:04:07.136970 15724 net.cpp:106] Creating Layer Dropout13
I0211 13:04:07.136976 15724 net.cpp:454] Dropout13 <- Convolution14
I0211 13:04:07.136981 15724 net.cpp:411] Dropout13 -> Dropout13
I0211 13:04:07.137039 15724 net.cpp:150] Setting up Dropout13
I0211 13:04:07.137051 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.137055 15724 net.cpp:165] Memory required for data: 2676600028
I0211 13:04:07.137060 15724 layer_factory.hpp:77] Creating layer Concat12
I0211 13:04:07.137068 15724 net.cpp:106] Creating Layer Concat12
I0211 13:04:07.137071 15724 net.cpp:454] Concat12 <- Concat11_Concat11_0_split_1
I0211 13:04:07.137078 15724 net.cpp:454] Concat12 <- Dropout13
I0211 13:04:07.137084 15724 net.cpp:411] Concat12 -> Concat12
I0211 13:04:07.137120 15724 net.cpp:150] Setting up Concat12
I0211 13:04:07.137128 15724 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0211 13:04:07.137132 15724 net.cpp:165] Memory required for data: 2700600028
I0211 13:04:07.137137 15724 layer_factory.hpp:77] Creating layer Concat12_Concat12_0_split
I0211 13:04:07.137143 15724 net.cpp:106] Creating Layer Concat12_Concat12_0_split
I0211 13:04:07.137148 15724 net.cpp:454] Concat12_Concat12_0_split <- Concat12
I0211 13:04:07.137154 15724 net.cpp:411] Concat12_Concat12_0_split -> Concat12_Concat12_0_split_0
I0211 13:04:07.137161 15724 net.cpp:411] Concat12_Concat12_0_split -> Concat12_Concat12_0_split_1
I0211 13:04:07.137214 15724 net.cpp:150] Setting up Concat12_Concat12_0_split
I0211 13:04:07.137225 15724 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0211 13:04:07.137231 15724 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0211 13:04:07.137236 15724 net.cpp:165] Memory required for data: 2748600028
I0211 13:04:07.137240 15724 layer_factory.hpp:77] Creating layer BatchNorm14
I0211 13:04:07.137248 15724 net.cpp:106] Creating Layer BatchNorm14
I0211 13:04:07.137251 15724 net.cpp:454] BatchNorm14 <- Concat12_Concat12_0_split_0
I0211 13:04:07.137259 15724 net.cpp:411] BatchNorm14 -> BatchNorm14
I0211 13:04:07.138175 15724 net.cpp:150] Setting up BatchNorm14
I0211 13:04:07.138187 15724 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0211 13:04:07.138192 15724 net.cpp:165] Memory required for data: 2772600028
I0211 13:04:07.138201 15724 layer_factory.hpp:77] Creating layer Scale14
I0211 13:04:07.138209 15724 net.cpp:106] Creating Layer Scale14
I0211 13:04:07.138214 15724 net.cpp:454] Scale14 <- BatchNorm14
I0211 13:04:07.138222 15724 net.cpp:397] Scale14 -> BatchNorm14 (in-place)
I0211 13:04:07.138286 15724 layer_factory.hpp:77] Creating layer Scale14
I0211 13:04:07.138509 15724 net.cpp:150] Setting up Scale14
I0211 13:04:07.138520 15724 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0211 13:04:07.138525 15724 net.cpp:165] Memory required for data: 2796600028
I0211 13:04:07.138532 15724 layer_factory.hpp:77] Creating layer ReLU14
I0211 13:04:07.138542 15724 net.cpp:106] Creating Layer ReLU14
I0211 13:04:07.138547 15724 net.cpp:454] ReLU14 <- BatchNorm14
I0211 13:04:07.138553 15724 net.cpp:397] ReLU14 -> BatchNorm14 (in-place)
I0211 13:04:07.138566 15724 net.cpp:150] Setting up ReLU14
I0211 13:04:07.138573 15724 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0211 13:04:07.138577 15724 net.cpp:165] Memory required for data: 2820600028
I0211 13:04:07.138581 15724 layer_factory.hpp:77] Creating layer Convolution15
I0211 13:04:07.138592 15724 net.cpp:106] Creating Layer Convolution15
I0211 13:04:07.138595 15724 net.cpp:454] Convolution15 <- BatchNorm14
I0211 13:04:07.138605 15724 net.cpp:411] Convolution15 -> Convolution15
I0211 13:04:07.139389 15724 net.cpp:150] Setting up Convolution15
I0211 13:04:07.139400 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.139405 15724 net.cpp:165] Memory required for data: 2822400028
I0211 13:04:07.139411 15724 layer_factory.hpp:77] Creating layer Dropout14
I0211 13:04:07.139420 15724 net.cpp:106] Creating Layer Dropout14
I0211 13:04:07.139426 15724 net.cpp:454] Dropout14 <- Convolution15
I0211 13:04:07.139432 15724 net.cpp:411] Dropout14 -> Dropout14
I0211 13:04:07.139493 15724 net.cpp:150] Setting up Dropout14
I0211 13:04:07.139503 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.139508 15724 net.cpp:165] Memory required for data: 2824200028
I0211 13:04:07.139513 15724 layer_factory.hpp:77] Creating layer Concat13
I0211 13:04:07.139518 15724 net.cpp:106] Creating Layer Concat13
I0211 13:04:07.139523 15724 net.cpp:454] Concat13 <- Concat12_Concat12_0_split_1
I0211 13:04:07.139529 15724 net.cpp:454] Concat13 <- Dropout14
I0211 13:04:07.139538 15724 net.cpp:411] Concat13 -> Concat13
I0211 13:04:07.139575 15724 net.cpp:150] Setting up Concat13
I0211 13:04:07.139583 15724 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0211 13:04:07.139587 15724 net.cpp:165] Memory required for data: 2850000028
I0211 13:04:07.139592 15724 layer_factory.hpp:77] Creating layer Concat13_Concat13_0_split
I0211 13:04:07.139603 15724 net.cpp:106] Creating Layer Concat13_Concat13_0_split
I0211 13:04:07.139607 15724 net.cpp:454] Concat13_Concat13_0_split <- Concat13
I0211 13:04:07.139613 15724 net.cpp:411] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_0
I0211 13:04:07.139621 15724 net.cpp:411] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_1
I0211 13:04:07.139674 15724 net.cpp:150] Setting up Concat13_Concat13_0_split
I0211 13:04:07.139683 15724 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0211 13:04:07.139688 15724 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0211 13:04:07.139693 15724 net.cpp:165] Memory required for data: 2901600028
I0211 13:04:07.139698 15724 layer_factory.hpp:77] Creating layer BatchNorm15
I0211 13:04:07.139706 15724 net.cpp:106] Creating Layer BatchNorm15
I0211 13:04:07.139711 15724 net.cpp:454] BatchNorm15 <- Concat13_Concat13_0_split_0
I0211 13:04:07.139719 15724 net.cpp:411] BatchNorm15 -> BatchNorm15
I0211 13:04:07.140043 15724 net.cpp:150] Setting up BatchNorm15
I0211 13:04:07.140055 15724 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0211 13:04:07.140059 15724 net.cpp:165] Memory required for data: 2927400028
I0211 13:04:07.140069 15724 layer_factory.hpp:77] Creating layer Scale15
I0211 13:04:07.140076 15724 net.cpp:106] Creating Layer Scale15
I0211 13:04:07.140081 15724 net.cpp:454] Scale15 <- BatchNorm15
I0211 13:04:07.140090 15724 net.cpp:397] Scale15 -> BatchNorm15 (in-place)
I0211 13:04:07.140151 15724 layer_factory.hpp:77] Creating layer Scale15
I0211 13:04:07.140372 15724 net.cpp:150] Setting up Scale15
I0211 13:04:07.140383 15724 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0211 13:04:07.140388 15724 net.cpp:165] Memory required for data: 2953200028
I0211 13:04:07.140395 15724 layer_factory.hpp:77] Creating layer ReLU15
I0211 13:04:07.140403 15724 net.cpp:106] Creating Layer ReLU15
I0211 13:04:07.140408 15724 net.cpp:454] ReLU15 <- BatchNorm15
I0211 13:04:07.140415 15724 net.cpp:397] ReLU15 -> BatchNorm15 (in-place)
I0211 13:04:07.140422 15724 net.cpp:150] Setting up ReLU15
I0211 13:04:07.140429 15724 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0211 13:04:07.140432 15724 net.cpp:165] Memory required for data: 2979000028
I0211 13:04:07.140436 15724 layer_factory.hpp:77] Creating layer Convolution16
I0211 13:04:07.140445 15724 net.cpp:106] Creating Layer Convolution16
I0211 13:04:07.140450 15724 net.cpp:454] Convolution16 <- BatchNorm15
I0211 13:04:07.140456 15724 net.cpp:411] Convolution16 -> Convolution16
I0211 13:04:07.141288 15724 net.cpp:150] Setting up Convolution16
I0211 13:04:07.141297 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.141302 15724 net.cpp:165] Memory required for data: 2980800028
I0211 13:04:07.141309 15724 layer_factory.hpp:77] Creating layer Dropout15
I0211 13:04:07.141315 15724 net.cpp:106] Creating Layer Dropout15
I0211 13:04:07.141320 15724 net.cpp:454] Dropout15 <- Convolution16
I0211 13:04:07.141330 15724 net.cpp:411] Dropout15 -> Dropout15
I0211 13:04:07.141387 15724 net.cpp:150] Setting up Dropout15
I0211 13:04:07.141396 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.141400 15724 net.cpp:165] Memory required for data: 2982600028
I0211 13:04:07.141405 15724 layer_factory.hpp:77] Creating layer Concat14
I0211 13:04:07.141413 15724 net.cpp:106] Creating Layer Concat14
I0211 13:04:07.141418 15724 net.cpp:454] Concat14 <- Concat13_Concat13_0_split_1
I0211 13:04:07.141424 15724 net.cpp:454] Concat14 <- Dropout15
I0211 13:04:07.141430 15724 net.cpp:411] Concat14 -> Concat14
I0211 13:04:07.141469 15724 net.cpp:150] Setting up Concat14
I0211 13:04:07.141477 15724 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0211 13:04:07.141481 15724 net.cpp:165] Memory required for data: 3010200028
I0211 13:04:07.141485 15724 layer_factory.hpp:77] Creating layer Concat14_Concat14_0_split
I0211 13:04:07.141491 15724 net.cpp:106] Creating Layer Concat14_Concat14_0_split
I0211 13:04:07.141496 15724 net.cpp:454] Concat14_Concat14_0_split <- Concat14
I0211 13:04:07.141505 15724 net.cpp:411] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_0
I0211 13:04:07.141511 15724 net.cpp:411] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_1
I0211 13:04:07.141566 15724 net.cpp:150] Setting up Concat14_Concat14_0_split
I0211 13:04:07.141574 15724 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0211 13:04:07.141579 15724 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0211 13:04:07.141583 15724 net.cpp:165] Memory required for data: 3065400028
I0211 13:04:07.141588 15724 layer_factory.hpp:77] Creating layer BatchNorm16
I0211 13:04:07.141597 15724 net.cpp:106] Creating Layer BatchNorm16
I0211 13:04:07.141602 15724 net.cpp:454] BatchNorm16 <- Concat14_Concat14_0_split_0
I0211 13:04:07.141610 15724 net.cpp:411] BatchNorm16 -> BatchNorm16
I0211 13:04:07.142516 15724 net.cpp:150] Setting up BatchNorm16
I0211 13:04:07.142529 15724 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0211 13:04:07.142534 15724 net.cpp:165] Memory required for data: 3093000028
I0211 13:04:07.142542 15724 layer_factory.hpp:77] Creating layer Scale16
I0211 13:04:07.142552 15724 net.cpp:106] Creating Layer Scale16
I0211 13:04:07.142563 15724 net.cpp:454] Scale16 <- BatchNorm16
I0211 13:04:07.142571 15724 net.cpp:397] Scale16 -> BatchNorm16 (in-place)
I0211 13:04:07.142633 15724 layer_factory.hpp:77] Creating layer Scale16
I0211 13:04:07.142866 15724 net.cpp:150] Setting up Scale16
I0211 13:04:07.142877 15724 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0211 13:04:07.142881 15724 net.cpp:165] Memory required for data: 3120600028
I0211 13:04:07.142889 15724 layer_factory.hpp:77] Creating layer ReLU16
I0211 13:04:07.142896 15724 net.cpp:106] Creating Layer ReLU16
I0211 13:04:07.142901 15724 net.cpp:454] ReLU16 <- BatchNorm16
I0211 13:04:07.142907 15724 net.cpp:397] ReLU16 -> BatchNorm16 (in-place)
I0211 13:04:07.142915 15724 net.cpp:150] Setting up ReLU16
I0211 13:04:07.142920 15724 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0211 13:04:07.142923 15724 net.cpp:165] Memory required for data: 3148200028
I0211 13:04:07.142928 15724 layer_factory.hpp:77] Creating layer Convolution17
I0211 13:04:07.142940 15724 net.cpp:106] Creating Layer Convolution17
I0211 13:04:07.142945 15724 net.cpp:454] Convolution17 <- BatchNorm16
I0211 13:04:07.142951 15724 net.cpp:411] Convolution17 -> Convolution17
I0211 13:04:07.143817 15724 net.cpp:150] Setting up Convolution17
I0211 13:04:07.143827 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.143832 15724 net.cpp:165] Memory required for data: 3150000028
I0211 13:04:07.143838 15724 layer_factory.hpp:77] Creating layer Dropout16
I0211 13:04:07.143847 15724 net.cpp:106] Creating Layer Dropout16
I0211 13:04:07.143852 15724 net.cpp:454] Dropout16 <- Convolution17
I0211 13:04:07.143859 15724 net.cpp:411] Dropout16 -> Dropout16
I0211 13:04:07.143920 15724 net.cpp:150] Setting up Dropout16
I0211 13:04:07.143929 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.143934 15724 net.cpp:165] Memory required for data: 3151800028
I0211 13:04:07.143939 15724 layer_factory.hpp:77] Creating layer Concat15
I0211 13:04:07.143946 15724 net.cpp:106] Creating Layer Concat15
I0211 13:04:07.143950 15724 net.cpp:454] Concat15 <- Concat14_Concat14_0_split_1
I0211 13:04:07.143956 15724 net.cpp:454] Concat15 <- Dropout16
I0211 13:04:07.143965 15724 net.cpp:411] Concat15 -> Concat15
I0211 13:04:07.144001 15724 net.cpp:150] Setting up Concat15
I0211 13:04:07.144009 15724 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0211 13:04:07.144014 15724 net.cpp:165] Memory required for data: 3181200028
I0211 13:04:07.144018 15724 layer_factory.hpp:77] Creating layer Concat15_Concat15_0_split
I0211 13:04:07.144026 15724 net.cpp:106] Creating Layer Concat15_Concat15_0_split
I0211 13:04:07.144032 15724 net.cpp:454] Concat15_Concat15_0_split <- Concat15
I0211 13:04:07.144037 15724 net.cpp:411] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_0
I0211 13:04:07.144044 15724 net.cpp:411] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_1
I0211 13:04:07.144100 15724 net.cpp:150] Setting up Concat15_Concat15_0_split
I0211 13:04:07.144109 15724 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0211 13:04:07.144115 15724 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0211 13:04:07.144119 15724 net.cpp:165] Memory required for data: 3240000028
I0211 13:04:07.144124 15724 layer_factory.hpp:77] Creating layer BatchNorm17
I0211 13:04:07.144132 15724 net.cpp:106] Creating Layer BatchNorm17
I0211 13:04:07.144137 15724 net.cpp:454] BatchNorm17 <- Concat15_Concat15_0_split_0
I0211 13:04:07.144143 15724 net.cpp:411] BatchNorm17 -> BatchNorm17
I0211 13:04:07.144556 15724 net.cpp:150] Setting up BatchNorm17
I0211 13:04:07.144567 15724 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0211 13:04:07.144572 15724 net.cpp:165] Memory required for data: 3269400028
I0211 13:04:07.144582 15724 layer_factory.hpp:77] Creating layer Scale17
I0211 13:04:07.144593 15724 net.cpp:106] Creating Layer Scale17
I0211 13:04:07.144598 15724 net.cpp:454] Scale17 <- BatchNorm17
I0211 13:04:07.144605 15724 net.cpp:397] Scale17 -> BatchNorm17 (in-place)
I0211 13:04:07.144665 15724 layer_factory.hpp:77] Creating layer Scale17
I0211 13:04:07.144893 15724 net.cpp:150] Setting up Scale17
I0211 13:04:07.144904 15724 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0211 13:04:07.144909 15724 net.cpp:165] Memory required for data: 3298800028
I0211 13:04:07.144917 15724 layer_factory.hpp:77] Creating layer ReLU17
I0211 13:04:07.144923 15724 net.cpp:106] Creating Layer ReLU17
I0211 13:04:07.144928 15724 net.cpp:454] ReLU17 <- BatchNorm17
I0211 13:04:07.144934 15724 net.cpp:397] ReLU17 -> BatchNorm17 (in-place)
I0211 13:04:07.144942 15724 net.cpp:150] Setting up ReLU17
I0211 13:04:07.144948 15724 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0211 13:04:07.144951 15724 net.cpp:165] Memory required for data: 3328200028
I0211 13:04:07.144955 15724 layer_factory.hpp:77] Creating layer Convolution18
I0211 13:04:07.144968 15724 net.cpp:106] Creating Layer Convolution18
I0211 13:04:07.144971 15724 net.cpp:454] Convolution18 <- BatchNorm17
I0211 13:04:07.144979 15724 net.cpp:411] Convolution18 -> Convolution18
I0211 13:04:07.145887 15724 net.cpp:150] Setting up Convolution18
I0211 13:04:07.145898 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.145902 15724 net.cpp:165] Memory required for data: 3330000028
I0211 13:04:07.145908 15724 layer_factory.hpp:77] Creating layer Dropout17
I0211 13:04:07.145917 15724 net.cpp:106] Creating Layer Dropout17
I0211 13:04:07.145922 15724 net.cpp:454] Dropout17 <- Convolution18
I0211 13:04:07.145930 15724 net.cpp:411] Dropout17 -> Dropout17
I0211 13:04:07.145989 15724 net.cpp:150] Setting up Dropout17
I0211 13:04:07.145998 15724 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0211 13:04:07.146003 15724 net.cpp:165] Memory required for data: 3331800028
I0211 13:04:07.146008 15724 layer_factory.hpp:77] Creating layer Concat16
I0211 13:04:07.146013 15724 net.cpp:106] Creating Layer Concat16
I0211 13:04:07.146018 15724 net.cpp:454] Concat16 <- Concat15_Concat15_0_split_1
I0211 13:04:07.146024 15724 net.cpp:454] Concat16 <- Dropout17
I0211 13:04:07.146033 15724 net.cpp:411] Concat16 -> Concat16
I0211 13:04:07.146069 15724 net.cpp:150] Setting up Concat16
I0211 13:04:07.146077 15724 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0211 13:04:07.146082 15724 net.cpp:165] Memory required for data: 3363000028
I0211 13:04:07.146086 15724 layer_factory.hpp:77] Creating layer BatchNorm18
I0211 13:04:07.146095 15724 net.cpp:106] Creating Layer BatchNorm18
I0211 13:04:07.146100 15724 net.cpp:454] BatchNorm18 <- Concat16
I0211 13:04:07.146108 15724 net.cpp:411] BatchNorm18 -> BatchNorm18
I0211 13:04:07.147022 15724 net.cpp:150] Setting up BatchNorm18
I0211 13:04:07.147035 15724 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0211 13:04:07.147040 15724 net.cpp:165] Memory required for data: 3394200028
I0211 13:04:07.147049 15724 layer_factory.hpp:77] Creating layer Scale18
I0211 13:04:07.147060 15724 net.cpp:106] Creating Layer Scale18
I0211 13:04:07.147065 15724 net.cpp:454] Scale18 <- BatchNorm18
I0211 13:04:07.147073 15724 net.cpp:397] Scale18 -> BatchNorm18 (in-place)
I0211 13:04:07.147133 15724 layer_factory.hpp:77] Creating layer Scale18
I0211 13:04:07.147362 15724 net.cpp:150] Setting up Scale18
I0211 13:04:07.147373 15724 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0211 13:04:07.147378 15724 net.cpp:165] Memory required for data: 3425400028
I0211 13:04:07.147385 15724 layer_factory.hpp:77] Creating layer ReLU18
I0211 13:04:07.147393 15724 net.cpp:106] Creating Layer ReLU18
I0211 13:04:07.147398 15724 net.cpp:454] ReLU18 <- BatchNorm18
I0211 13:04:07.147411 15724 net.cpp:397] ReLU18 -> BatchNorm18 (in-place)
I0211 13:04:07.147418 15724 net.cpp:150] Setting up ReLU18
I0211 13:04:07.147425 15724 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0211 13:04:07.147429 15724 net.cpp:165] Memory required for data: 3456600028
I0211 13:04:07.147433 15724 layer_factory.hpp:77] Creating layer Convolution19
I0211 13:04:07.147444 15724 net.cpp:106] Creating Layer Convolution19
I0211 13:04:07.147450 15724 net.cpp:454] Convolution19 <- BatchNorm18
I0211 13:04:07.147456 15724 net.cpp:411] Convolution19 -> Convolution19
I0211 13:04:07.148975 15724 net.cpp:150] Setting up Convolution19
I0211 13:04:07.148986 15724 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0211 13:04:07.148990 15724 net.cpp:165] Memory required for data: 3487800028
I0211 13:04:07.148996 15724 layer_factory.hpp:77] Creating layer Dropout18
I0211 13:04:07.149003 15724 net.cpp:106] Creating Layer Dropout18
I0211 13:04:07.149008 15724 net.cpp:454] Dropout18 <- Convolution19
I0211 13:04:07.149018 15724 net.cpp:411] Dropout18 -> Dropout18
I0211 13:04:07.149077 15724 net.cpp:150] Setting up Dropout18
I0211 13:04:07.149087 15724 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0211 13:04:07.149091 15724 net.cpp:165] Memory required for data: 3519000028
I0211 13:04:07.149096 15724 layer_factory.hpp:77] Creating layer Pooling2
I0211 13:04:07.149102 15724 net.cpp:106] Creating Layer Pooling2
I0211 13:04:07.149107 15724 net.cpp:454] Pooling2 <- Dropout18
I0211 13:04:07.149116 15724 net.cpp:411] Pooling2 -> Pooling2
I0211 13:04:07.149152 15724 net.cpp:150] Setting up Pooling2
I0211 13:04:07.149160 15724 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0211 13:04:07.149164 15724 net.cpp:165] Memory required for data: 3526800028
I0211 13:04:07.149168 15724 layer_factory.hpp:77] Creating layer Pooling2_Pooling2_0_split
I0211 13:04:07.149175 15724 net.cpp:106] Creating Layer Pooling2_Pooling2_0_split
I0211 13:04:07.149179 15724 net.cpp:454] Pooling2_Pooling2_0_split <- Pooling2
I0211 13:04:07.149188 15724 net.cpp:411] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_0
I0211 13:04:07.149194 15724 net.cpp:411] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_1
I0211 13:04:07.149248 15724 net.cpp:150] Setting up Pooling2_Pooling2_0_split
I0211 13:04:07.149257 15724 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0211 13:04:07.149262 15724 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0211 13:04:07.149266 15724 net.cpp:165] Memory required for data: 3542400028
I0211 13:04:07.149271 15724 layer_factory.hpp:77] Creating layer BatchNorm19
I0211 13:04:07.149287 15724 net.cpp:106] Creating Layer BatchNorm19
I0211 13:04:07.149292 15724 net.cpp:454] BatchNorm19 <- Pooling2_Pooling2_0_split_0
I0211 13:04:07.149300 15724 net.cpp:411] BatchNorm19 -> BatchNorm19
I0211 13:04:07.149616 15724 net.cpp:150] Setting up BatchNorm19
I0211 13:04:07.149626 15724 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0211 13:04:07.149629 15724 net.cpp:165] Memory required for data: 3550200028
I0211 13:04:07.149641 15724 layer_factory.hpp:77] Creating layer Scale19
I0211 13:04:07.149648 15724 net.cpp:106] Creating Layer Scale19
I0211 13:04:07.149653 15724 net.cpp:454] Scale19 <- BatchNorm19
I0211 13:04:07.149660 15724 net.cpp:397] Scale19 -> BatchNorm19 (in-place)
I0211 13:04:07.149720 15724 layer_factory.hpp:77] Creating layer Scale19
I0211 13:04:07.149914 15724 net.cpp:150] Setting up Scale19
I0211 13:04:07.149924 15724 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0211 13:04:07.149929 15724 net.cpp:165] Memory required for data: 3558000028
I0211 13:04:07.149935 15724 layer_factory.hpp:77] Creating layer ReLU19
I0211 13:04:07.149942 15724 net.cpp:106] Creating Layer ReLU19
I0211 13:04:07.149947 15724 net.cpp:454] ReLU19 <- BatchNorm19
I0211 13:04:07.149952 15724 net.cpp:397] ReLU19 -> BatchNorm19 (in-place)
I0211 13:04:07.149960 15724 net.cpp:150] Setting up ReLU19
I0211 13:04:07.149966 15724 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0211 13:04:07.149968 15724 net.cpp:165] Memory required for data: 3565800028
I0211 13:04:07.149973 15724 layer_factory.hpp:77] Creating layer Convolution20
I0211 13:04:07.149984 15724 net.cpp:106] Creating Layer Convolution20
I0211 13:04:07.149991 15724 net.cpp:454] Convolution20 <- BatchNorm19
I0211 13:04:07.149997 15724 net.cpp:411] Convolution20 -> Convolution20
I0211 13:04:07.150945 15724 net.cpp:150] Setting up Convolution20
I0211 13:04:07.150956 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.150960 15724 net.cpp:165] Memory required for data: 3566250028
I0211 13:04:07.150967 15724 layer_factory.hpp:77] Creating layer Dropout19
I0211 13:04:07.150976 15724 net.cpp:106] Creating Layer Dropout19
I0211 13:04:07.150982 15724 net.cpp:454] Dropout19 <- Convolution20
I0211 13:04:07.150988 15724 net.cpp:411] Dropout19 -> Dropout19
I0211 13:04:07.151048 15724 net.cpp:150] Setting up Dropout19
I0211 13:04:07.151057 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.151062 15724 net.cpp:165] Memory required for data: 3566700028
I0211 13:04:07.151067 15724 layer_factory.hpp:77] Creating layer Concat17
I0211 13:04:07.151074 15724 net.cpp:106] Creating Layer Concat17
I0211 13:04:07.151079 15724 net.cpp:454] Concat17 <- Pooling2_Pooling2_0_split_1
I0211 13:04:07.151085 15724 net.cpp:454] Concat17 <- Dropout19
I0211 13:04:07.151091 15724 net.cpp:411] Concat17 -> Concat17
I0211 13:04:07.151129 15724 net.cpp:150] Setting up Concat17
I0211 13:04:07.151137 15724 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0211 13:04:07.151141 15724 net.cpp:165] Memory required for data: 3574950028
I0211 13:04:07.151145 15724 layer_factory.hpp:77] Creating layer Concat17_Concat17_0_split
I0211 13:04:07.151154 15724 net.cpp:106] Creating Layer Concat17_Concat17_0_split
I0211 13:04:07.151160 15724 net.cpp:454] Concat17_Concat17_0_split <- Concat17
I0211 13:04:07.151165 15724 net.cpp:411] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_0
I0211 13:04:07.151173 15724 net.cpp:411] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_1
I0211 13:04:07.151227 15724 net.cpp:150] Setting up Concat17_Concat17_0_split
I0211 13:04:07.151237 15724 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0211 13:04:07.151242 15724 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0211 13:04:07.151247 15724 net.cpp:165] Memory required for data: 3591450028
I0211 13:04:07.151250 15724 layer_factory.hpp:77] Creating layer BatchNorm20
I0211 13:04:07.151259 15724 net.cpp:106] Creating Layer BatchNorm20
I0211 13:04:07.151264 15724 net.cpp:454] BatchNorm20 <- Concat17_Concat17_0_split_0
I0211 13:04:07.151270 15724 net.cpp:411] BatchNorm20 -> BatchNorm20
I0211 13:04:07.151619 15724 net.cpp:150] Setting up BatchNorm20
I0211 13:04:07.151629 15724 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0211 13:04:07.151633 15724 net.cpp:165] Memory required for data: 3599700028
I0211 13:04:07.151641 15724 layer_factory.hpp:77] Creating layer Scale20
I0211 13:04:07.151651 15724 net.cpp:106] Creating Layer Scale20
I0211 13:04:07.151656 15724 net.cpp:454] Scale20 <- BatchNorm20
I0211 13:04:07.151662 15724 net.cpp:397] Scale20 -> BatchNorm20 (in-place)
I0211 13:04:07.151724 15724 layer_factory.hpp:77] Creating layer Scale20
I0211 13:04:07.151922 15724 net.cpp:150] Setting up Scale20
I0211 13:04:07.151934 15724 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0211 13:04:07.151939 15724 net.cpp:165] Memory required for data: 3607950028
I0211 13:04:07.151947 15724 layer_factory.hpp:77] Creating layer ReLU20
I0211 13:04:07.151953 15724 net.cpp:106] Creating Layer ReLU20
I0211 13:04:07.151958 15724 net.cpp:454] ReLU20 <- BatchNorm20
I0211 13:04:07.151963 15724 net.cpp:397] ReLU20 -> BatchNorm20 (in-place)
I0211 13:04:07.151970 15724 net.cpp:150] Setting up ReLU20
I0211 13:04:07.151976 15724 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0211 13:04:07.151980 15724 net.cpp:165] Memory required for data: 3616200028
I0211 13:04:07.151984 15724 layer_factory.hpp:77] Creating layer Convolution21
I0211 13:04:07.151993 15724 net.cpp:106] Creating Layer Convolution21
I0211 13:04:07.151996 15724 net.cpp:454] Convolution21 <- BatchNorm20
I0211 13:04:07.152005 15724 net.cpp:411] Convolution21 -> Convolution21
I0211 13:04:07.153000 15724 net.cpp:150] Setting up Convolution21
I0211 13:04:07.153013 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.153017 15724 net.cpp:165] Memory required for data: 3616650028
I0211 13:04:07.153023 15724 layer_factory.hpp:77] Creating layer Dropout20
I0211 13:04:07.153030 15724 net.cpp:106] Creating Layer Dropout20
I0211 13:04:07.153035 15724 net.cpp:454] Dropout20 <- Convolution21
I0211 13:04:07.153041 15724 net.cpp:411] Dropout20 -> Dropout20
I0211 13:04:07.153101 15724 net.cpp:150] Setting up Dropout20
I0211 13:04:07.153110 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.153115 15724 net.cpp:165] Memory required for data: 3617100028
I0211 13:04:07.153120 15724 layer_factory.hpp:77] Creating layer Concat18
I0211 13:04:07.153131 15724 net.cpp:106] Creating Layer Concat18
I0211 13:04:07.153136 15724 net.cpp:454] Concat18 <- Concat17_Concat17_0_split_1
I0211 13:04:07.153141 15724 net.cpp:454] Concat18 <- Dropout20
I0211 13:04:07.153147 15724 net.cpp:411] Concat18 -> Concat18
I0211 13:04:07.153183 15724 net.cpp:150] Setting up Concat18
I0211 13:04:07.153193 15724 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0211 13:04:07.153198 15724 net.cpp:165] Memory required for data: 3625800028
I0211 13:04:07.153203 15724 layer_factory.hpp:77] Creating layer Concat18_Concat18_0_split
I0211 13:04:07.153209 15724 net.cpp:106] Creating Layer Concat18_Concat18_0_split
I0211 13:04:07.153213 15724 net.cpp:454] Concat18_Concat18_0_split <- Concat18
I0211 13:04:07.153219 15724 net.cpp:411] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_0
I0211 13:04:07.153226 15724 net.cpp:411] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_1
I0211 13:04:07.153281 15724 net.cpp:150] Setting up Concat18_Concat18_0_split
I0211 13:04:07.153290 15724 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0211 13:04:07.153295 15724 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0211 13:04:07.153300 15724 net.cpp:165] Memory required for data: 3643200028
I0211 13:04:07.153304 15724 layer_factory.hpp:77] Creating layer BatchNorm21
I0211 13:04:07.153313 15724 net.cpp:106] Creating Layer BatchNorm21
I0211 13:04:07.153317 15724 net.cpp:454] BatchNorm21 <- Concat18_Concat18_0_split_0
I0211 13:04:07.153324 15724 net.cpp:411] BatchNorm21 -> BatchNorm21
I0211 13:04:07.153667 15724 net.cpp:150] Setting up BatchNorm21
I0211 13:04:07.153677 15724 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0211 13:04:07.153681 15724 net.cpp:165] Memory required for data: 3651900028
I0211 13:04:07.153690 15724 layer_factory.hpp:77] Creating layer Scale21
I0211 13:04:07.153699 15724 net.cpp:106] Creating Layer Scale21
I0211 13:04:07.153705 15724 net.cpp:454] Scale21 <- BatchNorm21
I0211 13:04:07.153712 15724 net.cpp:397] Scale21 -> BatchNorm21 (in-place)
I0211 13:04:07.153774 15724 layer_factory.hpp:77] Creating layer Scale21
I0211 13:04:07.154150 15724 net.cpp:150] Setting up Scale21
I0211 13:04:07.154161 15724 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0211 13:04:07.154166 15724 net.cpp:165] Memory required for data: 3660600028
I0211 13:04:07.154175 15724 layer_factory.hpp:77] Creating layer ReLU21
I0211 13:04:07.154181 15724 net.cpp:106] Creating Layer ReLU21
I0211 13:04:07.154186 15724 net.cpp:454] ReLU21 <- BatchNorm21
I0211 13:04:07.154192 15724 net.cpp:397] ReLU21 -> BatchNorm21 (in-place)
I0211 13:04:07.154201 15724 net.cpp:150] Setting up ReLU21
I0211 13:04:07.154206 15724 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0211 13:04:07.154209 15724 net.cpp:165] Memory required for data: 3669300028
I0211 13:04:07.154214 15724 layer_factory.hpp:77] Creating layer Convolution22
I0211 13:04:07.154225 15724 net.cpp:106] Creating Layer Convolution22
I0211 13:04:07.154230 15724 net.cpp:454] Convolution22 <- BatchNorm21
I0211 13:04:07.154237 15724 net.cpp:411] Convolution22 -> Convolution22
I0211 13:04:07.155270 15724 net.cpp:150] Setting up Convolution22
I0211 13:04:07.155282 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.155285 15724 net.cpp:165] Memory required for data: 3669750028
I0211 13:04:07.155292 15724 layer_factory.hpp:77] Creating layer Dropout21
I0211 13:04:07.155299 15724 net.cpp:106] Creating Layer Dropout21
I0211 13:04:07.155304 15724 net.cpp:454] Dropout21 <- Convolution22
I0211 13:04:07.155310 15724 net.cpp:411] Dropout21 -> Dropout21
I0211 13:04:07.155370 15724 net.cpp:150] Setting up Dropout21
I0211 13:04:07.155380 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.155385 15724 net.cpp:165] Memory required for data: 3670200028
I0211 13:04:07.155390 15724 layer_factory.hpp:77] Creating layer Concat19
I0211 13:04:07.155397 15724 net.cpp:106] Creating Layer Concat19
I0211 13:04:07.155402 15724 net.cpp:454] Concat19 <- Concat18_Concat18_0_split_1
I0211 13:04:07.155408 15724 net.cpp:454] Concat19 <- Dropout21
I0211 13:04:07.155414 15724 net.cpp:411] Concat19 -> Concat19
I0211 13:04:07.155452 15724 net.cpp:150] Setting up Concat19
I0211 13:04:07.155462 15724 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0211 13:04:07.155467 15724 net.cpp:165] Memory required for data: 3679350028
I0211 13:04:07.155472 15724 layer_factory.hpp:77] Creating layer Concat19_Concat19_0_split
I0211 13:04:07.155478 15724 net.cpp:106] Creating Layer Concat19_Concat19_0_split
I0211 13:04:07.155483 15724 net.cpp:454] Concat19_Concat19_0_split <- Concat19
I0211 13:04:07.155488 15724 net.cpp:411] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_0
I0211 13:04:07.155506 15724 net.cpp:411] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_1
I0211 13:04:07.155565 15724 net.cpp:150] Setting up Concat19_Concat19_0_split
I0211 13:04:07.155576 15724 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0211 13:04:07.155582 15724 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0211 13:04:07.155586 15724 net.cpp:165] Memory required for data: 3697650028
I0211 13:04:07.155591 15724 layer_factory.hpp:77] Creating layer BatchNorm22
I0211 13:04:07.155598 15724 net.cpp:106] Creating Layer BatchNorm22
I0211 13:04:07.155603 15724 net.cpp:454] BatchNorm22 <- Concat19_Concat19_0_split_0
I0211 13:04:07.155609 15724 net.cpp:411] BatchNorm22 -> BatchNorm22
I0211 13:04:07.155935 15724 net.cpp:150] Setting up BatchNorm22
I0211 13:04:07.155944 15724 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0211 13:04:07.155948 15724 net.cpp:165] Memory required for data: 3706800028
I0211 13:04:07.155972 15724 layer_factory.hpp:77] Creating layer Scale22
I0211 13:04:07.155980 15724 net.cpp:106] Creating Layer Scale22
I0211 13:04:07.155987 15724 net.cpp:454] Scale22 <- BatchNorm22
I0211 13:04:07.155994 15724 net.cpp:397] Scale22 -> BatchNorm22 (in-place)
I0211 13:04:07.156056 15724 layer_factory.hpp:77] Creating layer Scale22
I0211 13:04:07.156262 15724 net.cpp:150] Setting up Scale22
I0211 13:04:07.156272 15724 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0211 13:04:07.156277 15724 net.cpp:165] Memory required for data: 3715950028
I0211 13:04:07.156285 15724 layer_factory.hpp:77] Creating layer ReLU22
I0211 13:04:07.156294 15724 net.cpp:106] Creating Layer ReLU22
I0211 13:04:07.156299 15724 net.cpp:454] ReLU22 <- BatchNorm22
I0211 13:04:07.156304 15724 net.cpp:397] ReLU22 -> BatchNorm22 (in-place)
I0211 13:04:07.156311 15724 net.cpp:150] Setting up ReLU22
I0211 13:04:07.156317 15724 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0211 13:04:07.156322 15724 net.cpp:165] Memory required for data: 3725100028
I0211 13:04:07.156325 15724 layer_factory.hpp:77] Creating layer Convolution23
I0211 13:04:07.156334 15724 net.cpp:106] Creating Layer Convolution23
I0211 13:04:07.156339 15724 net.cpp:454] Convolution23 <- BatchNorm22
I0211 13:04:07.156345 15724 net.cpp:411] Convolution23 -> Convolution23
I0211 13:04:07.157403 15724 net.cpp:150] Setting up Convolution23
I0211 13:04:07.157413 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.157418 15724 net.cpp:165] Memory required for data: 3725550028
I0211 13:04:07.157424 15724 layer_factory.hpp:77] Creating layer Dropout22
I0211 13:04:07.157431 15724 net.cpp:106] Creating Layer Dropout22
I0211 13:04:07.157436 15724 net.cpp:454] Dropout22 <- Convolution23
I0211 13:04:07.157444 15724 net.cpp:411] Dropout22 -> Dropout22
I0211 13:04:07.157569 15724 net.cpp:150] Setting up Dropout22
I0211 13:04:07.157580 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.157585 15724 net.cpp:165] Memory required for data: 3726000028
I0211 13:04:07.157589 15724 layer_factory.hpp:77] Creating layer Concat20
I0211 13:04:07.157599 15724 net.cpp:106] Creating Layer Concat20
I0211 13:04:07.157604 15724 net.cpp:454] Concat20 <- Concat19_Concat19_0_split_1
I0211 13:04:07.157610 15724 net.cpp:454] Concat20 <- Dropout22
I0211 13:04:07.157618 15724 net.cpp:411] Concat20 -> Concat20
I0211 13:04:07.157658 15724 net.cpp:150] Setting up Concat20
I0211 13:04:07.157667 15724 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0211 13:04:07.157671 15724 net.cpp:165] Memory required for data: 3735600028
I0211 13:04:07.157675 15724 layer_factory.hpp:77] Creating layer Concat20_Concat20_0_split
I0211 13:04:07.157682 15724 net.cpp:106] Creating Layer Concat20_Concat20_0_split
I0211 13:04:07.157687 15724 net.cpp:454] Concat20_Concat20_0_split <- Concat20
I0211 13:04:07.157696 15724 net.cpp:411] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_0
I0211 13:04:07.157702 15724 net.cpp:411] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_1
I0211 13:04:07.157759 15724 net.cpp:150] Setting up Concat20_Concat20_0_split
I0211 13:04:07.157768 15724 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0211 13:04:07.157774 15724 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0211 13:04:07.157778 15724 net.cpp:165] Memory required for data: 3754800028
I0211 13:04:07.157783 15724 layer_factory.hpp:77] Creating layer BatchNorm23
I0211 13:04:07.157791 15724 net.cpp:106] Creating Layer BatchNorm23
I0211 13:04:07.157796 15724 net.cpp:454] BatchNorm23 <- Concat20_Concat20_0_split_0
I0211 13:04:07.157804 15724 net.cpp:411] BatchNorm23 -> BatchNorm23
I0211 13:04:07.158115 15724 net.cpp:150] Setting up BatchNorm23
I0211 13:04:07.158164 15724 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0211 13:04:07.158169 15724 net.cpp:165] Memory required for data: 3764400028
I0211 13:04:07.158177 15724 layer_factory.hpp:77] Creating layer Scale23
I0211 13:04:07.158187 15724 net.cpp:106] Creating Layer Scale23
I0211 13:04:07.158193 15724 net.cpp:454] Scale23 <- BatchNorm23
I0211 13:04:07.158200 15724 net.cpp:397] Scale23 -> BatchNorm23 (in-place)
I0211 13:04:07.158265 15724 layer_factory.hpp:77] Creating layer Scale23
I0211 13:04:07.158458 15724 net.cpp:150] Setting up Scale23
I0211 13:04:07.158468 15724 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0211 13:04:07.158473 15724 net.cpp:165] Memory required for data: 3774000028
I0211 13:04:07.158480 15724 layer_factory.hpp:77] Creating layer ReLU23
I0211 13:04:07.158489 15724 net.cpp:106] Creating Layer ReLU23
I0211 13:04:07.158494 15724 net.cpp:454] ReLU23 <- BatchNorm23
I0211 13:04:07.158500 15724 net.cpp:397] ReLU23 -> BatchNorm23 (in-place)
I0211 13:04:07.158507 15724 net.cpp:150] Setting up ReLU23
I0211 13:04:07.158514 15724 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0211 13:04:07.158517 15724 net.cpp:165] Memory required for data: 3783600028
I0211 13:04:07.158521 15724 layer_factory.hpp:77] Creating layer Convolution24
I0211 13:04:07.158534 15724 net.cpp:106] Creating Layer Convolution24
I0211 13:04:07.158537 15724 net.cpp:454] Convolution24 <- BatchNorm23
I0211 13:04:07.158546 15724 net.cpp:411] Convolution24 -> Convolution24
I0211 13:04:07.159651 15724 net.cpp:150] Setting up Convolution24
I0211 13:04:07.159663 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.159667 15724 net.cpp:165] Memory required for data: 3784050028
I0211 13:04:07.159674 15724 layer_factory.hpp:77] Creating layer Dropout23
I0211 13:04:07.159683 15724 net.cpp:106] Creating Layer Dropout23
I0211 13:04:07.159688 15724 net.cpp:454] Dropout23 <- Convolution24
I0211 13:04:07.159695 15724 net.cpp:411] Dropout23 -> Dropout23
I0211 13:04:07.159756 15724 net.cpp:150] Setting up Dropout23
I0211 13:04:07.159765 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.159770 15724 net.cpp:165] Memory required for data: 3784500028
I0211 13:04:07.159775 15724 layer_factory.hpp:77] Creating layer Concat21
I0211 13:04:07.159782 15724 net.cpp:106] Creating Layer Concat21
I0211 13:04:07.159787 15724 net.cpp:454] Concat21 <- Concat20_Concat20_0_split_1
I0211 13:04:07.159793 15724 net.cpp:454] Concat21 <- Dropout23
I0211 13:04:07.159801 15724 net.cpp:411] Concat21 -> Concat21
I0211 13:04:07.159838 15724 net.cpp:150] Setting up Concat21
I0211 13:04:07.159847 15724 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0211 13:04:07.159852 15724 net.cpp:165] Memory required for data: 3794550028
I0211 13:04:07.159857 15724 layer_factory.hpp:77] Creating layer Concat21_Concat21_0_split
I0211 13:04:07.159862 15724 net.cpp:106] Creating Layer Concat21_Concat21_0_split
I0211 13:04:07.159869 15724 net.cpp:454] Concat21_Concat21_0_split <- Concat21
I0211 13:04:07.159875 15724 net.cpp:411] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_0
I0211 13:04:07.159883 15724 net.cpp:411] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_1
I0211 13:04:07.159940 15724 net.cpp:150] Setting up Concat21_Concat21_0_split
I0211 13:04:07.159948 15724 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0211 13:04:07.159955 15724 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0211 13:04:07.159958 15724 net.cpp:165] Memory required for data: 3814650028
I0211 13:04:07.159962 15724 layer_factory.hpp:77] Creating layer BatchNorm24
I0211 13:04:07.159971 15724 net.cpp:106] Creating Layer BatchNorm24
I0211 13:04:07.159976 15724 net.cpp:454] BatchNorm24 <- Concat21_Concat21_0_split_0
I0211 13:04:07.159983 15724 net.cpp:411] BatchNorm24 -> BatchNorm24
I0211 13:04:07.168951 15724 net.cpp:150] Setting up BatchNorm24
I0211 13:04:07.168970 15724 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0211 13:04:07.168975 15724 net.cpp:165] Memory required for data: 3824700028
I0211 13:04:07.168987 15724 layer_factory.hpp:77] Creating layer Scale24
I0211 13:04:07.168994 15724 net.cpp:106] Creating Layer Scale24
I0211 13:04:07.169000 15724 net.cpp:454] Scale24 <- BatchNorm24
I0211 13:04:07.169008 15724 net.cpp:397] Scale24 -> BatchNorm24 (in-place)
I0211 13:04:07.169080 15724 layer_factory.hpp:77] Creating layer Scale24
I0211 13:04:07.169292 15724 net.cpp:150] Setting up Scale24
I0211 13:04:07.169303 15724 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0211 13:04:07.169307 15724 net.cpp:165] Memory required for data: 3834750028
I0211 13:04:07.169315 15724 layer_factory.hpp:77] Creating layer ReLU24
I0211 13:04:07.169322 15724 net.cpp:106] Creating Layer ReLU24
I0211 13:04:07.169328 15724 net.cpp:454] ReLU24 <- BatchNorm24
I0211 13:04:07.169337 15724 net.cpp:397] ReLU24 -> BatchNorm24 (in-place)
I0211 13:04:07.169343 15724 net.cpp:150] Setting up ReLU24
I0211 13:04:07.169349 15724 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0211 13:04:07.169353 15724 net.cpp:165] Memory required for data: 3844800028
I0211 13:04:07.169358 15724 layer_factory.hpp:77] Creating layer Convolution25
I0211 13:04:07.169368 15724 net.cpp:106] Creating Layer Convolution25
I0211 13:04:07.169373 15724 net.cpp:454] Convolution25 <- BatchNorm24
I0211 13:04:07.169379 15724 net.cpp:411] Convolution25 -> Convolution25
I0211 13:04:07.171653 15724 net.cpp:150] Setting up Convolution25
I0211 13:04:07.171666 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.171671 15724 net.cpp:165] Memory required for data: 3845250028
I0211 13:04:07.171679 15724 layer_factory.hpp:77] Creating layer Dropout24
I0211 13:04:07.171686 15724 net.cpp:106] Creating Layer Dropout24
I0211 13:04:07.171691 15724 net.cpp:454] Dropout24 <- Convolution25
I0211 13:04:07.171700 15724 net.cpp:411] Dropout24 -> Dropout24
I0211 13:04:07.171766 15724 net.cpp:150] Setting up Dropout24
I0211 13:04:07.171777 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.171780 15724 net.cpp:165] Memory required for data: 3845700028
I0211 13:04:07.171785 15724 layer_factory.hpp:77] Creating layer Concat22
I0211 13:04:07.171792 15724 net.cpp:106] Creating Layer Concat22
I0211 13:04:07.171797 15724 net.cpp:454] Concat22 <- Concat21_Concat21_0_split_1
I0211 13:04:07.171802 15724 net.cpp:454] Concat22 <- Dropout24
I0211 13:04:07.171810 15724 net.cpp:411] Concat22 -> Concat22
I0211 13:04:07.171850 15724 net.cpp:150] Setting up Concat22
I0211 13:04:07.171859 15724 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0211 13:04:07.171864 15724 net.cpp:165] Memory required for data: 3856200028
I0211 13:04:07.171869 15724 layer_factory.hpp:77] Creating layer Concat22_Concat22_0_split
I0211 13:04:07.171875 15724 net.cpp:106] Creating Layer Concat22_Concat22_0_split
I0211 13:04:07.171880 15724 net.cpp:454] Concat22_Concat22_0_split <- Concat22
I0211 13:04:07.171888 15724 net.cpp:411] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_0
I0211 13:04:07.171895 15724 net.cpp:411] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_1
I0211 13:04:07.171954 15724 net.cpp:150] Setting up Concat22_Concat22_0_split
I0211 13:04:07.171963 15724 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0211 13:04:07.171969 15724 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0211 13:04:07.171973 15724 net.cpp:165] Memory required for data: 3877200028
I0211 13:04:07.171977 15724 layer_factory.hpp:77] Creating layer BatchNorm25
I0211 13:04:07.171986 15724 net.cpp:106] Creating Layer BatchNorm25
I0211 13:04:07.171991 15724 net.cpp:454] BatchNorm25 <- Concat22_Concat22_0_split_0
I0211 13:04:07.171998 15724 net.cpp:411] BatchNorm25 -> BatchNorm25
I0211 13:04:07.172333 15724 net.cpp:150] Setting up BatchNorm25
I0211 13:04:07.172343 15724 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0211 13:04:07.172348 15724 net.cpp:165] Memory required for data: 3887700028
I0211 13:04:07.172358 15724 layer_factory.hpp:77] Creating layer Scale25
I0211 13:04:07.172368 15724 net.cpp:106] Creating Layer Scale25
I0211 13:04:07.172372 15724 net.cpp:454] Scale25 <- BatchNorm25
I0211 13:04:07.172379 15724 net.cpp:397] Scale25 -> BatchNorm25 (in-place)
I0211 13:04:07.172446 15724 layer_factory.hpp:77] Creating layer Scale25
I0211 13:04:07.172652 15724 net.cpp:150] Setting up Scale25
I0211 13:04:07.172663 15724 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0211 13:04:07.172667 15724 net.cpp:165] Memory required for data: 3898200028
I0211 13:04:07.172675 15724 layer_factory.hpp:77] Creating layer ReLU25
I0211 13:04:07.172683 15724 net.cpp:106] Creating Layer ReLU25
I0211 13:04:07.172688 15724 net.cpp:454] ReLU25 <- BatchNorm25
I0211 13:04:07.172693 15724 net.cpp:397] ReLU25 -> BatchNorm25 (in-place)
I0211 13:04:07.172700 15724 net.cpp:150] Setting up ReLU25
I0211 13:04:07.172706 15724 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0211 13:04:07.172710 15724 net.cpp:165] Memory required for data: 3908700028
I0211 13:04:07.172714 15724 layer_factory.hpp:77] Creating layer Convolution26
I0211 13:04:07.172726 15724 net.cpp:106] Creating Layer Convolution26
I0211 13:04:07.172731 15724 net.cpp:454] Convolution26 <- BatchNorm25
I0211 13:04:07.172739 15724 net.cpp:411] Convolution26 -> Convolution26
I0211 13:04:07.173916 15724 net.cpp:150] Setting up Convolution26
I0211 13:04:07.173928 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.173931 15724 net.cpp:165] Memory required for data: 3909150028
I0211 13:04:07.173938 15724 layer_factory.hpp:77] Creating layer Dropout25
I0211 13:04:07.173945 15724 net.cpp:106] Creating Layer Dropout25
I0211 13:04:07.173950 15724 net.cpp:454] Dropout25 <- Convolution26
I0211 13:04:07.173959 15724 net.cpp:411] Dropout25 -> Dropout25
I0211 13:04:07.174021 15724 net.cpp:150] Setting up Dropout25
I0211 13:04:07.174031 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.174034 15724 net.cpp:165] Memory required for data: 3909600028
I0211 13:04:07.174039 15724 layer_factory.hpp:77] Creating layer Concat23
I0211 13:04:07.174046 15724 net.cpp:106] Creating Layer Concat23
I0211 13:04:07.174052 15724 net.cpp:454] Concat23 <- Concat22_Concat22_0_split_1
I0211 13:04:07.174057 15724 net.cpp:454] Concat23 <- Dropout25
I0211 13:04:07.174064 15724 net.cpp:411] Concat23 -> Concat23
I0211 13:04:07.174103 15724 net.cpp:150] Setting up Concat23
I0211 13:04:07.174111 15724 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0211 13:04:07.174116 15724 net.cpp:165] Memory required for data: 3920550028
I0211 13:04:07.174120 15724 layer_factory.hpp:77] Creating layer Concat23_Concat23_0_split
I0211 13:04:07.174127 15724 net.cpp:106] Creating Layer Concat23_Concat23_0_split
I0211 13:04:07.174131 15724 net.cpp:454] Concat23_Concat23_0_split <- Concat23
I0211 13:04:07.174139 15724 net.cpp:411] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_0
I0211 13:04:07.174147 15724 net.cpp:411] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_1
I0211 13:04:07.174206 15724 net.cpp:150] Setting up Concat23_Concat23_0_split
I0211 13:04:07.174216 15724 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0211 13:04:07.174221 15724 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0211 13:04:07.174226 15724 net.cpp:165] Memory required for data: 3942450028
I0211 13:04:07.174229 15724 layer_factory.hpp:77] Creating layer BatchNorm26
I0211 13:04:07.174238 15724 net.cpp:106] Creating Layer BatchNorm26
I0211 13:04:07.174244 15724 net.cpp:454] BatchNorm26 <- Concat23_Concat23_0_split_0
I0211 13:04:07.174250 15724 net.cpp:411] BatchNorm26 -> BatchNorm26
I0211 13:04:07.174607 15724 net.cpp:150] Setting up BatchNorm26
I0211 13:04:07.174618 15724 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0211 13:04:07.174623 15724 net.cpp:165] Memory required for data: 3953400028
I0211 13:04:07.174631 15724 layer_factory.hpp:77] Creating layer Scale26
I0211 13:04:07.174641 15724 net.cpp:106] Creating Layer Scale26
I0211 13:04:07.174648 15724 net.cpp:454] Scale26 <- BatchNorm26
I0211 13:04:07.174654 15724 net.cpp:397] Scale26 -> BatchNorm26 (in-place)
I0211 13:04:07.174721 15724 layer_factory.hpp:77] Creating layer Scale26
I0211 13:04:07.174935 15724 net.cpp:150] Setting up Scale26
I0211 13:04:07.174947 15724 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0211 13:04:07.174950 15724 net.cpp:165] Memory required for data: 3964350028
I0211 13:04:07.174958 15724 layer_factory.hpp:77] Creating layer ReLU26
I0211 13:04:07.174965 15724 net.cpp:106] Creating Layer ReLU26
I0211 13:04:07.174971 15724 net.cpp:454] ReLU26 <- BatchNorm26
I0211 13:04:07.174976 15724 net.cpp:397] ReLU26 -> BatchNorm26 (in-place)
I0211 13:04:07.174983 15724 net.cpp:150] Setting up ReLU26
I0211 13:04:07.174989 15724 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0211 13:04:07.174993 15724 net.cpp:165] Memory required for data: 3975300028
I0211 13:04:07.174998 15724 layer_factory.hpp:77] Creating layer Convolution27
I0211 13:04:07.175009 15724 net.cpp:106] Creating Layer Convolution27
I0211 13:04:07.175014 15724 net.cpp:454] Convolution27 <- BatchNorm26
I0211 13:04:07.175020 15724 net.cpp:411] Convolution27 -> Convolution27
I0211 13:04:07.176232 15724 net.cpp:150] Setting up Convolution27
I0211 13:04:07.176244 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.176247 15724 net.cpp:165] Memory required for data: 3975750028
I0211 13:04:07.176254 15724 layer_factory.hpp:77] Creating layer Dropout26
I0211 13:04:07.176262 15724 net.cpp:106] Creating Layer Dropout26
I0211 13:04:07.176268 15724 net.cpp:454] Dropout26 <- Convolution27
I0211 13:04:07.176275 15724 net.cpp:411] Dropout26 -> Dropout26
I0211 13:04:07.176340 15724 net.cpp:150] Setting up Dropout26
I0211 13:04:07.176349 15724 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0211 13:04:07.176353 15724 net.cpp:165] Memory required for data: 3976200028
I0211 13:04:07.176358 15724 layer_factory.hpp:77] Creating layer Concat24
I0211 13:04:07.176365 15724 net.cpp:106] Creating Layer Concat24
I0211 13:04:07.176370 15724 net.cpp:454] Concat24 <- Concat23_Concat23_0_split_1
I0211 13:04:07.176375 15724 net.cpp:454] Concat24 <- Dropout26
I0211 13:04:07.176383 15724 net.cpp:411] Concat24 -> Concat24
I0211 13:04:07.176421 15724 net.cpp:150] Setting up Concat24
I0211 13:04:07.176430 15724 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0211 13:04:07.176435 15724 net.cpp:165] Memory required for data: 3987600028
I0211 13:04:07.176440 15724 layer_factory.hpp:77] Creating layer BatchNorm27
I0211 13:04:07.176448 15724 net.cpp:106] Creating Layer BatchNorm27
I0211 13:04:07.176453 15724 net.cpp:454] BatchNorm27 <- Concat24
I0211 13:04:07.176461 15724 net.cpp:411] BatchNorm27 -> BatchNorm27
I0211 13:04:07.176797 15724 net.cpp:150] Setting up BatchNorm27
I0211 13:04:07.176808 15724 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0211 13:04:07.176812 15724 net.cpp:165] Memory required for data: 3999000028
I0211 13:04:07.176821 15724 layer_factory.hpp:77] Creating layer Scale27
I0211 13:04:07.176831 15724 net.cpp:106] Creating Layer Scale27
I0211 13:04:07.176837 15724 net.cpp:454] Scale27 <- BatchNorm27
I0211 13:04:07.176843 15724 net.cpp:397] Scale27 -> BatchNorm27 (in-place)
I0211 13:04:07.176908 15724 layer_factory.hpp:77] Creating layer Scale27
I0211 13:04:07.177120 15724 net.cpp:150] Setting up Scale27
I0211 13:04:07.177131 15724 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0211 13:04:07.177135 15724 net.cpp:165] Memory required for data: 4010400028
I0211 13:04:07.177144 15724 layer_factory.hpp:77] Creating layer ReLU27
I0211 13:04:07.177150 15724 net.cpp:106] Creating Layer ReLU27
I0211 13:04:07.177155 15724 net.cpp:454] ReLU27 <- BatchNorm27
I0211 13:04:07.177163 15724 net.cpp:397] ReLU27 -> BatchNorm27 (in-place)
I0211 13:04:07.177170 15724 net.cpp:150] Setting up ReLU27
I0211 13:04:07.177176 15724 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0211 13:04:07.177181 15724 net.cpp:165] Memory required for data: 4021800028
I0211 13:04:07.177186 15724 layer_factory.hpp:77] Creating layer Convolution28
I0211 13:04:07.177193 15724 net.cpp:106] Creating Layer Convolution28
I0211 13:04:07.177198 15724 net.cpp:454] Convolution28 <- BatchNorm27
I0211 13:04:07.177206 15724 net.cpp:411] Convolution28 -> Convolution28
I0211 13:04:07.180202 15724 net.cpp:150] Setting up Convolution28
I0211 13:04:07.180213 15724 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0211 13:04:07.180218 15724 net.cpp:165] Memory required for data: 4033200028
I0211 13:04:07.180224 15724 layer_factory.hpp:77] Creating layer Dropout27
I0211 13:04:07.180234 15724 net.cpp:106] Creating Layer Dropout27
I0211 13:04:07.180239 15724 net.cpp:454] Dropout27 <- Convolution28
I0211 13:04:07.180246 15724 net.cpp:411] Dropout27 -> Dropout27
I0211 13:04:07.180307 15724 net.cpp:150] Setting up Dropout27
I0211 13:04:07.180317 15724 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0211 13:04:07.180321 15724 net.cpp:165] Memory required for data: 4044600028
I0211 13:04:07.180326 15724 layer_factory.hpp:77] Creating layer Pooling3
I0211 13:04:07.180335 15724 net.cpp:106] Creating Layer Pooling3
I0211 13:04:07.180341 15724 net.cpp:454] Pooling3 <- Dropout27
I0211 13:04:07.180346 15724 net.cpp:411] Pooling3 -> Pooling3
I0211 13:04:07.180384 15724 net.cpp:150] Setting up Pooling3
I0211 13:04:07.180393 15724 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0211 13:04:07.180397 15724 net.cpp:165] Memory required for data: 4047511132
I0211 13:04:07.180402 15724 layer_factory.hpp:77] Creating layer Pooling3_Pooling3_0_split
I0211 13:04:07.180410 15724 net.cpp:106] Creating Layer Pooling3_Pooling3_0_split
I0211 13:04:07.180415 15724 net.cpp:454] Pooling3_Pooling3_0_split <- Pooling3
I0211 13:04:07.180421 15724 net.cpp:411] Pooling3_Pooling3_0_split -> Pooling3_Pooling3_0_split_0
I0211 13:04:07.180430 15724 net.cpp:411] Pooling3_Pooling3_0_split -> Pooling3_Pooling3_0_split_1
I0211 13:04:07.180487 15724 net.cpp:150] Setting up Pooling3_Pooling3_0_split
I0211 13:04:07.180497 15724 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0211 13:04:07.180502 15724 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0211 13:04:07.180506 15724 net.cpp:165] Memory required for data: 4053333340
I0211 13:04:07.180511 15724 layer_factory.hpp:77] Creating layer BatchNorm28
I0211 13:04:07.180519 15724 net.cpp:106] Creating Layer BatchNorm28
I0211 13:04:07.180524 15724 net.cpp:454] BatchNorm28 <- Pooling3_Pooling3_0_split_0
I0211 13:04:07.180533 15724 net.cpp:411] BatchNorm28 -> BatchNorm28
I0211 13:04:07.180871 15724 net.cpp:150] Setting up BatchNorm28
I0211 13:04:07.180883 15724 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0211 13:04:07.180887 15724 net.cpp:165] Memory required for data: 4056244444
I0211 13:04:07.180896 15724 layer_factory.hpp:77] Creating layer Scale28
I0211 13:04:07.180903 15724 net.cpp:106] Creating Layer Scale28
I0211 13:04:07.180908 15724 net.cpp:454] Scale28 <- BatchNorm28
I0211 13:04:07.180917 15724 net.cpp:397] Scale28 -> BatchNorm28 (in-place)
I0211 13:04:07.180982 15724 layer_factory.hpp:77] Creating layer Scale28
I0211 13:04:07.181179 15724 net.cpp:150] Setting up Scale28
I0211 13:04:07.181188 15724 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0211 13:04:07.181193 15724 net.cpp:165] Memory required for data: 4059155548
I0211 13:04:07.181200 15724 layer_factory.hpp:77] Creating layer ReLU28
I0211 13:04:07.181207 15724 net.cpp:106] Creating Layer ReLU28
I0211 13:04:07.181212 15724 net.cpp:454] ReLU28 <- BatchNorm28
I0211 13:04:07.181221 15724 net.cpp:397] ReLU28 -> BatchNorm28 (in-place)
I0211 13:04:07.181226 15724 net.cpp:150] Setting up ReLU28
I0211 13:04:07.181232 15724 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0211 13:04:07.181236 15724 net.cpp:165] Memory required for data: 4062066652
I0211 13:04:07.181241 15724 layer_factory.hpp:77] Creating layer Convolution29
I0211 13:04:07.181249 15724 net.cpp:106] Creating Layer Convolution29
I0211 13:04:07.181253 15724 net.cpp:454] Convolution29 <- BatchNorm28
I0211 13:04:07.181259 15724 net.cpp:411] Convolution29 -> Convolution29
I0211 13:04:07.182483 15724 net.cpp:150] Setting up Convolution29
I0211 13:04:07.182493 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.182498 15724 net.cpp:165] Memory required for data: 4062181564
I0211 13:04:07.182504 15724 layer_factory.hpp:77] Creating layer Dropout28
I0211 13:04:07.182512 15724 net.cpp:106] Creating Layer Dropout28
I0211 13:04:07.182517 15724 net.cpp:454] Dropout28 <- Convolution29
I0211 13:04:07.182524 15724 net.cpp:411] Dropout28 -> Dropout28
I0211 13:04:07.182608 15724 net.cpp:150] Setting up Dropout28
I0211 13:04:07.182618 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.182622 15724 net.cpp:165] Memory required for data: 4062296476
I0211 13:04:07.182627 15724 layer_factory.hpp:77] Creating layer Concat25
I0211 13:04:07.182636 15724 net.cpp:106] Creating Layer Concat25
I0211 13:04:07.182641 15724 net.cpp:454] Concat25 <- Pooling3_Pooling3_0_split_1
I0211 13:04:07.182647 15724 net.cpp:454] Concat25 <- Dropout28
I0211 13:04:07.182653 15724 net.cpp:411] Concat25 -> Concat25
I0211 13:04:07.182693 15724 net.cpp:150] Setting up Concat25
I0211 13:04:07.182703 15724 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0211 13:04:07.182706 15724 net.cpp:165] Memory required for data: 4065322492
I0211 13:04:07.182710 15724 layer_factory.hpp:77] Creating layer Concat25_Concat25_0_split
I0211 13:04:07.182718 15724 net.cpp:106] Creating Layer Concat25_Concat25_0_split
I0211 13:04:07.182721 15724 net.cpp:454] Concat25_Concat25_0_split <- Concat25
I0211 13:04:07.182729 15724 net.cpp:411] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_0
I0211 13:04:07.182736 15724 net.cpp:411] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_1
I0211 13:04:07.182793 15724 net.cpp:150] Setting up Concat25_Concat25_0_split
I0211 13:04:07.182808 15724 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0211 13:04:07.182813 15724 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0211 13:04:07.182818 15724 net.cpp:165] Memory required for data: 4071374524
I0211 13:04:07.182822 15724 layer_factory.hpp:77] Creating layer BatchNorm29
I0211 13:04:07.182832 15724 net.cpp:106] Creating Layer BatchNorm29
I0211 13:04:07.182835 15724 net.cpp:454] BatchNorm29 <- Concat25_Concat25_0_split_0
I0211 13:04:07.182844 15724 net.cpp:411] BatchNorm29 -> BatchNorm29
I0211 13:04:07.183207 15724 net.cpp:150] Setting up BatchNorm29
I0211 13:04:07.183218 15724 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0211 13:04:07.183221 15724 net.cpp:165] Memory required for data: 4074400540
I0211 13:04:07.183230 15724 layer_factory.hpp:77] Creating layer Scale29
I0211 13:04:07.183238 15724 net.cpp:106] Creating Layer Scale29
I0211 13:04:07.183243 15724 net.cpp:454] Scale29 <- BatchNorm29
I0211 13:04:07.183251 15724 net.cpp:397] Scale29 -> BatchNorm29 (in-place)
I0211 13:04:07.183321 15724 layer_factory.hpp:77] Creating layer Scale29
I0211 13:04:07.183533 15724 net.cpp:150] Setting up Scale29
I0211 13:04:07.183544 15724 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0211 13:04:07.183548 15724 net.cpp:165] Memory required for data: 4077426556
I0211 13:04:07.183557 15724 layer_factory.hpp:77] Creating layer ReLU29
I0211 13:04:07.183567 15724 net.cpp:106] Creating Layer ReLU29
I0211 13:04:07.183571 15724 net.cpp:454] ReLU29 <- BatchNorm29
I0211 13:04:07.183578 15724 net.cpp:397] ReLU29 -> BatchNorm29 (in-place)
I0211 13:04:07.183583 15724 net.cpp:150] Setting up ReLU29
I0211 13:04:07.183589 15724 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0211 13:04:07.183593 15724 net.cpp:165] Memory required for data: 4080452572
I0211 13:04:07.183598 15724 layer_factory.hpp:77] Creating layer Convolution30
I0211 13:04:07.183606 15724 net.cpp:106] Creating Layer Convolution30
I0211 13:04:07.183611 15724 net.cpp:454] Convolution30 <- BatchNorm29
I0211 13:04:07.183617 15724 net.cpp:411] Convolution30 -> Convolution30
I0211 13:04:07.184907 15724 net.cpp:150] Setting up Convolution30
I0211 13:04:07.184918 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.184922 15724 net.cpp:165] Memory required for data: 4080567484
I0211 13:04:07.184929 15724 layer_factory.hpp:77] Creating layer Dropout29
I0211 13:04:07.184937 15724 net.cpp:106] Creating Layer Dropout29
I0211 13:04:07.184942 15724 net.cpp:454] Dropout29 <- Convolution30
I0211 13:04:07.184949 15724 net.cpp:411] Dropout29 -> Dropout29
I0211 13:04:07.185010 15724 net.cpp:150] Setting up Dropout29
I0211 13:04:07.185019 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.185024 15724 net.cpp:165] Memory required for data: 4080682396
I0211 13:04:07.185029 15724 layer_factory.hpp:77] Creating layer Concat26
I0211 13:04:07.185037 15724 net.cpp:106] Creating Layer Concat26
I0211 13:04:07.185042 15724 net.cpp:454] Concat26 <- Concat25_Concat25_0_split_1
I0211 13:04:07.185047 15724 net.cpp:454] Concat26 <- Dropout29
I0211 13:04:07.185056 15724 net.cpp:411] Concat26 -> Concat26
I0211 13:04:07.185093 15724 net.cpp:150] Setting up Concat26
I0211 13:04:07.185103 15724 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0211 13:04:07.185107 15724 net.cpp:165] Memory required for data: 4083823324
I0211 13:04:07.185111 15724 layer_factory.hpp:77] Creating layer Concat26_Concat26_0_split
I0211 13:04:07.185117 15724 net.cpp:106] Creating Layer Concat26_Concat26_0_split
I0211 13:04:07.185122 15724 net.cpp:454] Concat26_Concat26_0_split <- Concat26
I0211 13:04:07.185130 15724 net.cpp:411] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_0
I0211 13:04:07.185137 15724 net.cpp:411] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_1
I0211 13:04:07.185194 15724 net.cpp:150] Setting up Concat26_Concat26_0_split
I0211 13:04:07.185204 15724 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0211 13:04:07.185209 15724 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0211 13:04:07.185214 15724 net.cpp:165] Memory required for data: 4090105180
I0211 13:04:07.185217 15724 layer_factory.hpp:77] Creating layer BatchNorm30
I0211 13:04:07.185226 15724 net.cpp:106] Creating Layer BatchNorm30
I0211 13:04:07.185231 15724 net.cpp:454] BatchNorm30 <- Concat26_Concat26_0_split_0
I0211 13:04:07.185238 15724 net.cpp:411] BatchNorm30 -> BatchNorm30
I0211 13:04:07.185585 15724 net.cpp:150] Setting up BatchNorm30
I0211 13:04:07.185595 15724 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0211 13:04:07.185600 15724 net.cpp:165] Memory required for data: 4093246108
I0211 13:04:07.185608 15724 layer_factory.hpp:77] Creating layer Scale30
I0211 13:04:07.185616 15724 net.cpp:106] Creating Layer Scale30
I0211 13:04:07.185621 15724 net.cpp:454] Scale30 <- BatchNorm30
I0211 13:04:07.185633 15724 net.cpp:397] Scale30 -> BatchNorm30 (in-place)
I0211 13:04:07.185703 15724 layer_factory.hpp:77] Creating layer Scale30
I0211 13:04:07.185906 15724 net.cpp:150] Setting up Scale30
I0211 13:04:07.185916 15724 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0211 13:04:07.185920 15724 net.cpp:165] Memory required for data: 4096387036
I0211 13:04:07.185928 15724 layer_factory.hpp:77] Creating layer ReLU30
I0211 13:04:07.185937 15724 net.cpp:106] Creating Layer ReLU30
I0211 13:04:07.185942 15724 net.cpp:454] ReLU30 <- BatchNorm30
I0211 13:04:07.185948 15724 net.cpp:397] ReLU30 -> BatchNorm30 (in-place)
I0211 13:04:07.185963 15724 net.cpp:150] Setting up ReLU30
I0211 13:04:07.185969 15724 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0211 13:04:07.185973 15724 net.cpp:165] Memory required for data: 4099527964
I0211 13:04:07.185977 15724 layer_factory.hpp:77] Creating layer Convolution31
I0211 13:04:07.185986 15724 net.cpp:106] Creating Layer Convolution31
I0211 13:04:07.185992 15724 net.cpp:454] Convolution31 <- BatchNorm30
I0211 13:04:07.185997 15724 net.cpp:411] Convolution31 -> Convolution31
I0211 13:04:07.187386 15724 net.cpp:150] Setting up Convolution31
I0211 13:04:07.187398 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.187402 15724 net.cpp:165] Memory required for data: 4099642876
I0211 13:04:07.187409 15724 layer_factory.hpp:77] Creating layer Dropout30
I0211 13:04:07.187417 15724 net.cpp:106] Creating Layer Dropout30
I0211 13:04:07.187422 15724 net.cpp:454] Dropout30 <- Convolution31
I0211 13:04:07.187430 15724 net.cpp:411] Dropout30 -> Dropout30
I0211 13:04:07.187492 15724 net.cpp:150] Setting up Dropout30
I0211 13:04:07.187502 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.187506 15724 net.cpp:165] Memory required for data: 4099757788
I0211 13:04:07.187511 15724 layer_factory.hpp:77] Creating layer Concat27
I0211 13:04:07.187517 15724 net.cpp:106] Creating Layer Concat27
I0211 13:04:07.187522 15724 net.cpp:454] Concat27 <- Concat26_Concat26_0_split_1
I0211 13:04:07.187528 15724 net.cpp:454] Concat27 <- Dropout30
I0211 13:04:07.187536 15724 net.cpp:411] Concat27 -> Concat27
I0211 13:04:07.187574 15724 net.cpp:150] Setting up Concat27
I0211 13:04:07.187583 15724 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0211 13:04:07.187587 15724 net.cpp:165] Memory required for data: 4103013628
I0211 13:04:07.187592 15724 layer_factory.hpp:77] Creating layer Concat27_Concat27_0_split
I0211 13:04:07.187598 15724 net.cpp:106] Creating Layer Concat27_Concat27_0_split
I0211 13:04:07.187603 15724 net.cpp:454] Concat27_Concat27_0_split <- Concat27
I0211 13:04:07.187611 15724 net.cpp:411] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_0
I0211 13:04:07.187618 15724 net.cpp:411] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_1
I0211 13:04:07.187678 15724 net.cpp:150] Setting up Concat27_Concat27_0_split
I0211 13:04:07.187687 15724 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0211 13:04:07.187692 15724 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0211 13:04:07.187696 15724 net.cpp:165] Memory required for data: 4109525308
I0211 13:04:07.187701 15724 layer_factory.hpp:77] Creating layer BatchNorm31
I0211 13:04:07.187711 15724 net.cpp:106] Creating Layer BatchNorm31
I0211 13:04:07.187716 15724 net.cpp:454] BatchNorm31 <- Concat27_Concat27_0_split_0
I0211 13:04:07.187721 15724 net.cpp:411] BatchNorm31 -> BatchNorm31
I0211 13:04:07.188076 15724 net.cpp:150] Setting up BatchNorm31
I0211 13:04:07.188091 15724 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0211 13:04:07.188096 15724 net.cpp:165] Memory required for data: 4112781148
I0211 13:04:07.188104 15724 layer_factory.hpp:77] Creating layer Scale31
I0211 13:04:07.188114 15724 net.cpp:106] Creating Layer Scale31
I0211 13:04:07.188122 15724 net.cpp:454] Scale31 <- BatchNorm31
I0211 13:04:07.188128 15724 net.cpp:397] Scale31 -> BatchNorm31 (in-place)
I0211 13:04:07.188199 15724 layer_factory.hpp:77] Creating layer Scale31
I0211 13:04:07.188406 15724 net.cpp:150] Setting up Scale31
I0211 13:04:07.188418 15724 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0211 13:04:07.188423 15724 net.cpp:165] Memory required for data: 4116036988
I0211 13:04:07.188431 15724 layer_factory.hpp:77] Creating layer ReLU31
I0211 13:04:07.188441 15724 net.cpp:106] Creating Layer ReLU31
I0211 13:04:07.188447 15724 net.cpp:454] ReLU31 <- BatchNorm31
I0211 13:04:07.188453 15724 net.cpp:397] ReLU31 -> BatchNorm31 (in-place)
I0211 13:04:07.188462 15724 net.cpp:150] Setting up ReLU31
I0211 13:04:07.188468 15724 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0211 13:04:07.188474 15724 net.cpp:165] Memory required for data: 4119292828
I0211 13:04:07.188478 15724 layer_factory.hpp:77] Creating layer Convolution32
I0211 13:04:07.188488 15724 net.cpp:106] Creating Layer Convolution32
I0211 13:04:07.188494 15724 net.cpp:454] Convolution32 <- BatchNorm31
I0211 13:04:07.188503 15724 net.cpp:411] Convolution32 -> Convolution32
I0211 13:04:07.189909 15724 net.cpp:150] Setting up Convolution32
I0211 13:04:07.189924 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.189929 15724 net.cpp:165] Memory required for data: 4119407740
I0211 13:04:07.189936 15724 layer_factory.hpp:77] Creating layer Dropout31
I0211 13:04:07.189947 15724 net.cpp:106] Creating Layer Dropout31
I0211 13:04:07.189954 15724 net.cpp:454] Dropout31 <- Convolution32
I0211 13:04:07.189961 15724 net.cpp:411] Dropout31 -> Dropout31
I0211 13:04:07.190026 15724 net.cpp:150] Setting up Dropout31
I0211 13:04:07.190037 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.190042 15724 net.cpp:165] Memory required for data: 4119522652
I0211 13:04:07.190047 15724 layer_factory.hpp:77] Creating layer Concat28
I0211 13:04:07.190055 15724 net.cpp:106] Creating Layer Concat28
I0211 13:04:07.190062 15724 net.cpp:454] Concat28 <- Concat27_Concat27_0_split_1
I0211 13:04:07.190068 15724 net.cpp:454] Concat28 <- Dropout31
I0211 13:04:07.190078 15724 net.cpp:411] Concat28 -> Concat28
I0211 13:04:07.190117 15724 net.cpp:150] Setting up Concat28
I0211 13:04:07.190129 15724 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0211 13:04:07.190135 15724 net.cpp:165] Memory required for data: 4122893404
I0211 13:04:07.190138 15724 layer_factory.hpp:77] Creating layer Concat28_Concat28_0_split
I0211 13:04:07.190150 15724 net.cpp:106] Creating Layer Concat28_Concat28_0_split
I0211 13:04:07.190156 15724 net.cpp:454] Concat28_Concat28_0_split <- Concat28
I0211 13:04:07.190165 15724 net.cpp:411] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_0
I0211 13:04:07.190173 15724 net.cpp:411] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_1
I0211 13:04:07.190232 15724 net.cpp:150] Setting up Concat28_Concat28_0_split
I0211 13:04:07.190243 15724 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0211 13:04:07.190248 15724 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0211 13:04:07.190253 15724 net.cpp:165] Memory required for data: 4129634908
I0211 13:04:07.190258 15724 layer_factory.hpp:77] Creating layer BatchNorm32
I0211 13:04:07.190268 15724 net.cpp:106] Creating Layer BatchNorm32
I0211 13:04:07.190274 15724 net.cpp:454] BatchNorm32 <- Concat28_Concat28_0_split_0
I0211 13:04:07.190284 15724 net.cpp:411] BatchNorm32 -> BatchNorm32
I0211 13:04:07.190634 15724 net.cpp:150] Setting up BatchNorm32
I0211 13:04:07.190646 15724 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0211 13:04:07.190651 15724 net.cpp:165] Memory required for data: 4133005660
I0211 13:04:07.190661 15724 layer_factory.hpp:77] Creating layer Scale32
I0211 13:04:07.190672 15724 net.cpp:106] Creating Layer Scale32
I0211 13:04:07.190678 15724 net.cpp:454] Scale32 <- BatchNorm32
I0211 13:04:07.190687 15724 net.cpp:397] Scale32 -> BatchNorm32 (in-place)
I0211 13:04:07.190757 15724 layer_factory.hpp:77] Creating layer Scale32
I0211 13:04:07.190961 15724 net.cpp:150] Setting up Scale32
I0211 13:04:07.190973 15724 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0211 13:04:07.190979 15724 net.cpp:165] Memory required for data: 4136376412
I0211 13:04:07.190986 15724 layer_factory.hpp:77] Creating layer ReLU32
I0211 13:04:07.190994 15724 net.cpp:106] Creating Layer ReLU32
I0211 13:04:07.191001 15724 net.cpp:454] ReLU32 <- BatchNorm32
I0211 13:04:07.191006 15724 net.cpp:397] ReLU32 -> BatchNorm32 (in-place)
I0211 13:04:07.191014 15724 net.cpp:150] Setting up ReLU32
I0211 13:04:07.191020 15724 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0211 13:04:07.191025 15724 net.cpp:165] Memory required for data: 4139747164
I0211 13:04:07.191030 15724 layer_factory.hpp:77] Creating layer Convolution33
I0211 13:04:07.191040 15724 net.cpp:106] Creating Layer Convolution33
I0211 13:04:07.191046 15724 net.cpp:454] Convolution33 <- BatchNorm32
I0211 13:04:07.191056 15724 net.cpp:411] Convolution33 -> Convolution33
I0211 13:04:07.193042 15724 net.cpp:150] Setting up Convolution33
I0211 13:04:07.193058 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.193063 15724 net.cpp:165] Memory required for data: 4139862076
I0211 13:04:07.193070 15724 layer_factory.hpp:77] Creating layer Dropout32
I0211 13:04:07.193078 15724 net.cpp:106] Creating Layer Dropout32
I0211 13:04:07.193084 15724 net.cpp:454] Dropout32 <- Convolution33
I0211 13:04:07.193095 15724 net.cpp:411] Dropout32 -> Dropout32
I0211 13:04:07.193161 15724 net.cpp:150] Setting up Dropout32
I0211 13:04:07.193171 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.193176 15724 net.cpp:165] Memory required for data: 4139976988
I0211 13:04:07.193181 15724 layer_factory.hpp:77] Creating layer Concat29
I0211 13:04:07.193192 15724 net.cpp:106] Creating Layer Concat29
I0211 13:04:07.193197 15724 net.cpp:454] Concat29 <- Concat28_Concat28_0_split_1
I0211 13:04:07.193204 15724 net.cpp:454] Concat29 <- Dropout32
I0211 13:04:07.193212 15724 net.cpp:411] Concat29 -> Concat29
I0211 13:04:07.193253 15724 net.cpp:150] Setting up Concat29
I0211 13:04:07.193262 15724 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0211 13:04:07.193269 15724 net.cpp:165] Memory required for data: 4143462652
I0211 13:04:07.193272 15724 layer_factory.hpp:77] Creating layer Concat29_Concat29_0_split
I0211 13:04:07.193284 15724 net.cpp:106] Creating Layer Concat29_Concat29_0_split
I0211 13:04:07.193289 15724 net.cpp:454] Concat29_Concat29_0_split <- Concat29
I0211 13:04:07.193297 15724 net.cpp:411] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_0
I0211 13:04:07.193306 15724 net.cpp:411] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_1
I0211 13:04:07.193366 15724 net.cpp:150] Setting up Concat29_Concat29_0_split
I0211 13:04:07.193375 15724 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0211 13:04:07.193382 15724 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0211 13:04:07.193387 15724 net.cpp:165] Memory required for data: 4150433980
I0211 13:04:07.193392 15724 layer_factory.hpp:77] Creating layer BatchNorm33
I0211 13:04:07.193403 15724 net.cpp:106] Creating Layer BatchNorm33
I0211 13:04:07.193408 15724 net.cpp:454] BatchNorm33 <- Concat29_Concat29_0_split_0
I0211 13:04:07.193416 15724 net.cpp:411] BatchNorm33 -> BatchNorm33
I0211 13:04:07.193760 15724 net.cpp:150] Setting up BatchNorm33
I0211 13:04:07.193773 15724 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0211 13:04:07.193778 15724 net.cpp:165] Memory required for data: 4153919644
I0211 13:04:07.193788 15724 layer_factory.hpp:77] Creating layer Scale33
I0211 13:04:07.193796 15724 net.cpp:106] Creating Layer Scale33
I0211 13:04:07.193802 15724 net.cpp:454] Scale33 <- BatchNorm33
I0211 13:04:07.193812 15724 net.cpp:397] Scale33 -> BatchNorm33 (in-place)
I0211 13:04:07.193884 15724 layer_factory.hpp:77] Creating layer Scale33
I0211 13:04:07.194092 15724 net.cpp:150] Setting up Scale33
I0211 13:04:07.194105 15724 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0211 13:04:07.194110 15724 net.cpp:165] Memory required for data: 4157405308
I0211 13:04:07.194118 15724 layer_factory.hpp:77] Creating layer ReLU33
I0211 13:04:07.194126 15724 net.cpp:106] Creating Layer ReLU33
I0211 13:04:07.194133 15724 net.cpp:454] ReLU33 <- BatchNorm33
I0211 13:04:07.194142 15724 net.cpp:397] ReLU33 -> BatchNorm33 (in-place)
I0211 13:04:07.194150 15724 net.cpp:150] Setting up ReLU33
I0211 13:04:07.194157 15724 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0211 13:04:07.194164 15724 net.cpp:165] Memory required for data: 4160890972
I0211 13:04:07.194167 15724 layer_factory.hpp:77] Creating layer Convolution34
I0211 13:04:07.194177 15724 net.cpp:106] Creating Layer Convolution34
I0211 13:04:07.194182 15724 net.cpp:454] Convolution34 <- BatchNorm33
I0211 13:04:07.194190 15724 net.cpp:411] Convolution34 -> Convolution34
I0211 13:04:07.195632 15724 net.cpp:150] Setting up Convolution34
I0211 13:04:07.195646 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.195650 15724 net.cpp:165] Memory required for data: 4161005884
I0211 13:04:07.195657 15724 layer_factory.hpp:77] Creating layer Dropout33
I0211 13:04:07.195667 15724 net.cpp:106] Creating Layer Dropout33
I0211 13:04:07.195673 15724 net.cpp:454] Dropout33 <- Convolution34
I0211 13:04:07.195682 15724 net.cpp:411] Dropout33 -> Dropout33
I0211 13:04:07.195745 15724 net.cpp:150] Setting up Dropout33
I0211 13:04:07.195757 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.195762 15724 net.cpp:165] Memory required for data: 4161120796
I0211 13:04:07.195767 15724 layer_factory.hpp:77] Creating layer Concat30
I0211 13:04:07.195775 15724 net.cpp:106] Creating Layer Concat30
I0211 13:04:07.195781 15724 net.cpp:454] Concat30 <- Concat29_Concat29_0_split_1
I0211 13:04:07.195787 15724 net.cpp:454] Concat30 <- Dropout33
I0211 13:04:07.195797 15724 net.cpp:411] Concat30 -> Concat30
I0211 13:04:07.195837 15724 net.cpp:150] Setting up Concat30
I0211 13:04:07.195848 15724 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0211 13:04:07.195853 15724 net.cpp:165] Memory required for data: 4164721372
I0211 13:04:07.195858 15724 layer_factory.hpp:77] Creating layer Concat30_Concat30_0_split
I0211 13:04:07.195866 15724 net.cpp:106] Creating Layer Concat30_Concat30_0_split
I0211 13:04:07.195873 15724 net.cpp:454] Concat30_Concat30_0_split <- Concat30
I0211 13:04:07.195878 15724 net.cpp:411] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_0
I0211 13:04:07.195886 15724 net.cpp:411] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_1
I0211 13:04:07.195947 15724 net.cpp:150] Setting up Concat30_Concat30_0_split
I0211 13:04:07.195957 15724 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0211 13:04:07.195965 15724 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0211 13:04:07.195969 15724 net.cpp:165] Memory required for data: 4171922524
I0211 13:04:07.195973 15724 layer_factory.hpp:77] Creating layer BatchNorm34
I0211 13:04:07.195983 15724 net.cpp:106] Creating Layer BatchNorm34
I0211 13:04:07.195989 15724 net.cpp:454] BatchNorm34 <- Concat30_Concat30_0_split_0
I0211 13:04:07.195996 15724 net.cpp:411] BatchNorm34 -> BatchNorm34
I0211 13:04:07.196341 15724 net.cpp:150] Setting up BatchNorm34
I0211 13:04:07.196352 15724 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0211 13:04:07.196357 15724 net.cpp:165] Memory required for data: 4175523100
I0211 13:04:07.196367 15724 layer_factory.hpp:77] Creating layer Scale34
I0211 13:04:07.196377 15724 net.cpp:106] Creating Layer Scale34
I0211 13:04:07.196384 15724 net.cpp:454] Scale34 <- BatchNorm34
I0211 13:04:07.196393 15724 net.cpp:397] Scale34 -> BatchNorm34 (in-place)
I0211 13:04:07.196461 15724 layer_factory.hpp:77] Creating layer Scale34
I0211 13:04:07.196668 15724 net.cpp:150] Setting up Scale34
I0211 13:04:07.196681 15724 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0211 13:04:07.196686 15724 net.cpp:165] Memory required for data: 4179123676
I0211 13:04:07.196694 15724 layer_factory.hpp:77] Creating layer ReLU34
I0211 13:04:07.196702 15724 net.cpp:106] Creating Layer ReLU34
I0211 13:04:07.196708 15724 net.cpp:454] ReLU34 <- BatchNorm34
I0211 13:04:07.196717 15724 net.cpp:397] ReLU34 -> BatchNorm34 (in-place)
I0211 13:04:07.196724 15724 net.cpp:150] Setting up ReLU34
I0211 13:04:07.196732 15724 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0211 13:04:07.196737 15724 net.cpp:165] Memory required for data: 4182724252
I0211 13:04:07.196740 15724 layer_factory.hpp:77] Creating layer Convolution35
I0211 13:04:07.196750 15724 net.cpp:106] Creating Layer Convolution35
I0211 13:04:07.196756 15724 net.cpp:454] Convolution35 <- BatchNorm34
I0211 13:04:07.196763 15724 net.cpp:411] Convolution35 -> Convolution35
I0211 13:04:07.198245 15724 net.cpp:150] Setting up Convolution35
I0211 13:04:07.198257 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.198261 15724 net.cpp:165] Memory required for data: 4182839164
I0211 13:04:07.198268 15724 layer_factory.hpp:77] Creating layer Dropout34
I0211 13:04:07.198277 15724 net.cpp:106] Creating Layer Dropout34
I0211 13:04:07.198282 15724 net.cpp:454] Dropout34 <- Convolution35
I0211 13:04:07.198290 15724 net.cpp:411] Dropout34 -> Dropout34
I0211 13:04:07.198356 15724 net.cpp:150] Setting up Dropout34
I0211 13:04:07.198367 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.198374 15724 net.cpp:165] Memory required for data: 4182954076
I0211 13:04:07.198377 15724 layer_factory.hpp:77] Creating layer Concat31
I0211 13:04:07.198385 15724 net.cpp:106] Creating Layer Concat31
I0211 13:04:07.198391 15724 net.cpp:454] Concat31 <- Concat30_Concat30_0_split_1
I0211 13:04:07.198397 15724 net.cpp:454] Concat31 <- Dropout34
I0211 13:04:07.198406 15724 net.cpp:411] Concat31 -> Concat31
I0211 13:04:07.198446 15724 net.cpp:150] Setting up Concat31
I0211 13:04:07.198457 15724 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0211 13:04:07.198462 15724 net.cpp:165] Memory required for data: 4186669564
I0211 13:04:07.198467 15724 layer_factory.hpp:77] Creating layer Concat31_Concat31_0_split
I0211 13:04:07.198477 15724 net.cpp:106] Creating Layer Concat31_Concat31_0_split
I0211 13:04:07.198482 15724 net.cpp:454] Concat31_Concat31_0_split <- Concat31
I0211 13:04:07.198489 15724 net.cpp:411] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_0
I0211 13:04:07.198498 15724 net.cpp:411] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_1
I0211 13:04:07.198557 15724 net.cpp:150] Setting up Concat31_Concat31_0_split
I0211 13:04:07.198577 15724 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0211 13:04:07.198582 15724 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0211 13:04:07.198586 15724 net.cpp:165] Memory required for data: 4194100540
I0211 13:04:07.198591 15724 layer_factory.hpp:77] Creating layer BatchNorm35
I0211 13:04:07.198603 15724 net.cpp:106] Creating Layer BatchNorm35
I0211 13:04:07.198609 15724 net.cpp:454] BatchNorm35 <- Concat31_Concat31_0_split_0
I0211 13:04:07.198618 15724 net.cpp:411] BatchNorm35 -> BatchNorm35
I0211 13:04:07.198956 15724 net.cpp:150] Setting up BatchNorm35
I0211 13:04:07.198968 15724 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0211 13:04:07.198974 15724 net.cpp:165] Memory required for data: 4197816028
I0211 13:04:07.198984 15724 layer_factory.hpp:77] Creating layer Scale35
I0211 13:04:07.198994 15724 net.cpp:106] Creating Layer Scale35
I0211 13:04:07.199002 15724 net.cpp:454] Scale35 <- BatchNorm35
I0211 13:04:07.199008 15724 net.cpp:397] Scale35 -> BatchNorm35 (in-place)
I0211 13:04:07.199079 15724 layer_factory.hpp:77] Creating layer Scale35
I0211 13:04:07.199288 15724 net.cpp:150] Setting up Scale35
I0211 13:04:07.199301 15724 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0211 13:04:07.199306 15724 net.cpp:165] Memory required for data: 4201531516
I0211 13:04:07.199313 15724 layer_factory.hpp:77] Creating layer ReLU35
I0211 13:04:07.199321 15724 net.cpp:106] Creating Layer ReLU35
I0211 13:04:07.199327 15724 net.cpp:454] ReLU35 <- BatchNorm35
I0211 13:04:07.199335 15724 net.cpp:397] ReLU35 -> BatchNorm35 (in-place)
I0211 13:04:07.199344 15724 net.cpp:150] Setting up ReLU35
I0211 13:04:07.199352 15724 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0211 13:04:07.199357 15724 net.cpp:165] Memory required for data: 4205247004
I0211 13:04:07.199360 15724 layer_factory.hpp:77] Creating layer Convolution36
I0211 13:04:07.199371 15724 net.cpp:106] Creating Layer Convolution36
I0211 13:04:07.199376 15724 net.cpp:454] Convolution36 <- BatchNorm35
I0211 13:04:07.199383 15724 net.cpp:411] Convolution36 -> Convolution36
I0211 13:04:07.200907 15724 net.cpp:150] Setting up Convolution36
I0211 13:04:07.200920 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.200925 15724 net.cpp:165] Memory required for data: 4205361916
I0211 13:04:07.200932 15724 layer_factory.hpp:77] Creating layer Dropout35
I0211 13:04:07.200942 15724 net.cpp:106] Creating Layer Dropout35
I0211 13:04:07.200948 15724 net.cpp:454] Dropout35 <- Convolution36
I0211 13:04:07.200954 15724 net.cpp:411] Dropout35 -> Dropout35
I0211 13:04:07.201020 15724 net.cpp:150] Setting up Dropout35
I0211 13:04:07.201030 15724 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0211 13:04:07.201035 15724 net.cpp:165] Memory required for data: 4205476828
I0211 13:04:07.201040 15724 layer_factory.hpp:77] Creating layer Concat32
I0211 13:04:07.201052 15724 net.cpp:106] Creating Layer Concat32
I0211 13:04:07.201058 15724 net.cpp:454] Concat32 <- Concat31_Concat31_0_split_1
I0211 13:04:07.201064 15724 net.cpp:454] Concat32 <- Dropout35
I0211 13:04:07.201071 15724 net.cpp:411] Concat32 -> Concat32
I0211 13:04:07.201112 15724 net.cpp:150] Setting up Concat32
I0211 13:04:07.201122 15724 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0211 13:04:07.201128 15724 net.cpp:165] Memory required for data: 4209307228
I0211 13:04:07.201133 15724 layer_factory.hpp:77] Creating layer BatchNorm36
I0211 13:04:07.201143 15724 net.cpp:106] Creating Layer BatchNorm36
I0211 13:04:07.201148 15724 net.cpp:454] BatchNorm36 <- Concat32
I0211 13:04:07.201158 15724 net.cpp:411] BatchNorm36 -> BatchNorm36
I0211 13:04:07.201503 15724 net.cpp:150] Setting up BatchNorm36
I0211 13:04:07.201514 15724 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0211 13:04:07.201519 15724 net.cpp:165] Memory required for data: 4213137628
I0211 13:04:07.201529 15724 layer_factory.hpp:77] Creating layer Scale36
I0211 13:04:07.201536 15724 net.cpp:106] Creating Layer Scale36
I0211 13:04:07.201544 15724 net.cpp:454] Scale36 <- BatchNorm36
I0211 13:04:07.201553 15724 net.cpp:397] Scale36 -> BatchNorm36 (in-place)
I0211 13:04:07.201623 15724 layer_factory.hpp:77] Creating layer Scale36
I0211 13:04:07.201830 15724 net.cpp:150] Setting up Scale36
I0211 13:04:07.201843 15724 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0211 13:04:07.201848 15724 net.cpp:165] Memory required for data: 4216968028
I0211 13:04:07.201855 15724 layer_factory.hpp:77] Creating layer ReLU36
I0211 13:04:07.201864 15724 net.cpp:106] Creating Layer ReLU36
I0211 13:04:07.201869 15724 net.cpp:454] ReLU36 <- BatchNorm36
I0211 13:04:07.201876 15724 net.cpp:397] ReLU36 -> BatchNorm36 (in-place)
I0211 13:04:07.201885 15724 net.cpp:150] Setting up ReLU36
I0211 13:04:07.201891 15724 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0211 13:04:07.201896 15724 net.cpp:165] Memory required for data: 4220798428
I0211 13:04:07.201901 15724 layer_factory.hpp:77] Creating layer Convolution37
I0211 13:04:07.201911 15724 net.cpp:106] Creating Layer Convolution37
I0211 13:04:07.201916 15724 net.cpp:454] Convolution37 <- BatchNorm36
I0211 13:04:07.201923 15724 net.cpp:411] Convolution37 -> Convolution37
I0211 13:04:07.207654 15724 net.cpp:150] Setting up Convolution37
I0211 13:04:07.207670 15724 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0211 13:04:07.207677 15724 net.cpp:165] Memory required for data: 4224628828
I0211 13:04:07.207684 15724 layer_factory.hpp:77] Creating layer Dropout36
I0211 13:04:07.207692 15724 net.cpp:106] Creating Layer Dropout36
I0211 13:04:07.207700 15724 net.cpp:454] Dropout36 <- Convolution37
I0211 13:04:07.207710 15724 net.cpp:411] Dropout36 -> Dropout36
I0211 13:04:07.207787 15724 net.cpp:150] Setting up Dropout36
I0211 13:04:07.207798 15724 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0211 13:04:07.207803 15724 net.cpp:165] Memory required for data: 4228459228
I0211 13:04:07.207808 15724 layer_factory.hpp:77] Creating layer Pooling4
I0211 13:04:07.207816 15724 net.cpp:106] Creating Layer Pooling4
I0211 13:04:07.207823 15724 net.cpp:454] Pooling4 <- Dropout36
I0211 13:04:07.207831 15724 net.cpp:411] Pooling4 -> Pooling4
I0211 13:04:07.207872 15724 net.cpp:150] Setting up Pooling4
I0211 13:04:07.207883 15724 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0211 13:04:07.207888 15724 net.cpp:165] Memory required for data: 4229432028
I0211 13:04:07.207893 15724 layer_factory.hpp:77] Creating layer BatchNorm37
I0211 13:04:07.207902 15724 net.cpp:106] Creating Layer BatchNorm37
I0211 13:04:07.207908 15724 net.cpp:454] BatchNorm37 <- Pooling4
I0211 13:04:07.207917 15724 net.cpp:411] BatchNorm37 -> BatchNorm37
I0211 13:04:07.208267 15724 net.cpp:150] Setting up BatchNorm37
I0211 13:04:07.208279 15724 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0211 13:04:07.208286 15724 net.cpp:165] Memory required for data: 4230404828
I0211 13:04:07.208297 15724 layer_factory.hpp:77] Creating layer Scale37
I0211 13:04:07.208305 15724 net.cpp:106] Creating Layer Scale37
I0211 13:04:07.208312 15724 net.cpp:454] Scale37 <- BatchNorm37
I0211 13:04:07.208319 15724 net.cpp:397] Scale37 -> BatchNorm37 (in-place)
I0211 13:04:07.208390 15724 layer_factory.hpp:77] Creating layer Scale37
I0211 13:04:07.208600 15724 net.cpp:150] Setting up Scale37
I0211 13:04:07.208611 15724 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0211 13:04:07.208616 15724 net.cpp:165] Memory required for data: 4231377628
I0211 13:04:07.208624 15724 layer_factory.hpp:77] Creating layer ReLU37
I0211 13:04:07.208632 15724 net.cpp:106] Creating Layer ReLU37
I0211 13:04:07.208638 15724 net.cpp:454] ReLU37 <- BatchNorm37
I0211 13:04:07.208647 15724 net.cpp:397] ReLU37 -> BatchNorm37 (in-place)
I0211 13:04:07.208655 15724 net.cpp:150] Setting up ReLU37
I0211 13:04:07.208662 15724 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0211 13:04:07.208667 15724 net.cpp:165] Memory required for data: 4232350428
I0211 13:04:07.208673 15724 layer_factory.hpp:77] Creating layer rpn_conv/3x3
I0211 13:04:07.208683 15724 net.cpp:106] Creating Layer rpn_conv/3x3
I0211 13:04:07.208688 15724 net.cpp:454] rpn_conv/3x3 <- BatchNorm37
I0211 13:04:07.208695 15724 net.cpp:411] rpn_conv/3x3 -> rpn/output
I0211 13:04:07.265204 15724 net.cpp:150] Setting up rpn_conv/3x3
I0211 13:04:07.265238 15724 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0211 13:04:07.265244 15724 net.cpp:165] Memory required for data: 4233595612
I0211 13:04:07.265254 15724 layer_factory.hpp:77] Creating layer rpn_relu/3x3
I0211 13:04:07.265266 15724 net.cpp:106] Creating Layer rpn_relu/3x3
I0211 13:04:07.265274 15724 net.cpp:454] rpn_relu/3x3 <- rpn/output
I0211 13:04:07.265283 15724 net.cpp:397] rpn_relu/3x3 -> rpn/output (in-place)
I0211 13:04:07.265295 15724 net.cpp:150] Setting up rpn_relu/3x3
I0211 13:04:07.265301 15724 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0211 13:04:07.265307 15724 net.cpp:165] Memory required for data: 4234840796
I0211 13:04:07.265311 15724 layer_factory.hpp:77] Creating layer rpn/output_rpn_relu/3x3_0_split
I0211 13:04:07.265352 15724 net.cpp:106] Creating Layer rpn/output_rpn_relu/3x3_0_split
I0211 13:04:07.265360 15724 net.cpp:454] rpn/output_rpn_relu/3x3_0_split <- rpn/output
I0211 13:04:07.265367 15724 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_0
I0211 13:04:07.265377 15724 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_1
I0211 13:04:07.265442 15724 net.cpp:150] Setting up rpn/output_rpn_relu/3x3_0_split
I0211 13:04:07.265455 15724 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0211 13:04:07.265460 15724 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0211 13:04:07.265466 15724 net.cpp:165] Memory required for data: 4237331164
I0211 13:04:07.265471 15724 layer_factory.hpp:77] Creating layer rpn_cls_score
I0211 13:04:07.265487 15724 net.cpp:106] Creating Layer rpn_cls_score
I0211 13:04:07.265493 15724 net.cpp:454] rpn_cls_score <- rpn/output_rpn_relu/3x3_0_split_0
I0211 13:04:07.265501 15724 net.cpp:411] rpn_cls_score -> rpn_cls_score
I0211 13:04:07.266201 15724 net.cpp:150] Setting up rpn_cls_score
I0211 13:04:07.266214 15724 net.cpp:157] Top shape: 1 18 19 32 (10944)
I0211 13:04:07.266221 15724 net.cpp:165] Memory required for data: 4237374940
I0211 13:04:07.266227 15724 layer_factory.hpp:77] Creating layer rpn_cls_score_rpn_cls_score_0_split
I0211 13:04:07.266237 15724 net.cpp:106] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I0211 13:04:07.266242 15724 net.cpp:454] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I0211 13:04:07.266252 15724 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I0211 13:04:07.266258 15724 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I0211 13:04:07.266322 15724 net.cpp:150] Setting up rpn_cls_score_rpn_cls_score_0_split
I0211 13:04:07.266333 15724 net.cpp:157] Top shape: 1 18 19 32 (10944)
I0211 13:04:07.266340 15724 net.cpp:157] Top shape: 1 18 19 32 (10944)
I0211 13:04:07.266345 15724 net.cpp:165] Memory required for data: 4237462492
I0211 13:04:07.266350 15724 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I0211 13:04:07.266365 15724 net.cpp:106] Creating Layer rpn_bbox_pred
I0211 13:04:07.266371 15724 net.cpp:454] rpn_bbox_pred <- rpn/output_rpn_relu/3x3_0_split_1
I0211 13:04:07.266381 15724 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I0211 13:04:07.267348 15724 net.cpp:150] Setting up rpn_bbox_pred
I0211 13:04:07.267361 15724 net.cpp:157] Top shape: 1 36 19 32 (21888)
I0211 13:04:07.267366 15724 net.cpp:165] Memory required for data: 4237550044
I0211 13:04:07.267374 15724 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I0211 13:04:07.267386 15724 net.cpp:106] Creating Layer rpn_cls_score_reshape
I0211 13:04:07.267392 15724 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I0211 13:04:07.267401 15724 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0211 13:04:07.267451 15724 net.cpp:150] Setting up rpn_cls_score_reshape
I0211 13:04:07.267462 15724 net.cpp:157] Top shape: 1 2 171 32 (10944)
I0211 13:04:07.267467 15724 net.cpp:165] Memory required for data: 4237593820
I0211 13:04:07.267472 15724 layer_factory.hpp:77] Creating layer rpn-data
I0211 13:04:07.268098 15724 net.cpp:106] Creating Layer rpn-data
I0211 13:04:07.268112 15724 net.cpp:454] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I0211 13:04:07.268121 15724 net.cpp:454] rpn-data <- gt_boxes
I0211 13:04:07.268126 15724 net.cpp:454] rpn-data <- im_info
I0211 13:04:07.268132 15724 net.cpp:454] rpn-data <- data_input-data_0_split_1
I0211 13:04:07.268141 15724 net.cpp:411] rpn-data -> rpn_labels
I0211 13:04:07.268151 15724 net.cpp:411] rpn-data -> rpn_bbox_targets
I0211 13:04:07.268162 15724 net.cpp:411] rpn-data -> rpn_bbox_inside_weights
I0211 13:04:07.268170 15724 net.cpp:411] rpn-data -> rpn_bbox_outside_weights
I0211 13:04:07.270617 15724 net.cpp:150] Setting up rpn-data
I0211 13:04:07.270634 15724 net.cpp:157] Top shape: 1 1 171 32 (5472)
I0211 13:04:07.270642 15724 net.cpp:157] Top shape: 1 36 19 32 (21888)
I0211 13:04:07.270649 15724 net.cpp:157] Top shape: 1 36 19 32 (21888)
I0211 13:04:07.270656 15724 net.cpp:157] Top shape: 1 36 19 32 (21888)
I0211 13:04:07.270661 15724 net.cpp:165] Memory required for data: 4237878364
I0211 13:04:07.270666 15724 layer_factory.hpp:77] Creating layer rpn_loss_cls
I0211 13:04:07.270678 15724 net.cpp:106] Creating Layer rpn_loss_cls
I0211 13:04:07.270684 15724 net.cpp:454] rpn_loss_cls <- rpn_cls_score_reshape
I0211 13:04:07.270692 15724 net.cpp:454] rpn_loss_cls <- rpn_labels
I0211 13:04:07.270699 15724 net.cpp:411] rpn_loss_cls -> rpn_cls_loss
I0211 13:04:07.270712 15724 layer_factory.hpp:77] Creating layer rpn_loss_cls
I0211 13:04:07.270900 15724 net.cpp:150] Setting up rpn_loss_cls
I0211 13:04:07.270911 15724 net.cpp:157] Top shape: (1)
I0211 13:04:07.270916 15724 net.cpp:160]     with loss weight 1
I0211 13:04:07.270927 15724 net.cpp:165] Memory required for data: 4237878368
I0211 13:04:07.270932 15724 layer_factory.hpp:77] Creating layer rpn_loss_bbox
I0211 13:04:07.270943 15724 net.cpp:106] Creating Layer rpn_loss_bbox
I0211 13:04:07.270949 15724 net.cpp:454] rpn_loss_bbox <- rpn_bbox_pred
I0211 13:04:07.270956 15724 net.cpp:454] rpn_loss_bbox <- rpn_bbox_targets
I0211 13:04:07.270963 15724 net.cpp:454] rpn_loss_bbox <- rpn_bbox_inside_weights
I0211 13:04:07.270969 15724 net.cpp:454] rpn_loss_bbox <- rpn_bbox_outside_weights
I0211 13:04:07.270978 15724 net.cpp:411] rpn_loss_bbox -> rpn_loss_bbox
I0211 13:04:07.271244 15724 net.cpp:150] Setting up rpn_loss_bbox
I0211 13:04:07.271258 15724 net.cpp:157] Top shape: (1)
I0211 13:04:07.271263 15724 net.cpp:160]     with loss weight 1
I0211 13:04:07.271271 15724 net.cpp:165] Memory required for data: 4237878372
I0211 13:04:07.271276 15724 net.cpp:226] rpn_loss_bbox needs backward computation.
I0211 13:04:07.271282 15724 net.cpp:226] rpn_loss_cls needs backward computation.
I0211 13:04:07.271287 15724 net.cpp:226] rpn-data needs backward computation.
I0211 13:04:07.271296 15724 net.cpp:226] rpn_cls_score_reshape needs backward computation.
I0211 13:04:07.271301 15724 net.cpp:226] rpn_bbox_pred needs backward computation.
I0211 13:04:07.271306 15724 net.cpp:226] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I0211 13:04:07.271311 15724 net.cpp:226] rpn_cls_score needs backward computation.
I0211 13:04:07.271314 15724 net.cpp:226] rpn/output_rpn_relu/3x3_0_split needs backward computation.
I0211 13:04:07.271319 15724 net.cpp:226] rpn_relu/3x3 needs backward computation.
I0211 13:04:07.271324 15724 net.cpp:226] rpn_conv/3x3 needs backward computation.
I0211 13:04:07.271329 15724 net.cpp:226] ReLU37 needs backward computation.
I0211 13:04:07.271334 15724 net.cpp:226] Scale37 needs backward computation.
I0211 13:04:07.271338 15724 net.cpp:226] BatchNorm37 needs backward computation.
I0211 13:04:07.271344 15724 net.cpp:226] Pooling4 needs backward computation.
I0211 13:04:07.271350 15724 net.cpp:226] Dropout36 needs backward computation.
I0211 13:04:07.271355 15724 net.cpp:226] Convolution37 needs backward computation.
I0211 13:04:07.271361 15724 net.cpp:226] ReLU36 needs backward computation.
I0211 13:04:07.271365 15724 net.cpp:226] Scale36 needs backward computation.
I0211 13:04:07.271370 15724 net.cpp:226] BatchNorm36 needs backward computation.
I0211 13:04:07.271374 15724 net.cpp:226] Concat32 needs backward computation.
I0211 13:04:07.271380 15724 net.cpp:226] Dropout35 needs backward computation.
I0211 13:04:07.271386 15724 net.cpp:226] Convolution36 needs backward computation.
I0211 13:04:07.271391 15724 net.cpp:226] ReLU35 needs backward computation.
I0211 13:04:07.271396 15724 net.cpp:226] Scale35 needs backward computation.
I0211 13:04:07.271401 15724 net.cpp:226] BatchNorm35 needs backward computation.
I0211 13:04:07.271406 15724 net.cpp:226] Concat31_Concat31_0_split needs backward computation.
I0211 13:04:07.271411 15724 net.cpp:226] Concat31 needs backward computation.
I0211 13:04:07.271416 15724 net.cpp:226] Dropout34 needs backward computation.
I0211 13:04:07.271422 15724 net.cpp:226] Convolution35 needs backward computation.
I0211 13:04:07.271426 15724 net.cpp:226] ReLU34 needs backward computation.
I0211 13:04:07.271431 15724 net.cpp:226] Scale34 needs backward computation.
I0211 13:04:07.271435 15724 net.cpp:226] BatchNorm34 needs backward computation.
I0211 13:04:07.271441 15724 net.cpp:226] Concat30_Concat30_0_split needs backward computation.
I0211 13:04:07.271447 15724 net.cpp:226] Concat30 needs backward computation.
I0211 13:04:07.271453 15724 net.cpp:226] Dropout33 needs backward computation.
I0211 13:04:07.271458 15724 net.cpp:226] Convolution34 needs backward computation.
I0211 13:04:07.271463 15724 net.cpp:226] ReLU33 needs backward computation.
I0211 13:04:07.271468 15724 net.cpp:226] Scale33 needs backward computation.
I0211 13:04:07.271472 15724 net.cpp:226] BatchNorm33 needs backward computation.
I0211 13:04:07.271477 15724 net.cpp:226] Concat29_Concat29_0_split needs backward computation.
I0211 13:04:07.271482 15724 net.cpp:226] Concat29 needs backward computation.
I0211 13:04:07.271487 15724 net.cpp:226] Dropout32 needs backward computation.
I0211 13:04:07.271493 15724 net.cpp:226] Convolution33 needs backward computation.
I0211 13:04:07.271497 15724 net.cpp:226] ReLU32 needs backward computation.
I0211 13:04:07.271502 15724 net.cpp:226] Scale32 needs backward computation.
I0211 13:04:07.271507 15724 net.cpp:226] BatchNorm32 needs backward computation.
I0211 13:04:07.271512 15724 net.cpp:226] Concat28_Concat28_0_split needs backward computation.
I0211 13:04:07.271517 15724 net.cpp:226] Concat28 needs backward computation.
I0211 13:04:07.271522 15724 net.cpp:226] Dropout31 needs backward computation.
I0211 13:04:07.271527 15724 net.cpp:226] Convolution32 needs backward computation.
I0211 13:04:07.271530 15724 net.cpp:226] ReLU31 needs backward computation.
I0211 13:04:07.271535 15724 net.cpp:226] Scale31 needs backward computation.
I0211 13:04:07.271540 15724 net.cpp:226] BatchNorm31 needs backward computation.
I0211 13:04:07.271546 15724 net.cpp:226] Concat27_Concat27_0_split needs backward computation.
I0211 13:04:07.271553 15724 net.cpp:226] Concat27 needs backward computation.
I0211 13:04:07.271558 15724 net.cpp:226] Dropout30 needs backward computation.
I0211 13:04:07.271562 15724 net.cpp:226] Convolution31 needs backward computation.
I0211 13:04:07.271567 15724 net.cpp:226] ReLU30 needs backward computation.
I0211 13:04:07.271572 15724 net.cpp:226] Scale30 needs backward computation.
I0211 13:04:07.271576 15724 net.cpp:226] BatchNorm30 needs backward computation.
I0211 13:04:07.271581 15724 net.cpp:226] Concat26_Concat26_0_split needs backward computation.
I0211 13:04:07.271587 15724 net.cpp:226] Concat26 needs backward computation.
I0211 13:04:07.271594 15724 net.cpp:226] Dropout29 needs backward computation.
I0211 13:04:07.271600 15724 net.cpp:226] Convolution30 needs backward computation.
I0211 13:04:07.271603 15724 net.cpp:226] ReLU29 needs backward computation.
I0211 13:04:07.271610 15724 net.cpp:226] Scale29 needs backward computation.
I0211 13:04:07.271613 15724 net.cpp:226] BatchNorm29 needs backward computation.
I0211 13:04:07.271617 15724 net.cpp:226] Concat25_Concat25_0_split needs backward computation.
I0211 13:04:07.271623 15724 net.cpp:226] Concat25 needs backward computation.
I0211 13:04:07.271627 15724 net.cpp:226] Dropout28 needs backward computation.
I0211 13:04:07.271633 15724 net.cpp:226] Convolution29 needs backward computation.
I0211 13:04:07.271638 15724 net.cpp:226] ReLU28 needs backward computation.
I0211 13:04:07.271643 15724 net.cpp:226] Scale28 needs backward computation.
I0211 13:04:07.271647 15724 net.cpp:226] BatchNorm28 needs backward computation.
I0211 13:04:07.271653 15724 net.cpp:226] Pooling3_Pooling3_0_split needs backward computation.
I0211 13:04:07.271658 15724 net.cpp:226] Pooling3 needs backward computation.
I0211 13:04:07.271664 15724 net.cpp:226] Dropout27 needs backward computation.
I0211 13:04:07.271669 15724 net.cpp:226] Convolution28 needs backward computation.
I0211 13:04:07.271674 15724 net.cpp:226] ReLU27 needs backward computation.
I0211 13:04:07.271678 15724 net.cpp:226] Scale27 needs backward computation.
I0211 13:04:07.271683 15724 net.cpp:226] BatchNorm27 needs backward computation.
I0211 13:04:07.271688 15724 net.cpp:226] Concat24 needs backward computation.
I0211 13:04:07.271693 15724 net.cpp:226] Dropout26 needs backward computation.
I0211 13:04:07.271698 15724 net.cpp:226] Convolution27 needs backward computation.
I0211 13:04:07.271703 15724 net.cpp:226] ReLU26 needs backward computation.
I0211 13:04:07.271708 15724 net.cpp:226] Scale26 needs backward computation.
I0211 13:04:07.271713 15724 net.cpp:226] BatchNorm26 needs backward computation.
I0211 13:04:07.271718 15724 net.cpp:226] Concat23_Concat23_0_split needs backward computation.
I0211 13:04:07.271723 15724 net.cpp:226] Concat23 needs backward computation.
I0211 13:04:07.271729 15724 net.cpp:226] Dropout25 needs backward computation.
I0211 13:04:07.271734 15724 net.cpp:226] Convolution26 needs backward computation.
I0211 13:04:07.271739 15724 net.cpp:226] ReLU25 needs backward computation.
I0211 13:04:07.271744 15724 net.cpp:226] Scale25 needs backward computation.
I0211 13:04:07.271749 15724 net.cpp:226] BatchNorm25 needs backward computation.
I0211 13:04:07.271757 15724 net.cpp:226] Concat22_Concat22_0_split needs backward computation.
I0211 13:04:07.271764 15724 net.cpp:226] Concat22 needs backward computation.
I0211 13:04:07.271767 15724 net.cpp:226] Dropout24 needs backward computation.
I0211 13:04:07.271773 15724 net.cpp:226] Convolution25 needs backward computation.
I0211 13:04:07.271777 15724 net.cpp:226] ReLU24 needs backward computation.
I0211 13:04:07.271781 15724 net.cpp:226] Scale24 needs backward computation.
I0211 13:04:07.271785 15724 net.cpp:226] BatchNorm24 needs backward computation.
I0211 13:04:07.271790 15724 net.cpp:226] Concat21_Concat21_0_split needs backward computation.
I0211 13:04:07.271796 15724 net.cpp:226] Concat21 needs backward computation.
I0211 13:04:07.271801 15724 net.cpp:226] Dropout23 needs backward computation.
I0211 13:04:07.271806 15724 net.cpp:226] Convolution24 needs backward computation.
I0211 13:04:07.271811 15724 net.cpp:226] ReLU23 needs backward computation.
I0211 13:04:07.271816 15724 net.cpp:226] Scale23 needs backward computation.
I0211 13:04:07.271821 15724 net.cpp:226] BatchNorm23 needs backward computation.
I0211 13:04:07.271826 15724 net.cpp:226] Concat20_Concat20_0_split needs backward computation.
I0211 13:04:07.271831 15724 net.cpp:226] Concat20 needs backward computation.
I0211 13:04:07.271836 15724 net.cpp:226] Dropout22 needs backward computation.
I0211 13:04:07.271842 15724 net.cpp:226] Convolution23 needs backward computation.
I0211 13:04:07.271847 15724 net.cpp:226] ReLU22 needs backward computation.
I0211 13:04:07.271852 15724 net.cpp:226] Scale22 needs backward computation.
I0211 13:04:07.271857 15724 net.cpp:226] BatchNorm22 needs backward computation.
I0211 13:04:07.271862 15724 net.cpp:226] Concat19_Concat19_0_split needs backward computation.
I0211 13:04:07.271867 15724 net.cpp:226] Concat19 needs backward computation.
I0211 13:04:07.271872 15724 net.cpp:226] Dropout21 needs backward computation.
I0211 13:04:07.271878 15724 net.cpp:226] Convolution22 needs backward computation.
I0211 13:04:07.271883 15724 net.cpp:226] ReLU21 needs backward computation.
I0211 13:04:07.271888 15724 net.cpp:226] Scale21 needs backward computation.
I0211 13:04:07.271893 15724 net.cpp:226] BatchNorm21 needs backward computation.
I0211 13:04:07.271898 15724 net.cpp:226] Concat18_Concat18_0_split needs backward computation.
I0211 13:04:07.271903 15724 net.cpp:226] Concat18 needs backward computation.
I0211 13:04:07.271908 15724 net.cpp:226] Dropout20 needs backward computation.
I0211 13:04:07.271914 15724 net.cpp:226] Convolution21 needs backward computation.
I0211 13:04:07.271919 15724 net.cpp:226] ReLU20 needs backward computation.
I0211 13:04:07.271924 15724 net.cpp:226] Scale20 needs backward computation.
I0211 13:04:07.271929 15724 net.cpp:226] BatchNorm20 needs backward computation.
I0211 13:04:07.271932 15724 net.cpp:226] Concat17_Concat17_0_split needs backward computation.
I0211 13:04:07.271939 15724 net.cpp:226] Concat17 needs backward computation.
I0211 13:04:07.271944 15724 net.cpp:226] Dropout19 needs backward computation.
I0211 13:04:07.271948 15724 net.cpp:226] Convolution20 needs backward computation.
I0211 13:04:07.271953 15724 net.cpp:226] ReLU19 needs backward computation.
I0211 13:04:07.271958 15724 net.cpp:226] Scale19 needs backward computation.
I0211 13:04:07.271963 15724 net.cpp:226] BatchNorm19 needs backward computation.
I0211 13:04:07.271970 15724 net.cpp:226] Pooling2_Pooling2_0_split needs backward computation.
I0211 13:04:07.271975 15724 net.cpp:226] Pooling2 needs backward computation.
I0211 13:04:07.271980 15724 net.cpp:226] Dropout18 needs backward computation.
I0211 13:04:07.271984 15724 net.cpp:226] Convolution19 needs backward computation.
I0211 13:04:07.271991 15724 net.cpp:226] ReLU18 needs backward computation.
I0211 13:04:07.271994 15724 net.cpp:226] Scale18 needs backward computation.
I0211 13:04:07.271999 15724 net.cpp:226] BatchNorm18 needs backward computation.
I0211 13:04:07.272004 15724 net.cpp:226] Concat16 needs backward computation.
I0211 13:04:07.272009 15724 net.cpp:226] Dropout17 needs backward computation.
I0211 13:04:07.272016 15724 net.cpp:226] Convolution18 needs backward computation.
I0211 13:04:07.272022 15724 net.cpp:226] ReLU17 needs backward computation.
I0211 13:04:07.272025 15724 net.cpp:226] Scale17 needs backward computation.
I0211 13:04:07.272030 15724 net.cpp:226] BatchNorm17 needs backward computation.
I0211 13:04:07.272035 15724 net.cpp:226] Concat15_Concat15_0_split needs backward computation.
I0211 13:04:07.272040 15724 net.cpp:226] Concat15 needs backward computation.
I0211 13:04:07.272045 15724 net.cpp:226] Dropout16 needs backward computation.
I0211 13:04:07.272050 15724 net.cpp:226] Convolution17 needs backward computation.
I0211 13:04:07.272058 15724 net.cpp:226] ReLU16 needs backward computation.
I0211 13:04:07.272063 15724 net.cpp:226] Scale16 needs backward computation.
I0211 13:04:07.272068 15724 net.cpp:226] BatchNorm16 needs backward computation.
I0211 13:04:07.272073 15724 net.cpp:226] Concat14_Concat14_0_split needs backward computation.
I0211 13:04:07.272079 15724 net.cpp:226] Concat14 needs backward computation.
I0211 13:04:07.272086 15724 net.cpp:226] Dropout15 needs backward computation.
I0211 13:04:07.272091 15724 net.cpp:226] Convolution16 needs backward computation.
I0211 13:04:07.272096 15724 net.cpp:226] ReLU15 needs backward computation.
I0211 13:04:07.272101 15724 net.cpp:226] Scale15 needs backward computation.
I0211 13:04:07.272105 15724 net.cpp:226] BatchNorm15 needs backward computation.
I0211 13:04:07.272111 15724 net.cpp:226] Concat13_Concat13_0_split needs backward computation.
I0211 13:04:07.272117 15724 net.cpp:226] Concat13 needs backward computation.
I0211 13:04:07.272125 15724 net.cpp:226] Dropout14 needs backward computation.
I0211 13:04:07.272130 15724 net.cpp:226] Convolution15 needs backward computation.
I0211 13:04:07.272135 15724 net.cpp:226] ReLU14 needs backward computation.
I0211 13:04:07.272140 15724 net.cpp:226] Scale14 needs backward computation.
I0211 13:04:07.272145 15724 net.cpp:226] BatchNorm14 needs backward computation.
I0211 13:04:07.272150 15724 net.cpp:226] Concat12_Concat12_0_split needs backward computation.
I0211 13:04:07.272156 15724 net.cpp:226] Concat12 needs backward computation.
I0211 13:04:07.272161 15724 net.cpp:226] Dropout13 needs backward computation.
I0211 13:04:07.272166 15724 net.cpp:226] Convolution14 needs backward computation.
I0211 13:04:07.272171 15724 net.cpp:226] ReLU13 needs backward computation.
I0211 13:04:07.272176 15724 net.cpp:226] Scale13 needs backward computation.
I0211 13:04:07.272179 15724 net.cpp:226] BatchNorm13 needs backward computation.
I0211 13:04:07.272186 15724 net.cpp:226] Concat11_Concat11_0_split needs backward computation.
I0211 13:04:07.272192 15724 net.cpp:226] Concat11 needs backward computation.
I0211 13:04:07.272197 15724 net.cpp:226] Dropout12 needs backward computation.
I0211 13:04:07.272203 15724 net.cpp:226] Convolution13 needs backward computation.
I0211 13:04:07.272209 15724 net.cpp:226] ReLU12 needs backward computation.
I0211 13:04:07.272213 15724 net.cpp:226] Scale12 needs backward computation.
I0211 13:04:07.272218 15724 net.cpp:226] BatchNorm12 needs backward computation.
I0211 13:04:07.272223 15724 net.cpp:226] Concat10_Concat10_0_split needs backward computation.
I0211 13:04:07.272229 15724 net.cpp:226] Concat10 needs backward computation.
I0211 13:04:07.272234 15724 net.cpp:226] Dropout11 needs backward computation.
I0211 13:04:07.272240 15724 net.cpp:226] Convolution12 needs backward computation.
I0211 13:04:07.272244 15724 net.cpp:226] ReLU11 needs backward computation.
I0211 13:04:07.272250 15724 net.cpp:226] Scale11 needs backward computation.
I0211 13:04:07.272254 15724 net.cpp:226] BatchNorm11 needs backward computation.
I0211 13:04:07.272260 15724 net.cpp:226] Concat9_Concat9_0_split needs backward computation.
I0211 13:04:07.272265 15724 net.cpp:226] Concat9 needs backward computation.
I0211 13:04:07.272271 15724 net.cpp:226] Dropout10 needs backward computation.
I0211 13:04:07.272279 15724 net.cpp:226] Convolution11 needs backward computation.
I0211 13:04:07.272284 15724 net.cpp:226] ReLU10 needs backward computation.
I0211 13:04:07.272289 15724 net.cpp:226] Scale10 needs backward computation.
I0211 13:04:07.272294 15724 net.cpp:226] BatchNorm10 needs backward computation.
I0211 13:04:07.272300 15724 net.cpp:226] Pooling1_Pooling1_0_split needs backward computation.
I0211 13:04:07.272305 15724 net.cpp:226] Pooling1 needs backward computation.
I0211 13:04:07.272310 15724 net.cpp:226] Dropout9 needs backward computation.
I0211 13:04:07.272315 15724 net.cpp:226] Convolution10 needs backward computation.
I0211 13:04:07.272320 15724 net.cpp:226] ReLU9 needs backward computation.
I0211 13:04:07.272325 15724 net.cpp:226] Scale9 needs backward computation.
I0211 13:04:07.272328 15724 net.cpp:226] BatchNorm9 needs backward computation.
I0211 13:04:07.272333 15724 net.cpp:226] Concat8 needs backward computation.
I0211 13:04:07.272338 15724 net.cpp:226] Dropout8 needs backward computation.
I0211 13:04:07.272343 15724 net.cpp:226] Convolution9 needs backward computation.
I0211 13:04:07.272348 15724 net.cpp:226] ReLU8 needs backward computation.
I0211 13:04:07.272352 15724 net.cpp:226] Scale8 needs backward computation.
I0211 13:04:07.272356 15724 net.cpp:226] BatchNorm8 needs backward computation.
I0211 13:04:07.272361 15724 net.cpp:226] Concat7_Concat7_0_split needs backward computation.
I0211 13:04:07.272366 15724 net.cpp:226] Concat7 needs backward computation.
I0211 13:04:07.272370 15724 net.cpp:226] Dropout7 needs backward computation.
I0211 13:04:07.272375 15724 net.cpp:226] Convolution8 needs backward computation.
I0211 13:04:07.272380 15724 net.cpp:226] ReLU7 needs backward computation.
I0211 13:04:07.272384 15724 net.cpp:226] Scale7 needs backward computation.
I0211 13:04:07.272388 15724 net.cpp:226] BatchNorm7 needs backward computation.
I0211 13:04:07.272392 15724 net.cpp:226] Concat6_Concat6_0_split needs backward computation.
I0211 13:04:07.272397 15724 net.cpp:226] Concat6 needs backward computation.
I0211 13:04:07.272403 15724 net.cpp:226] Dropout6 needs backward computation.
I0211 13:04:07.272406 15724 net.cpp:226] Convolution7 needs backward computation.
I0211 13:04:07.272411 15724 net.cpp:226] ReLU6 needs backward computation.
I0211 13:04:07.272415 15724 net.cpp:226] Scale6 needs backward computation.
I0211 13:04:07.272419 15724 net.cpp:226] BatchNorm6 needs backward computation.
I0211 13:04:07.272424 15724 net.cpp:226] Concat5_Concat5_0_split needs backward computation.
I0211 13:04:07.272428 15724 net.cpp:226] Concat5 needs backward computation.
I0211 13:04:07.272433 15724 net.cpp:226] Dropout5 needs backward computation.
I0211 13:04:07.272439 15724 net.cpp:226] Convolution6 needs backward computation.
I0211 13:04:07.272442 15724 net.cpp:226] ReLU5 needs backward computation.
I0211 13:04:07.272446 15724 net.cpp:226] Scale5 needs backward computation.
I0211 13:04:07.272450 15724 net.cpp:226] BatchNorm5 needs backward computation.
I0211 13:04:07.272455 15724 net.cpp:226] Concat4_Concat4_0_split needs backward computation.
I0211 13:04:07.272459 15724 net.cpp:226] Concat4 needs backward computation.
I0211 13:04:07.272464 15724 net.cpp:226] Dropout4 needs backward computation.
I0211 13:04:07.272469 15724 net.cpp:226] Convolution5 needs backward computation.
I0211 13:04:07.272474 15724 net.cpp:226] ReLU4 needs backward computation.
I0211 13:04:07.272477 15724 net.cpp:226] Scale4 needs backward computation.
I0211 13:04:07.272482 15724 net.cpp:226] BatchNorm4 needs backward computation.
I0211 13:04:07.272486 15724 net.cpp:226] Concat3_Concat3_0_split needs backward computation.
I0211 13:04:07.272490 15724 net.cpp:226] Concat3 needs backward computation.
I0211 13:04:07.272495 15724 net.cpp:226] Dropout3 needs backward computation.
I0211 13:04:07.272500 15724 net.cpp:226] Convolution4 needs backward computation.
I0211 13:04:07.272505 15724 net.cpp:226] ReLU3 needs backward computation.
I0211 13:04:07.272510 15724 net.cpp:226] Scale3 needs backward computation.
I0211 13:04:07.272513 15724 net.cpp:226] BatchNorm3 needs backward computation.
I0211 13:04:07.272518 15724 net.cpp:226] Concat2_Concat2_0_split needs backward computation.
I0211 13:04:07.272522 15724 net.cpp:226] Concat2 needs backward computation.
I0211 13:04:07.272527 15724 net.cpp:226] Dropout2 needs backward computation.
I0211 13:04:07.272532 15724 net.cpp:226] Convolution3 needs backward computation.
I0211 13:04:07.272536 15724 net.cpp:226] ReLU2 needs backward computation.
I0211 13:04:07.272541 15724 net.cpp:226] Scale2 needs backward computation.
I0211 13:04:07.272544 15724 net.cpp:226] BatchNorm2 needs backward computation.
I0211 13:04:07.272549 15724 net.cpp:226] Concat1_Concat1_0_split needs backward computation.
I0211 13:04:07.272554 15724 net.cpp:226] Concat1 needs backward computation.
I0211 13:04:07.272562 15724 net.cpp:226] Dropout1 needs backward computation.
I0211 13:04:07.272567 15724 net.cpp:226] Convolution2 needs backward computation.
I0211 13:04:07.272572 15724 net.cpp:226] ReLU1 needs backward computation.
I0211 13:04:07.272575 15724 net.cpp:226] Scale1 needs backward computation.
I0211 13:04:07.272579 15724 net.cpp:226] BatchNorm1 needs backward computation.
I0211 13:04:07.272584 15724 net.cpp:226] Convolution1_Convolution1_0_split needs backward computation.
I0211 13:04:07.272589 15724 net.cpp:226] Convolution1 needs backward computation.
I0211 13:04:07.272594 15724 net.cpp:228] data_input-data_0_split does not need backward computation.
I0211 13:04:07.272599 15724 net.cpp:228] input-data does not need backward computation.
I0211 13:04:07.272603 15724 net.cpp:270] This network produces output rpn_cls_loss
I0211 13:04:07.272608 15724 net.cpp:270] This network produces output rpn_loss_bbox
I0211 13:04:07.272764 15724 net.cpp:283] Network initialization done.
I0211 13:04:07.273282 15724 solver.cpp:60] Solver scaffolding done.
I0211 13:04:08.629324 15724 solver.cpp:229] Iteration 0, loss = 0.699527
I0211 13:04:08.629421 15724 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.693225 (* 1 = 0.693225 loss)
I0211 13:04:08.629442 15724 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0063019 (* 1 = 0.0063019 loss)
I0211 13:04:08.629461 15724 sgd_solver.cpp:106] Iteration 0, lr = 0.001
Init model: None
Using config:
{'DATA_DIR': '/home/duchenting/dct/py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'default',
 'GPU_ID': 1,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/duchenting/dct/py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/duchenting/dct/py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': False,
          'MAX_SIZE': 500,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [300],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.1,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 500,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [300],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for training
Set proposal method: gt
Appending horizontally-flipped training examples...
voc_2007_trainval gt roidb loaded from /home/duchenting/dct/py-faster-rcnn/data/cache/voc_2007_trainval_gt_roidb.pkl
done
Preparing training data...
done
roidb len: 10022
Output will be saved to `/home/duchenting/dct/py-faster-rcnn/output/default/voc_2007_trainval`
Filtered 0 roidb entries: 10022 -> 10022
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
anchors:
[[ -38.  -16.   53.   31.]
 [ -84.  -40.   99.   55.]
 [-176.  -88.  191.  103.]
 [ -24.  -24.   39.   39.]
 [ -56.  -56.   71.   71.]
 [-120. -120.  135.  135.]
 [ -14.  -36.   29.   51.]
 [ -36.  -80.   51.   95.]
 [ -80. -168.   95.  183.]]
anchor shapes:
[[  91.   47.]
 [ 183.   95.]
 [ 367.  191.]
 [  63.   63.]
 [ 127.  127.]
 [ 255.  255.]
 [  43.   87.]
 [  87.  175.]
 [ 175.  351.]]
AnchorTargetLayer: height 19 width 32
Solving...

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  30.39999962   48.          299.20001221  399.20001221   15.        ]
 [  35.20000076   79.19999695  291.20001221  399.20001221    2.        ]]
total_anchors 4275
inds_inside 1484
anchors.shape (1484, 4)
means:
[[ 0.00882812  0.03023437  0.00875913  0.23553948]]
stdevs:
[[ 0.03016232  0.08861029  0.01458148  0.02782761]]
rpn: max max_overlap 0.773491725959
rpn: num_positive 10
rpn: num_negative 246
rpn: num_positive avg 10
rpn: num_negative avg 246

im_size: (452.0, 300.0)
scale: 0.903614461422
height, width: (29, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  53.3132515   117.46987915  299.09637451  450.90362549   14.        ]
 [ 117.46987915   55.1204834   299.09637451  396.68673706   15.        ]]
total_anchors 4959
inds_inside 1758
anchors.shape (1758, 4)
means:
[[ 0.03649341  0.01950042  0.00869244  0.15105969]]
stdevs:
[[ 0.04150782  0.08267652  0.02859453  0.1319983 ]]
rpn: max max_overlap 0.879571022493
rpn: num_positive 13
rpn: num_negative 243
rpn: num_positive avg 11
rpn: num_negative avg 244

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[   0.           69.59999847   84.          253.6000061     9.        ]
 [ 106.40000153   86.40000153  178.3999939   201.6000061     9.        ]
 [ 187.19999695   99.19999695  255.19999695  182.3999939    12.        ]
 [ 124.80000305  185.6000061   216.          299.20001221   12.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.03254678  0.02191856 -0.03752214  0.06182927]]
stdevs:
[[ 0.05805037  0.0832615   0.10452859  0.22709456]]
rpn: max max_overlap 0.69018585955
rpn: num_positive 8
rpn: num_negative 248
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 209.6000061   177.6000061   230.3999939   239.19999695   15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.03677259  0.02121    -0.0778105   0.03744133]]
stdevs:
[[ 0.058672    0.08379132  0.18820971  0.24013605]]
rpn: max max_overlap 0.352448098206
rpn: num_positive 2
rpn: num_negative 254
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 453.0)
scale: 0.906344413757
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 309.06344604   79.75830841  396.07250977  290.93655396   15.        ]
 [  83.38368225  160.42295837  144.10876465  278.2477417     9.        ]]
total_anchors 4959
inds_inside 1753
anchors.shape (1753, 4)
means:
[[ 0.01883268  0.01675709 -0.04974832  0.06307943]]
stdevs:
[[ 0.07210392  0.08487381  0.19893617  0.23414081]]
rpn: max max_overlap 0.717281026142
rpn: num_positive 5
rpn: num_negative 251
rpn: num_positive avg 7
rpn: num_negative avg 248

im_size: (259.0, 500.0)
scale: 1.0
height, width: (17, 32)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[   5.   12.  492.  258.    7.]
 [  99.    3.  139.   79.   15.]
 [  66.   13.  100.   83.   15.]]
total_anchors 4896
inds_inside 1540
anchors.shape (1540, 4)
means:
[[ 0.01080444  0.00544397  0.06126207  0.12030464]]
stdevs:
[[ 0.07824423  0.08241999  0.22539936  0.2122452 ]]
rpn: max max_overlap 0.657392124499
rpn: num_positive 23
rpn: num_negative 233
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  20.          204.          218.3999939   299.20001221   11.        ]
 [ 164.           56.          332.          299.20001221   15.        ]
 [  74.40000153   81.59999847  226.3999939   210.3999939    15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.01045301  0.00793257  0.04209213  0.10016563]]
stdevs:
[[ 0.07846822  0.07778352  0.23678962  0.20131887]]
rpn: max max_overlap 0.849872500555
rpn: num_positive 11
rpn: num_negative 245
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[ 232.          170.3999939   326.3999939   272.            9.        ]
 [ 303.20001221  158.3999939   376.79998779  244.            9.        ]
 [ 194.3999939   138.3999939   235.19999695  189.6000061     9.        ]
 [ 144.          156.80000305  202.3999939   221.6000061     9.        ]
 [  64.80000305  138.3999939   102.40000153  177.6000061     9.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00559062  0.01062142  0.01000226  0.03317657]]
stdevs:
[[ 0.08015165  0.08180434  0.23664931  0.2726707 ]]
rpn: max max_overlap 0.706968145499
rpn: num_positive 13
rpn: num_negative 243
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (6, 5)
rpn: gt_boxes [[   1.60000002  197.6000061   354.3999939   299.20001221   14.        ]
 [  86.40000153  135.19999695  184.80000305  298.3999939    15.        ]
 [  85.59999847  109.59999847  226.3999939   299.20001221   15.        ]
 [ 226.3999939   175.19999695  277.6000061   271.20001221   15.        ]
 [ 187.19999695  128.80000305  288.          299.20001221   15.        ]
 [ 224.          116.80000305  336.          298.3999939    15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00438817  0.01460705  0.07204212  0.04532547]]
stdevs:
[[ 0.10951027  0.07893412  0.29293924  0.25326635]]
rpn: max max_overlap 0.740194308193
rpn: num_positive 18
rpn: num_negative 238
rpn: num_positive avg 11
rpn: num_negative avg 244

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  76.80000305  100.80000305  312.79998779  297.6000061    12.        ]
 [  66.40000153   28.          399.20001221  241.6000061    15.        ]
 [   0.            0.          399.20001221  297.6000061    18.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.0125842   0.00938101  0.04139623  0.06523664]]
stdevs:
[[ 0.10336209  0.08873152  0.2814935   0.24481341]]
rpn: max max_overlap 0.818668251944
rpn: num_positive 22
rpn: num_negative 234
rpn: num_positive avg 12
rpn: num_negative avg 243

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 176.94610596   45.80838394  328.74252319  299.10180664   15.        ]
 [ 277.54492188   32.33533096  425.74850464  299.10180664   15.        ]
 [ 140.11976624   17.96407127  253.29341125  160.77844238    9.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.01279359  0.00889378  0.00885413  0.06302863]]
stdevs:
[[ 0.10322075  0.08604005  0.29860264  0.23617909]]
rpn: max max_overlap 0.803439278577
rpn: num_positive 10
rpn: num_negative 246
rpn: num_positive avg 12
rpn: num_negative avg 243

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  20.79999924  140.80000305  183.19999695  273.6000061     7.        ]
 [   1.60000002  154.3999939    28.79999924  212.            7.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.01081541  0.00991165  0.01066872  0.05924612]]
stdevs:
[[ 0.104277    0.08512712  0.29878176  0.23612566]]
rpn: max max_overlap 0.701578266463
rpn: num_positive 4
rpn: num_negative 252
rpn: num_positive avg 11
rpn: num_negative avg 244

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  53.89221573    0.          356.58682251  299.10180664   15.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00925419  0.00851891  0.02339798  0.06716916]]
stdevs:
[[ 0.10210695  0.08306719  0.2899191   0.22814815]]
rpn: max max_overlap 0.719075084881
rpn: num_positive 12
rpn: num_negative 244
rpn: num_positive avg 11
rpn: num_negative avg 244

im_size: (300.0, 343.0)
scale: 0.686498880386
height, width: (19, 22)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   7.55148745  164.75971985  188.78718567  275.97253418    2.        ]
 [ 175.74371338  172.99771118  340.50341797  276.6590271     2.        ]]
total_anchors 3762
inds_inside 1174
anchors.shape (1174, 4)
means:
[[ 0.00938654  0.00805639  0.02132278  0.0685639 ]]
stdevs:
[[ 0.10102446  0.08320646  0.28653878  0.22541752]]
rpn: max max_overlap 0.810980020064
rpn: num_positive 4
rpn: num_negative 252
rpn: num_positive avg 11
rpn: num_negative avg 244

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 119.19999695   48.          314.3999939   251.19999695   20.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.0091501   0.00720698  0.00555271  0.05239455]]
stdevs:
[[ 0.09894309  0.08184567  0.28615     0.22918824]]
rpn: max max_overlap 0.611328716706
rpn: num_positive 9
rpn: num_negative 247
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 343.0)
scale: 0.686498880386
height, width: (19, 22)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 248.5125885   187.41418457  330.20596313  251.25857544    3.        ]]
total_anchors 3762
inds_inside 1174
anchoI0211 13:04:25.208204 15724 solver.cpp:229] Iteration 20, loss = 0.43098
I0211 13:04:25.208261 15724 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.42968 (* 1 = 0.42968 loss)
I0211 13:04:25.208274 15724 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00129992 (* 1 = 0.00129992 loss)
I0211 13:04:25.208284 15724 sgd_solver.cpp:106] Iteration 20, lr = 0.001
rs.shape (1174, 4)
means:
[[ 0.00851314  0.00752659  0.00707213  0.05215644]]
stdevs:
[[ 0.0989795   0.08169987  0.28594437  0.22851302]]
rpn: max max_overlap 0.694802750955
rpn: num_positive 1
rpn: num_negative 255
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 100.          108.          320.          210.3999939     6.        ]
 [ 327.20001221  164.          339.20001221  199.19999695   15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00654503  0.00628481 -0.03094647  0.02041814]]
stdevs:
[[ 0.10265054  0.08507588  0.35765361  0.28028735]]
rpn: max max_overlap 0.717671340124
rpn: num_positive 10
rpn: num_negative 246
rpn: num_positive avg 10
rpn: num_negative avg 245

im_size: (300.0, 448.0)
scale: 0.895522415638
height, width: (19, 28)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 313.43283081  174.62686157  385.97015381  203.28358459    7.        ]
 [ 224.77612305  172.83581543  329.55224609  212.23880005    7.        ]
 [ 171.94029236  174.62686157  224.77612305  214.92536926    7.        ]]
total_anchors 4788
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00643418  0.00654087 -0.0326591   0.01141481]]
stdevs:
[[ 0.10165967  0.0864035   0.35447224  0.28405619]]
rpn: max max_overlap 0.650867004007
rpn: num_positive 4
rpn: num_negative 252
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 113.59999847   48.          259.20001221  245.6000061    12.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00678876  0.00696143 -0.02897961  0.02076667]]
stdevs:
[[ 0.10057048  0.08796142  0.35143984  0.28781585]]
rpn: max max_overlap 0.56273855466
rpn: num_positive 4
rpn: num_negative 252
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 159.19999695   70.40000153  324.          299.20001221   15.        ]
 [  94.40000153   88.80000305  191.19999695  213.6000061    15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00751932  0.00845866 -0.04217609  0.01697783]]
stdevs:
[[ 0.10008806  0.08687646  0.35173529  0.2832368 ]]
rpn: max max_overlap 0.75093015885
rpn: num_positive 7
rpn: num_negative 249
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 374.0)
scale: 0.748129665852
height, width: (19, 24)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 225.93516541   94.26433563  372.568573    227.431427     13.        ]
 [   0.           76.30922699  240.89775085  299.25186157   15.        ]]
total_anchors 4104
inds_inside 1339
anchors.shape (1339, 4)
means:
[[ 0.00740156  0.00884677 -0.04033531  0.01650835]]
stdevs:
[[ 0.09944646  0.08658244  0.34949468  0.2812526 ]]
rpn: max max_overlap 0.785973617864
rpn: num_positive 3
rpn: num_negative 253
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 129.6000061   119.19999695  243.19999695  291.20001221    8.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00673124  0.0090882  -0.03721278  0.0161627 ]]
stdevs:
[[ 0.09958057  0.08616992  0.34904819  0.27982716]]
rpn: max max_overlap 0.726042813762
rpn: num_positive 2
rpn: num_negative 254
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 140.           15.19999981  196.80000305   53.59999847    4.        ]]
total_anchors 4275
inds_inside 1484
anchors.shape (1484, 4)
means:
[[ 0.00676864  0.00863526 -0.03754279  0.01360515]]
stdevs:
[[ 0.09932759  0.08618223  0.34818712  0.28138806]]
rpn: max max_overlap 0.55598633864
rpn: num_positive 1
rpn: num_negative 255
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (327.0, 300.0)
scale: 0.680272102356
height, width: (21, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 114.96598816   46.25850296  174.1496582   200.           15.        ]
 [  51.02040863   82.99319458  262.58502197  274.1496582    13.        ]]
total_anchors 3591
inds_inside 1086
anchors.shape (1086, 4)
means:
[[ 0.00742569  0.01025596 -0.04730262 -0.00083408]]
stdevs:
[[ 0.09768856  0.08483371  0.34174063  0.28086345]]
rpn: max max_overlap 0.623255377064
rpn: num_positive 11
rpn: num_negative 245
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 110.40000153    1.60000002  272.          266.3999939    14.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00718351  0.009802   -0.05875562  0.00024763]]
stdevs:
[[ 0.09796528  0.08367283  0.34354374  0.27695226]]
rpn: max max_overlap 0.620079289106
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 171.1711731    82.88288116  328.8288269   298.19821167    8.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00639809  0.01199927 -0.07025351 -0.00437395]]
stdevs:
[[ 0.09832753  0.08355073  0.34566164  0.27451659]]
rpn: max max_overlap 0.511976237327
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 189.18919373   87.38739014  264.86486816  172.07206726   15.        ]
 [ 149.54954529  108.10810852  304.5045166   208.10810852   14.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00594701  0.01173121 -0.06971373 -0.000761  ]]
stdevs:
[[ 0.09746033  0.08410687  0.34207621  0.27234255]]
rpn: max max_overlap 0.750925161213
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 401.0)
scale: 0.802139043808
height, width: (19, 26)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  53.74331665   61.76470566  164.43850708  230.21389771   12.        ]
 [ 143.58288574   92.24598694  247.86096191  210.96257019   12.        ]]
total_anchors 4446
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00560407  0.01167848 -0.0654662  -0.00197729]]
stdevs:
[[ 0.09723255  0.0832294   0.34020554  0.26889455]]
rpn: max max_overlap 0.769258518716
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (11, 5)
rpn: gt_boxes [[  97.59999847  102.40000153  371.20001221  299.20001221   11.        ]
 [ 182.3999939    44.79999924  250.3999939   132.80000305   15.        ]
 [ 136.           67.19999695  176.          120.80000305   15.        ]
 [  69.59999847   56.          112.80000305   88.80000305   15.        ]
 [   8.           98.40000153  151.19999695  299.20001221   15.        ]
 [  36.79999924   72.80000305  179.19999695  228.           15.        ]
 [   3.20000005   56.79999924   40.          144.80000305   15.        ]
 [ 200.           51.20000076  284.79998779  149.6000061    15.        ]
 [ 267.20001221   64.          336.79998779  172.80000305   15.        ]
 [ 275.20001221   84.80000305  399.20001221  269.6000061    15.        ]
 [ 222.3999939   180.          345.6000061   299.20001221   15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00749766  0.00971679 -0.07052568 -0.00326762]]
stdevs:
[[ 0.09530659  0.0889665   0.32822452  0.28125031]]
rpn: max max_overlap 0.819747899301
rpn: num_positive 38
rpn: num_negative 218
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 375.0)
scale: 0.75
height, width: (19, 24)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 210.75  148.5   291.75  193.5     3.  ]
 [ 204.    129.75  235.5   185.25    3.  ]]
total_anchors 4104
inds_inside 1339
anchors.shape (1339, 4)
means:
[[ 0.00827505  0.00971058 -0.07239844 -0.00664611]]
stdevs:
[[ 0.09510515  0.08894342  0.32702312  0.28221618]]
rpn: max max_overlap 0.771910841809
rpn: num_positive 3
rpn: num_negative 253
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   0.           56.          101.59999847  155.19999695   20.        ]
 [ 127.19999695   24.79999924  392.79998779  299.20001221   15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00799823  0.00998126 -0.07024888 -0.00607478]]
stdevs:
[[ 0.09501083  0.08810961  0.3219724   0.27821566]]
rpn: max max_overlap 0.892597865754
rpn: num_positive 10
rpn: num_negative 246
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 164.          159.19999695  325.6000061   294.3999939    12.        ]
 [  28.79999924  171.19999695  176.          263.20001221   12.        ]
 [  44.           45.59999847  352.          298.3999939    15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00693705  0.01040035 -0.06108418 -0.00562492]]
stdevs:
[[ 0.09456199  0.08657105  0.31928635  0.27211143]]
rpn: max max_overlap 0.802706834393
rpn: num_positive 13
rpn: num_negative 243
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  27.20000076   98.40000153  120.80000305  144.80000305   14.        ]
 [ 262.3999939   112.          370.3999939   167.19999695   14.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00716116  0.01076522 -0.06000809 -0.00509836]]
stdevs:
[[ 0.09428843  0.08640971  0.31853081  0.27135905]]
rpn: max max_overlap 0.869929038801
rpn: num_positive 2
rpn: num_negative 254
rpn: num_positive avg 9
rpn: num_negative avg 247

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  48.            8.          331.20001221  297.6000061    14.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[  6.59483230e-03   1.05443580e-02  -5.36193981e-02   2.27531569e-05]]
stdevs:
[[ 0.09350195  0.08531712  0.31389774  0.2672549 ]]
rpn: max max_overlap 0.793524279266
rpn: num_positive 12
rpn: num_negative 244
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 343.0)
scale: 0.686498880386
height, width: (19, 22)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 153.77574158  164.75971985  335.01144409  275.97253418    2.        ]
 [   2.05949664  172.99771118  166.81922913  276.6590271     2.        ]]
total_anchors 3762
inds_inside 1174
anchors.shape (1174, 4)
means:
[[ 0.00649871  0.00973927 -0.0532208   0.00228538]]
stdevs:
[[ 0.09311946  0.08541126  0.31144103  0.26573782]]
rpn: max max_overlap 0.829542543376
rpn: num_positive 5
rpn: num_negative 251
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   8.80000019    0.80000001  399.20001221  299.20001221   19.        ]
 [ 249.6000061   105.59999847  364.          299.20001221   15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.0060074   0.00903549 -0.04638222  0.0216319 ]]
stdevs:
[[ 0.09130939  0.09018139  0.30568457  0.27416244]]
rpn: max max_overlap 0.68967850001
rpn: num_positive 16
rpn: num_negative 240
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  66.40000153   58.40000153  226.3999939   141.6000061     4.        ]
 [ 186.3999939    79.19999695  225.6000061   111.19999695    3.        ]]
total_anchors 4275
inds_inside 1484
anchors.shape (1484, 4)
means:
[[ 0.00571493  0.00860133 -0.0518847   0.01257952]]
stdI0211 13:04:41.923935 15724 solver.cpp:229] Iteration 40, loss = 0.183058
I0211 13:04:41.923990 15724 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.172969 (* 1 = 0.172969 loss)
I0211 13:04:41.924000 15724 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100892 (* 1 = 0.0100892 loss)
I0211 13:04:41.924008 15724 sgd_solver.cpp:106] Iteration 40, lr = 0.001
evs:
[[ 0.0916329   0.09048669  0.30639145  0.28186814]]
rpn: max max_overlap 0.767447922022
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[   0.           93.59999847  164.          397.6000061     6.        ]
 [ 217.6000061   231.19999695  241.6000061   300.           15.        ]
 [ 255.19999695  229.6000061   286.3999939   321.6000061    15.        ]]
total_anchors 4275
inds_inside 1484
anchors.shape (1484, 4)
means:
[[ 0.00582483  0.00907667 -0.05423492  0.01106953]]
stdevs:
[[ 0.09158788  0.09034183  0.30617215  0.28077839]]
rpn: max max_overlap 0.781015521093
rpn: num_positive 4
rpn: num_negative 252
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (450.0, 300.0)
scale: 0.900900900364
height, width: (29, 19)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  25.22522545  214.41441345  206.30630493  449.54956055   13.        ]
 [  90.99098969  280.18017578  299.09909058  449.54956055   15.        ]
 [  84.68468475   72.97297668  241.44143677  370.27026367   15.        ]]
total_anchors 4959
inds_inside 1742
anchors.shape (1742, 4)
means:
[[ 0.00572325  0.00861309 -0.05315562  0.01866628]]
stdevs:
[[ 0.09012178  0.09300858  0.30008659  0.28720628]]
rpn: max max_overlap 0.759594704445
rpn: num_positive 19
rpn: num_negative 237
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (397.0, 300.0)
scale: 0.79365080595
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 103.17460632   69.84127045  264.28570557  224.60317993   12.        ]]
total_anchors 4275
inds_inside 1458
anchors.shape (1458, 4)
means:
[[ 0.00565873  0.00894953 -0.04839824  0.02158626]]
stdevs:
[[ 0.09033219  0.09262494  0.29987638  0.28573002]]
rpn: max max_overlap 0.648852416013
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  84.80000305  107.19999695  281.6000061   291.20001221   13.        ]
 [ 150.3999939    44.          208.          201.6000061    15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00580003  0.00993396 -0.05203331  0.01809817]]
stdevs:
[[ 0.09012735  0.09257427  0.29957791  0.28563011]]
rpn: max max_overlap 0.600074982288
rpn: num_positive 5
rpn: num_negative 251
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  12.           63.20000076   89.59999847  188.80000305   16.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00532382  0.0101511  -0.05252338  0.01530286]]
stdevs:
[[ 0.08991983  0.09247268  0.29842038  0.28616792]]
rpn: max max_overlap 0.64248192063
rpn: num_positive 3
rpn: num_negative 253
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (6, 5)
rpn: gt_boxes [[ 301.80178833   43.24324417  375.6756897    75.67567444    1.        ]
 [ 177.47747803   54.05405426  249.54954529   93.69369507    1.        ]
 [ 188.2882843   120.72071838  261.26126099  152.2522583     1.        ]
 [  41.44144058   73.87387085  118.01802063  107.20720673    1.        ]
 [  72.07207489  125.22522736  143.24324036  156.75675964    1.        ]
 [  77.47747803  197.29730225  146.84684753  227.92793274    1.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00477491  0.01051655 -0.05577623  0.00850689]]
stdevs:
[[ 0.08960026  0.09205281  0.29664336  0.28803748]]
rpn: max max_overlap 0.672468878302
rpn: num_positive 7
rpn: num_negative 249
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 444.0)
scale: 0.887573957443
height, width: (19, 28)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   6.21301794    4.43787003  436.68640137  294.67456055    1.        ]]
total_anchors 4788
inds_inside 1712
anchors.shape (1712, 4)
means:
[[ 0.00416028  0.01176563 -0.04300994  0.03275248]]
stdevs:
[[ 0.08773618  0.09590766  0.29214758  0.2955411 ]]
rpn: max max_overlap 0.562275227315
rpn: num_positive 24
rpn: num_negative 232
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[ 229.6000061   134.3999939   398.3999939   299.20001221    9.        ]
 [ 208.           36.          264.79998779   92.80000305   16.        ]
 [ 127.19999695  120.80000305  188.          177.6000061    20.        ]
 [  67.19999695  116.          128.          202.3999939    20.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00442214  0.01124334 -0.03606602  0.03832184]]
stdevs:
[[ 0.08811658  0.09619733  0.29144139  0.29351801]]
rpn: max max_overlap 0.748697910674
rpn: num_positive 13
rpn: num_negative 243
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 241.61676025  149.10179138  273.9520874   167.96406555    7.        ]
 [ 181.43711853  150.89820862  212.87425232  167.96406555    7.        ]
 [ 141.01795959  157.18562317  193.11376953  188.62275696    7.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00257084  0.01067978 -0.04155254  0.00546543]]
stdevs:
[[ 0.08820464  0.0995129   0.29025595  0.36323824]]
rpn: max max_overlap 0.420477512341
rpn: num_positive 10
rpn: num_negative 246
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 353.0)
scale: 0.705882370472
height, width: (19, 23)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 117.1764679   120.          197.64706421  212.47058105   15.        ]
 [  39.52941132   21.88235283  242.8235321   107.29412079    7.        ]]
total_anchors 3933
inds_inside 1216
anchors.shape (1216, 4)
means:
[[ 0.00284522  0.0111226  -0.03889542  0.00616713]]
stdevs:
[[ 0.08791485  0.09971473  0.28920776  0.36175969]]
rpn: max max_overlap 0.78305068542
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[  31.20000076   17.60000038  362.3999939   285.6000061     7.        ]
 [   0.           59.20000076   39.20000076  100.80000305    7.        ]
 [  47.20000076   30.39999962   69.59999847  104.           15.        ]
 [  32.           40.79999924   82.40000153   84.            7.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[  2.46892663e-03   1.05905594e-02  -3.61232574e-02   8.61019367e-05]]
stdevs:
[[ 0.08768024  0.09930162  0.28809988  0.36338124]]
rpn: max max_overlap 0.733378243954
rpn: num_positive 10
rpn: num_negative 246
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 304.5045166    21.62162209  360.36035156   66.66666412    1.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.00263695  0.01073014 -0.03630848 -0.00065568]]
stdevs:
[[ 0.08765281  0.09923323  0.28780167  0.36330742]]
rpn: max max_overlap 0.614454109036
rpn: num_positive 1
rpn: num_negative 255
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[ 176.           46.40000153  396.79998779  296.79998779   15.        ]
 [ 116.           84.80000305  208.          296.           15.        ]
 [  53.59999847   62.40000153  190.3999939   299.20001221   15.        ]
 [   1.60000002   58.40000153  138.3999939   296.79998779   15.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00262829  0.00973222 -0.03284916  0.0193354 ]]
stdevs:
[[ 0.08622766  0.10699091  0.28264653  0.37169547]]
rpn: max max_overlap 0.800107865602
rpn: num_positive 19
rpn: num_negative 237
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 330.3999939   175.19999695  382.3999939   211.19999695    3.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00240865  0.0098046  -0.03351902  0.01689539]]
stdevs:
[[ 0.0861069   0.10708052  0.28222213  0.37274865]]
rpn: max max_overlap 0.478759765625
rpn: num_positive 2
rpn: num_negative 254
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 452.0)
scale: 0.903614461422
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 190.66264343   83.13253021  305.42169189  248.49397278   19.        ]]
total_anchors 4959
inds_inside 1753
anchors.shape (1753, 4)
means:
[[ 0.00204943  0.00957541 -0.03263735  0.01814918]]
stdevs:
[[ 0.08604014  0.10684411  0.28148312  0.37130657]]
rpn: max max_overlap 0.728129971603
rpn: num_positive 5
rpn: num_negative 251
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 153.6000061    4.         274.3999939  116.          20.       ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.0020202   0.00936432 -0.03267347  0.01784601]]
stdevs:
[[ 0.08595109  0.10682869  0.28118523  0.37097042]]
rpn: max max_overlap 0.786914902784
rpn: num_positive 1
rpn: num_negative 255
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 446.0)
scale: 0.892857134342
height, width: (19, 28)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[ 389.28570557   94.64286041  410.71429443  122.32142639   15.        ]
 [ 230.35714722  135.71427917  268.75        216.07142639   15.        ]
 [ 315.17855835   70.5357132   444.64285278  299.10714722   15.        ]
 [ 105.35713959   48.2142868   165.17857361  152.67857361   15.        ]
 [   0.           50.          185.71427917  298.21429443   15.        ]]
total_anchors 4788
inds_inside 1725
anchors.shape (1725, 4)
means:
[[ 0.00245944  0.00858888 -0.04257061  0.00718869]]
stdevs:
[[ 0.08788272  0.11227747  0.2882691   0.39752923]]
rpn: max max_overlap 0.751347069992
rpn: num_positive 19
rpn: num_negative 237
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 452.0)
scale: 0.903614461422
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 236.74699402   36.14457703  298.19277954  206.02409363   15.        ]
 [ 190.66264343   74.09638214  351.50601196  236.74699402   13.        ]]
total_anchors 4959
inds_inside 1753
anchors.shape (1753, 4)
means:
[[ 0.00221884  0.00826433 -0.04154694  0.00896416]]
stdevs:
[[ 0.08777451  0.11179593  0.28822972  0.39570053]]
rpn: max max_overlap 0.68896601523
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 9
rpn: num_negative avg 246

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 156.          195.19999695  275.20001221  282.3999939     2.        ]
 [ 136.          189.6000061   240.80000305  279.20001221    2.        ]
 [ 259.20001221  191.19999695  399.20001221  299.20001221    7.        ]]
total_anchors 4275
inds_inside 1474
anchors.shape (1474, 4)
means:
[[ 0.00213234  0.00861697 -0.04190877  0.00644393]]
stdevs:
[[ 0.08752938  0.11145992  0.28729689  0.39518501]]
rpn: max max_overlap 0.731484360997
rpn: num_positive 4
rpn: num_negative 252
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  25.22522545   21.62162209  411.71170044  296.39639282   12.        ]]
total_anchors 4959
inds_inside 1732
anchors.shape (1732, 4)
means:
[[ 0.0022027   0.00848447 -0.04079999  0.01066107]]
stdevs:
[[ 0.08701125  0.11188225  0.28576778  0.39471684]]
rpn: max max_overlap 0.661207798096
rpn: num_positive 6
rpn: num_negative 250
rpn: num_positive avg 8
rpn: num_negative avg 247

im_size: (300.0, 410.0)
scale: 0.819672107697
height, width: (19, 26)
rpn: gt_boxes.shape (2, 5)
rpn: gt_bo