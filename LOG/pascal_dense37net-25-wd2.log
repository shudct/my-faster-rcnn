Called with args:
Namespace(cfg_file=None, gpu_id=1, imdb_name='voc_2007_trainval', net_name='VGG16', pretrained_model=None, set_cfgs=None)
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0215 10:12:15.921814 12693 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/pascal_voc/VGG16/dense37net/train_densenet.prototxt"
base_lr: 0.001
display: 20
max_iter: 240000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0002
snapshot: 50000
snapshot_prefix: "pascal_densenet-37L-lr1e-3-wd2"
solver_mode: GPU
random_seed: 831486
stepvalue: 100000
stepvalue: 150000
type: "Nesterov"
I0215 10:12:15.922008 12693 solver.cpp:81] Creating training net from train_net file: models/pascal_voc/VGG16/dense37net/train_densenet.prototxt
I0215 10:12:15.926537 12693 net.cpp:49] Initializing net from parameters: 
name: "Densenet_37_layers"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 21"
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "BatchNorm1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Convolution1"
  bottom: "Dropout1"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Concat1"
  top: "BatchNorm2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Concat1"
  bottom: "Dropout2"
  top: "Concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Concat2"
  top: "BatchNorm3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "BatchNorm3"
  top: "Convolution4"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution4"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Concat2"
  bottom: "Dropout3"
  top: "Concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Concat3"
  top: "BatchNorm4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "BatchNorm4"
  top: "Convolution5"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution5"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat4"
  type: "Concat"
  bottom: "Concat3"
  bottom: "Dropout4"
  top: "Concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Concat4"
  top: "BatchNorm5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "BatchNorm5"
  top: "Convolution6"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution6"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat5"
  type: "Concat"
  bottom: "Concat4"
  bottom: "Dropout5"
  top: "Concat5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Concat5"
  top: "BatchNorm6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "BatchNorm6"
  top: "Convolution7"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution7"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat6"
  type: "Concat"
  bottom: "Concat5"
  bottom: "Dropout6"
  top: "Concat6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Concat6"
  top: "BatchNorm7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "BatchNorm7"
  top: "Convolution8"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution8"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat7"
  type: "Concat"
  bottom: "Concat6"
  bottom: "Dropout7"
  top: "Concat7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Concat7"
  top: "BatchNorm8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "BatchNorm8"
  top: "Convolution9"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution9"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat8"
  type: "Concat"
  bottom: "Concat7"
  bottom: "Dropout8"
  top: "Concat8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Concat8"
  top: "BatchNorm9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "BatchNorm9"
  top: "Convolution10"
  convolution_param {
    num_output: 112
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution10"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Dropout9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Pooling1"
  top: "BatchNorm10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "BatchNorm10"
  top: "Convolution11"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution11"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat9"
  type: "Concat"
  bottom: "Pooling1"
  bottom: "Dropout10"
  top: "Concat9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Concat9"
  top: "BatchNorm11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "BatchNorm11"
  top: "Convolution12"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution12"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat10"
  type: "Concat"
  bottom: "Concat9"
  bottom: "Dropout11"
  top: "Concat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Concat10"
  top: "BatchNorm12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "BatchNorm12"
  top: "Convolution13"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout12"
  type: "Dropout"
  bottom: "Convolution13"
  top: "Dropout12"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat11"
  type: "Concat"
  bottom: "Concat10"
  bottom: "Dropout12"
  top: "Concat11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Concat11"
  top: "BatchNorm13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "BatchNorm13"
  top: "Convolution14"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout13"
  type: "Dropout"
  bottom: "Convolution14"
  top: "Dropout13"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat12"
  type: "Concat"
  bottom: "Concat11"
  bottom: "Dropout13"
  top: "Concat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Concat12"
  top: "BatchNorm14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "BatchNorm14"
  top: "BatchNorm14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "BatchNorm14"
  top: "Convolution15"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout14"
  type: "Dropout"
  bottom: "Convolution15"
  top: "Dropout14"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat13"
  type: "Concat"
  bottom: "Concat12"
  bottom: "Dropout14"
  top: "Concat13"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Concat13"
  top: "BatchNorm15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "BatchNorm15"
  top: "BatchNorm15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "BatchNorm15"
  top: "Convolution16"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout15"
  type: "Dropout"
  bottom: "Convolution16"
  top: "Dropout15"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat14"
  type: "Concat"
  bottom: "Concat13"
  bottom: "Dropout15"
  top: "Concat14"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Concat14"
  top: "BatchNorm16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "BatchNorm16"
  top: "BatchNorm16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "BatchNorm16"
  top: "Convolution17"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout16"
  type: "Dropout"
  bottom: "Convolution17"
  top: "Dropout16"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat15"
  type: "Concat"
  bottom: "Concat14"
  bottom: "Dropout16"
  top: "Concat15"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Concat15"
  top: "BatchNorm17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "BatchNorm17"
  top: "BatchNorm17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "BatchNorm17"
  top: "Convolution18"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout17"
  type: "Dropout"
  bottom: "Convolution18"
  top: "Dropout17"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat16"
  type: "Concat"
  bottom: "Concat15"
  bottom: "Dropout17"
  top: "Concat16"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Concat16"
  top: "BatchNorm18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "BatchNorm18"
  top: "BatchNorm18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "BatchNorm18"
  top: "Convolution19"
  convolution_param {
    num_output: 208
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout18"
  type: "Dropout"
  bottom: "Convolution19"
  top: "Dropout18"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Dropout18"
  top: "Pooling2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Pooling2"
  top: "BatchNorm19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "BatchNorm19"
  top: "BatchNorm19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "BatchNorm19"
  top: "Convolution20"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout19"
  type: "Dropout"
  bottom: "Convolution20"
  top: "Dropout19"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat17"
  type: "Concat"
  bottom: "Pooling2"
  bottom: "Dropout19"
  top: "Concat17"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Concat17"
  top: "BatchNorm20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "BatchNorm20"
  top: "BatchNorm20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "BatchNorm20"
  top: "Convolution21"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout20"
  type: "Dropout"
  bottom: "Convolution21"
  top: "Dropout20"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat18"
  type: "Concat"
  bottom: "Concat17"
  bottom: "Dropout20"
  top: "Concat18"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Concat18"
  top: "BatchNorm21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "BatchNorm21"
  top: "BatchNorm21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "BatchNorm21"
  top: "Convolution22"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout21"
  type: "Dropout"
  bottom: "Convolution22"
  top: "Dropout21"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat19"
  type: "Concat"
  bottom: "Concat18"
  bottom: "Dropout21"
  top: "Concat19"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Concat19"
  top: "BatchNorm22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "BatchNorm22"
  top: "BatchNorm22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "BatchNorm22"
  top: "Convolution23"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout22"
  type: "Dropout"
  bottom: "Convolution23"
  top: "Dropout22"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat20"
  type: "Concat"
  bottom: "Concat19"
  bottom: "Dropout22"
  top: "Concat20"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Concat20"
  top: "BatchNorm23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "BatchNorm23"
  top: "BatchNorm23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "BatchNorm23"
  top: "Convolution24"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout23"
  type: "Dropout"
  bottom: "Convolution24"
  top: "Dropout23"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat21"
  type: "Concat"
  bottom: "Concat20"
  bottom: "Dropout23"
  top: "Concat21"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Concat21"
  top: "BatchNorm24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "BatchNorm24"
  top: "BatchNorm24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "BatchNorm24"
  top: "Convolution25"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout24"
  type: "Dropout"
  bottom: "Convolution25"
  top: "Dropout24"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat22"
  type: "Concat"
  bottom: "Concat21"
  bottom: "Dropout24"
  top: "Concat22"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Concat22"
  top: "BatchNorm25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "BatchNorm25"
  top: "BatchNorm25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "BatchNorm25"
  top: "Convolution26"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout25"
  type: "Dropout"
  bottom: "Convolution26"
  top: "Dropout25"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat23"
  type: "Concat"
  bottom: "Concat22"
  bottom: "Dropout25"
  top: "Concat23"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Concat23"
  top: "BatchNorm26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "BatchNorm26"
  top: "BatchNorm26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "BatchNorm26"
  top: "Convolution27"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout26"
  type: "Dropout"
  bottom: "Convolution27"
  top: "Dropout26"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat24"
  type: "Concat"
  bottom: "Concat23"
  bottom: "Dropout26"
  top: "Concat24"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Concat24"
  top: "BatchNorm27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "BatchNorm27"
  top: "BatchNorm27"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "BatchNorm27"
  top: "BatchNorm27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "BatchNorm27"
  top: "Convolution28"
  convolution_param {
 
I0215 10:12:15.927799 12693 layer_factory.hpp:77] Creating layer input-data
I0215 10:12:15.946758 12693 net.cpp:106] Creating Layer input-data
I0215 10:12:15.946776 12693 net.cpp:411] input-data -> data
I0215 10:12:15.946794 12693 net.cpp:411] input-data -> im_info
I0215 10:12:15.946805 12693 net.cpp:411] input-data -> gt_boxes
I0215 10:12:15.960115 12693 net.cpp:150] Setting up input-data
I0215 10:12:15.960155 12693 net.cpp:157] Top shape: 1 3 300 500 (450000)
I0215 10:12:15.960162 12693 net.cpp:157] Top shape: 1 3 (3)
I0215 10:12:15.960168 12693 net.cpp:157] Top shape: 1 4 (4)
I0215 10:12:15.960175 12693 net.cpp:165] Memory required for data: 1800028
I0215 10:12:15.960185 12693 layer_factory.hpp:77] Creating layer data_input-data_0_split
I0215 10:12:15.960206 12693 net.cpp:106] Creating Layer data_input-data_0_split
I0215 10:12:15.960213 12693 net.cpp:454] data_input-data_0_split <- data
I0215 10:12:15.960227 12693 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_0
I0215 10:12:15.960242 12693 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_1
I0215 10:12:15.960302 12693 net.cpp:150] Setting up data_input-data_0_split
I0215 10:12:15.960319 12693 net.cpp:157] Top shape: 1 3 300 500 (450000)
I0215 10:12:15.960331 12693 net.cpp:157] Top shape: 1 3 300 500 (450000)
I0215 10:12:15.960340 12693 net.cpp:165] Memory required for data: 5400028
I0215 10:12:15.960350 12693 layer_factory.hpp:77] Creating layer Convolution1
I0215 10:12:15.960371 12693 net.cpp:106] Creating Layer Convolution1
I0215 10:12:15.960379 12693 net.cpp:454] Convolution1 <- data_input-data_0_split_0
I0215 10:12:15.960397 12693 net.cpp:411] Convolution1 -> Convolution1
I0215 10:12:15.961359 12693 net.cpp:150] Setting up Convolution1
I0215 10:12:15.961376 12693 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0215 10:12:15.961386 12693 net.cpp:165] Memory required for data: 15000028
I0215 10:12:15.961405 12693 layer_factory.hpp:77] Creating layer Convolution1_Convolution1_0_split
I0215 10:12:15.961417 12693 net.cpp:106] Creating Layer Convolution1_Convolution1_0_split
I0215 10:12:15.961426 12693 net.cpp:454] Convolution1_Convolution1_0_split <- Convolution1
I0215 10:12:15.961442 12693 net.cpp:411] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_0
I0215 10:12:15.961455 12693 net.cpp:411] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_1
I0215 10:12:15.961514 12693 net.cpp:150] Setting up Convolution1_Convolution1_0_split
I0215 10:12:15.961527 12693 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0215 10:12:15.961539 12693 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0215 10:12:15.961547 12693 net.cpp:165] Memory required for data: 34200028
I0215 10:12:15.961556 12693 layer_factory.hpp:77] Creating layer BatchNorm1
I0215 10:12:15.961575 12693 net.cpp:106] Creating Layer BatchNorm1
I0215 10:12:15.961583 12693 net.cpp:454] BatchNorm1 <- Convolution1_Convolution1_0_split_0
I0215 10:12:15.961596 12693 net.cpp:411] BatchNorm1 -> BatchNorm1
I0215 10:12:15.962638 12693 net.cpp:150] Setting up BatchNorm1
I0215 10:12:15.962658 12693 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0215 10:12:15.962668 12693 net.cpp:165] Memory required for data: 43800028
I0215 10:12:15.962689 12693 layer_factory.hpp:77] Creating layer Scale1
I0215 10:12:15.962704 12693 net.cpp:106] Creating Layer Scale1
I0215 10:12:15.962713 12693 net.cpp:454] Scale1 <- BatchNorm1
I0215 10:12:15.962729 12693 net.cpp:397] Scale1 -> BatchNorm1 (in-place)
I0215 10:12:15.962800 12693 layer_factory.hpp:77] Creating layer Scale1
I0215 10:12:15.964432 12693 net.cpp:150] Setting up Scale1
I0215 10:12:15.964449 12693 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0215 10:12:15.964459 12693 net.cpp:165] Memory required for data: 53400028
I0215 10:12:15.964478 12693 layer_factory.hpp:77] Creating layer ReLU1
I0215 10:12:15.964493 12693 net.cpp:106] Creating Layer ReLU1
I0215 10:12:15.964501 12693 net.cpp:454] ReLU1 <- BatchNorm1
I0215 10:12:15.964514 12693 net.cpp:397] ReLU1 -> BatchNorm1 (in-place)
I0215 10:12:15.964534 12693 net.cpp:150] Setting up ReLU1
I0215 10:12:15.964543 12693 net.cpp:157] Top shape: 1 16 300 500 (2400000)
I0215 10:12:15.964552 12693 net.cpp:165] Memory required for data: 63000028
I0215 10:12:15.964560 12693 layer_factory.hpp:77] Creating layer Convolution2
I0215 10:12:15.964579 12693 net.cpp:106] Creating Layer Convolution2
I0215 10:12:15.964586 12693 net.cpp:454] Convolution2 <- BatchNorm1
I0215 10:12:15.964601 12693 net.cpp:411] Convolution2 -> Convolution2
I0215 10:12:15.965533 12693 net.cpp:150] Setting up Convolution2
I0215 10:12:15.965549 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.965559 12693 net.cpp:165] Memory required for data: 70200028
I0215 10:12:15.965572 12693 layer_factory.hpp:77] Creating layer Dropout1
I0215 10:12:15.965593 12693 net.cpp:106] Creating Layer Dropout1
I0215 10:12:15.965601 12693 net.cpp:454] Dropout1 <- Convolution2
I0215 10:12:15.965615 12693 net.cpp:411] Dropout1 -> Dropout1
I0215 10:12:15.965672 12693 net.cpp:150] Setting up Dropout1
I0215 10:12:15.965685 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.965694 12693 net.cpp:165] Memory required for data: 77400028
I0215 10:12:15.965703 12693 layer_factory.hpp:77] Creating layer Concat1
I0215 10:12:15.965716 12693 net.cpp:106] Creating Layer Concat1
I0215 10:12:15.965724 12693 net.cpp:454] Concat1 <- Convolution1_Convolution1_0_split_1
I0215 10:12:15.965734 12693 net.cpp:454] Concat1 <- Dropout1
I0215 10:12:15.965749 12693 net.cpp:411] Concat1 -> Concat1
I0215 10:12:15.965790 12693 net.cpp:150] Setting up Concat1
I0215 10:12:15.965802 12693 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0215 10:12:15.965811 12693 net.cpp:165] Memory required for data: 94200028
I0215 10:12:15.965821 12693 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I0215 10:12:15.965837 12693 net.cpp:106] Creating Layer Concat1_Concat1_0_split
I0215 10:12:15.965843 12693 net.cpp:454] Concat1_Concat1_0_split <- Concat1
I0215 10:12:15.965855 12693 net.cpp:411] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I0215 10:12:15.965867 12693 net.cpp:411] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I0215 10:12:15.965922 12693 net.cpp:150] Setting up Concat1_Concat1_0_split
I0215 10:12:15.965934 12693 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0215 10:12:15.965945 12693 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0215 10:12:15.965953 12693 net.cpp:165] Memory required for data: 127800028
I0215 10:12:15.965962 12693 layer_factory.hpp:77] Creating layer BatchNorm2
I0215 10:12:15.965978 12693 net.cpp:106] Creating Layer BatchNorm2
I0215 10:12:15.965986 12693 net.cpp:454] BatchNorm2 <- Concat1_Concat1_0_split_0
I0215 10:12:15.965997 12693 net.cpp:411] BatchNorm2 -> BatchNorm2
I0215 10:12:15.967025 12693 net.cpp:150] Setting up BatchNorm2
I0215 10:12:15.967041 12693 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0215 10:12:15.967051 12693 net.cpp:165] Memory required for data: 144600028
I0215 10:12:15.967070 12693 layer_factory.hpp:77] Creating layer Scale2
I0215 10:12:15.967083 12693 net.cpp:106] Creating Layer Scale2
I0215 10:12:15.967092 12693 net.cpp:454] Scale2 <- BatchNorm2
I0215 10:12:15.967108 12693 net.cpp:397] Scale2 -> BatchNorm2 (in-place)
I0215 10:12:15.967171 12693 layer_factory.hpp:77] Creating layer Scale2
I0215 10:12:15.968770 12693 net.cpp:150] Setting up Scale2
I0215 10:12:15.968789 12693 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0215 10:12:15.968798 12693 net.cpp:165] Memory required for data: 161400028
I0215 10:12:15.968814 12693 layer_factory.hpp:77] Creating layer ReLU2
I0215 10:12:15.968827 12693 net.cpp:106] Creating Layer ReLU2
I0215 10:12:15.968834 12693 net.cpp:454] ReLU2 <- BatchNorm2
I0215 10:12:15.968847 12693 net.cpp:397] ReLU2 -> BatchNorm2 (in-place)
I0215 10:12:15.968858 12693 net.cpp:150] Setting up ReLU2
I0215 10:12:15.968868 12693 net.cpp:157] Top shape: 1 28 300 500 (4200000)
I0215 10:12:15.968875 12693 net.cpp:165] Memory required for data: 178200028
I0215 10:12:15.968884 12693 layer_factory.hpp:77] Creating layer Convolution3
I0215 10:12:15.968904 12693 net.cpp:106] Creating Layer Convolution3
I0215 10:12:15.968910 12693 net.cpp:454] Convolution3 <- BatchNorm2
I0215 10:12:15.968924 12693 net.cpp:411] Convolution3 -> Convolution3
I0215 10:12:15.969208 12693 net.cpp:150] Setting up Convolution3
I0215 10:12:15.969223 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.969230 12693 net.cpp:165] Memory required for data: 185400028
I0215 10:12:15.969244 12693 layer_factory.hpp:77] Creating layer Dropout2
I0215 10:12:15.969254 12693 net.cpp:106] Creating Layer Dropout2
I0215 10:12:15.969262 12693 net.cpp:454] Dropout2 <- Convolution3
I0215 10:12:15.969277 12693 net.cpp:411] Dropout2 -> Dropout2
I0215 10:12:15.969337 12693 net.cpp:150] Setting up Dropout2
I0215 10:12:15.969350 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.969358 12693 net.cpp:165] Memory required for data: 192600028
I0215 10:12:15.969367 12693 layer_factory.hpp:77] Creating layer Concat2
I0215 10:12:15.969383 12693 net.cpp:106] Creating Layer Concat2
I0215 10:12:15.969389 12693 net.cpp:454] Concat2 <- Concat1_Concat1_0_split_1
I0215 10:12:15.969400 12693 net.cpp:454] Concat2 <- Dropout2
I0215 10:12:15.969414 12693 net.cpp:411] Concat2 -> Concat2
I0215 10:12:15.969452 12693 net.cpp:150] Setting up Concat2
I0215 10:12:15.969465 12693 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0215 10:12:15.969473 12693 net.cpp:165] Memory required for data: 216600028
I0215 10:12:15.969482 12693 layer_factory.hpp:77] Creating layer Concat2_Concat2_0_split
I0215 10:12:15.969494 12693 net.cpp:106] Creating Layer Concat2_Concat2_0_split
I0215 10:12:15.969502 12693 net.cpp:454] Concat2_Concat2_0_split <- Concat2
I0215 10:12:15.969517 12693 net.cpp:411] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_0
I0215 10:12:15.969529 12693 net.cpp:411] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_1
I0215 10:12:15.969588 12693 net.cpp:150] Setting up Concat2_Concat2_0_split
I0215 10:12:15.969600 12693 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0215 10:12:15.969610 12693 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0215 10:12:15.969619 12693 net.cpp:165] Memory required for data: 264600028
I0215 10:12:15.969626 12693 layer_factory.hpp:77] Creating layer BatchNorm3
I0215 10:12:15.969641 12693 net.cpp:106] Creating Layer BatchNorm3
I0215 10:12:15.969650 12693 net.cpp:454] BatchNorm3 <- Concat2_Concat2_0_split_0
I0215 10:12:15.969661 12693 net.cpp:411] BatchNorm3 -> BatchNorm3
I0215 10:12:15.970681 12693 net.cpp:150] Setting up BatchNorm3
I0215 10:12:15.970696 12693 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0215 10:12:15.970710 12693 net.cpp:165] Memory required for data: 288600028
I0215 10:12:15.970727 12693 layer_factory.hpp:77] Creating layer Scale3
I0215 10:12:15.970742 12693 net.cpp:106] Creating Layer Scale3
I0215 10:12:15.970751 12693 net.cpp:454] Scale3 <- BatchNorm3
I0215 10:12:15.970765 12693 net.cpp:397] Scale3 -> BatchNorm3 (in-place)
I0215 10:12:15.970829 12693 layer_factory.hpp:77] Creating layer Scale3
I0215 10:12:15.972473 12693 net.cpp:150] Setting up Scale3
I0215 10:12:15.972491 12693 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0215 10:12:15.972501 12693 net.cpp:165] Memory required for data: 312600028
I0215 10:12:15.972522 12693 layer_factory.hpp:77] Creating layer ReLU3
I0215 10:12:15.972534 12693 net.cpp:106] Creating Layer ReLU3
I0215 10:12:15.972543 12693 net.cpp:454] ReLU3 <- BatchNorm3
I0215 10:12:15.972554 12693 net.cpp:397] ReLU3 -> BatchNorm3 (in-place)
I0215 10:12:15.972568 12693 net.cpp:150] Setting up ReLU3
I0215 10:12:15.972578 12693 net.cpp:157] Top shape: 1 40 300 500 (6000000)
I0215 10:12:15.972585 12693 net.cpp:165] Memory required for data: 336600028
I0215 10:12:15.972594 12693 layer_factory.hpp:77] Creating layer Convolution4
I0215 10:12:15.972612 12693 net.cpp:106] Creating Layer Convolution4
I0215 10:12:15.972620 12693 net.cpp:454] Convolution4 <- BatchNorm3
I0215 10:12:15.972635 12693 net.cpp:411] Convolution4 -> Convolution4
I0215 10:12:15.973577 12693 net.cpp:150] Setting up Convolution4
I0215 10:12:15.973592 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.973601 12693 net.cpp:165] Memory required for data: 343800028
I0215 10:12:15.973614 12693 layer_factory.hpp:77] Creating layer Dropout3
I0215 10:12:15.973629 12693 net.cpp:106] Creating Layer Dropout3
I0215 10:12:15.973637 12693 net.cpp:454] Dropout3 <- Convolution4
I0215 10:12:15.973649 12693 net.cpp:411] Dropout3 -> Dropout3
I0215 10:12:15.973711 12693 net.cpp:150] Setting up Dropout3
I0215 10:12:15.973722 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.973731 12693 net.cpp:165] Memory required for data: 351000028
I0215 10:12:15.973740 12693 layer_factory.hpp:77] Creating layer Concat3
I0215 10:12:15.973757 12693 net.cpp:106] Creating Layer Concat3
I0215 10:12:15.973764 12693 net.cpp:454] Concat3 <- Concat2_Concat2_0_split_1
I0215 10:12:15.973775 12693 net.cpp:454] Concat3 <- Dropout3
I0215 10:12:15.973786 12693 net.cpp:411] Concat3 -> Concat3
I0215 10:12:15.973825 12693 net.cpp:150] Setting up Concat3
I0215 10:12:15.973837 12693 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0215 10:12:15.973846 12693 net.cpp:165] Memory required for data: 382200028
I0215 10:12:15.973855 12693 layer_factory.hpp:77] Creating layer Concat3_Concat3_0_split
I0215 10:12:15.973871 12693 net.cpp:106] Creating Layer Concat3_Concat3_0_split
I0215 10:12:15.973879 12693 net.cpp:454] Concat3_Concat3_0_split <- Concat3
I0215 10:12:15.973891 12693 net.cpp:411] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_0
I0215 10:12:15.973903 12693 net.cpp:411] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_1
I0215 10:12:15.973963 12693 net.cpp:150] Setting up Concat3_Concat3_0_split
I0215 10:12:15.973975 12693 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0215 10:12:15.973985 12693 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0215 10:12:15.973994 12693 net.cpp:165] Memory required for data: 444600028
I0215 10:12:15.974002 12693 layer_factory.hpp:77] Creating layer BatchNorm4
I0215 10:12:15.974019 12693 net.cpp:106] Creating Layer BatchNorm4
I0215 10:12:15.974026 12693 net.cpp:454] BatchNorm4 <- Concat3_Concat3_0_split_0
I0215 10:12:15.974041 12693 net.cpp:411] BatchNorm4 -> BatchNorm4
I0215 10:12:15.975106 12693 net.cpp:150] Setting up BatchNorm4
I0215 10:12:15.975122 12693 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0215 10:12:15.975132 12693 net.cpp:165] Memory required for data: 475800028
I0215 10:12:15.975149 12693 layer_factory.hpp:77] Creating layer Scale4
I0215 10:12:15.975164 12693 net.cpp:106] Creating Layer Scale4
I0215 10:12:15.975172 12693 net.cpp:454] Scale4 <- BatchNorm4
I0215 10:12:15.975184 12693 net.cpp:397] Scale4 -> BatchNorm4 (in-place)
I0215 10:12:15.975250 12693 layer_factory.hpp:77] Creating layer Scale4
I0215 10:12:15.976891 12693 net.cpp:150] Setting up Scale4
I0215 10:12:15.976907 12693 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0215 10:12:15.976918 12693 net.cpp:165] Memory required for data: 507000028
I0215 10:12:15.976933 12693 layer_factory.hpp:77] Creating layer ReLU4
I0215 10:12:15.976948 12693 net.cpp:106] Creating Layer ReLU4
I0215 10:12:15.976956 12693 net.cpp:454] ReLU4 <- BatchNorm4
I0215 10:12:15.976969 12693 net.cpp:397] ReLU4 -> BatchNorm4 (in-place)
I0215 10:12:15.976981 12693 net.cpp:150] Setting up ReLU4
I0215 10:12:15.976991 12693 net.cpp:157] Top shape: 1 52 300 500 (7800000)
I0215 10:12:15.976999 12693 net.cpp:165] Memory required for data: 538200028
I0215 10:12:15.977008 12693 layer_factory.hpp:77] Creating layer Convolution5
I0215 10:12:15.977027 12693 net.cpp:106] Creating Layer Convolution5
I0215 10:12:15.977035 12693 net.cpp:454] Convolution5 <- BatchNorm4
I0215 10:12:15.977048 12693 net.cpp:411] Convolution5 -> Convolution5
I0215 10:12:15.977356 12693 net.cpp:150] Setting up Convolution5
I0215 10:12:15.977370 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.977378 12693 net.cpp:165] Memory required for data: 545400028
I0215 10:12:15.977391 12693 layer_factory.hpp:77] Creating layer Dropout4
I0215 10:12:15.977404 12693 net.cpp:106] Creating Layer Dropout4
I0215 10:12:15.977412 12693 net.cpp:454] Dropout4 <- Convolution5
I0215 10:12:15.977427 12693 net.cpp:411] Dropout4 -> Dropout4
I0215 10:12:15.977485 12693 net.cpp:150] Setting up Dropout4
I0215 10:12:15.977499 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.977506 12693 net.cpp:165] Memory required for data: 552600028
I0215 10:12:15.977516 12693 layer_factory.hpp:77] Creating layer Concat4
I0215 10:12:15.977531 12693 net.cpp:106] Creating Layer Concat4
I0215 10:12:15.977540 12693 net.cpp:454] Concat4 <- Concat3_Concat3_0_split_1
I0215 10:12:15.977550 12693 net.cpp:454] Concat4 <- Dropout4
I0215 10:12:15.977565 12693 net.cpp:411] Concat4 -> Concat4
I0215 10:12:15.977604 12693 net.cpp:150] Setting up Concat4
I0215 10:12:15.977617 12693 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0215 10:12:15.977625 12693 net.cpp:165] Memory required for data: 591000028
I0215 10:12:15.977634 12693 layer_factory.hpp:77] Creating layer Concat4_Concat4_0_split
I0215 10:12:15.977646 12693 net.cpp:106] Creating Layer Concat4_Concat4_0_split
I0215 10:12:15.977654 12693 net.cpp:454] Concat4_Concat4_0_split <- Concat4
I0215 10:12:15.977669 12693 net.cpp:411] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_0
I0215 10:12:15.977681 12693 net.cpp:411] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_1
I0215 10:12:15.977738 12693 net.cpp:150] Setting up Concat4_Concat4_0_split
I0215 10:12:15.977753 12693 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0215 10:12:15.977763 12693 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0215 10:12:15.977772 12693 net.cpp:165] Memory required for data: 667800028
I0215 10:12:15.977782 12693 layer_factory.hpp:77] Creating layer BatchNorm5
I0215 10:12:15.977799 12693 net.cpp:106] Creating Layer BatchNorm5
I0215 10:12:15.977807 12693 net.cpp:454] BatchNorm5 <- Concat4_Concat4_0_split_0
I0215 10:12:15.977818 12693 net.cpp:411] BatchNorm5 -> BatchNorm5
I0215 10:12:15.978827 12693 net.cpp:150] Setting up BatchNorm5
I0215 10:12:15.978844 12693 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0215 10:12:15.978853 12693 net.cpp:165] Memory required for data: 706200028
I0215 10:12:15.978870 12693 layer_factory.hpp:77] Creating layer Scale5
I0215 10:12:15.978885 12693 net.cpp:106] Creating Layer Scale5
I0215 10:12:15.978893 12693 net.cpp:454] Scale5 <- BatchNorm5
I0215 10:12:15.978906 12693 net.cpp:397] Scale5 -> BatchNorm5 (in-place)
I0215 10:12:15.978973 12693 layer_factory.hpp:77] Creating layer Scale5
I0215 10:12:15.980587 12693 net.cpp:150] Setting up Scale5
I0215 10:12:15.980604 12693 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0215 10:12:15.980613 12693 net.cpp:165] Memory required for data: 744600028
I0215 10:12:15.980629 12693 layer_factory.hpp:77] Creating layer ReLU5
I0215 10:12:15.980640 12693 net.cpp:106] Creating Layer ReLU5
I0215 10:12:15.980648 12693 net.cpp:454] ReLU5 <- BatchNorm5
I0215 10:12:15.980660 12693 net.cpp:397] ReLU5 -> BatchNorm5 (in-place)
I0215 10:12:15.980672 12693 net.cpp:150] Setting up ReLU5
I0215 10:12:15.980684 12693 net.cpp:157] Top shape: 1 64 300 500 (9600000)
I0215 10:12:15.980691 12693 net.cpp:165] Memory required for data: 783000028
I0215 10:12:15.980700 12693 layer_factory.hpp:77] Creating layer Convolution6
I0215 10:12:15.980720 12693 net.cpp:106] Creating Layer Convolution6
I0215 10:12:15.980726 12693 net.cpp:454] Convolution6 <- BatchNorm5
I0215 10:12:15.980743 12693 net.cpp:411] Convolution6 -> Convolution6
I0215 10:12:15.981062 12693 net.cpp:150] Setting up Convolution6
I0215 10:12:15.981076 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.981086 12693 net.cpp:165] Memory required for data: 790200028
I0215 10:12:15.981097 12693 layer_factory.hpp:77] Creating layer Dropout5
I0215 10:12:15.981109 12693 net.cpp:106] Creating Layer Dropout5
I0215 10:12:15.981118 12693 net.cpp:454] Dropout5 <- Convolution6
I0215 10:12:15.981134 12693 net.cpp:411] Dropout5 -> Dropout5
I0215 10:12:15.981195 12693 net.cpp:150] Setting up Dropout5
I0215 10:12:15.981209 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.981217 12693 net.cpp:165] Memory required for data: 797400028
I0215 10:12:15.981227 12693 layer_factory.hpp:77] Creating layer Concat5
I0215 10:12:15.981240 12693 net.cpp:106] Creating Layer Concat5
I0215 10:12:15.981248 12693 net.cpp:454] Concat5 <- Concat4_Concat4_0_split_1
I0215 10:12:15.981258 12693 net.cpp:454] Concat5 <- Dropout5
I0215 10:12:15.981272 12693 net.cpp:411] Concat5 -> Concat5
I0215 10:12:15.981314 12693 net.cpp:150] Setting up Concat5
I0215 10:12:15.981325 12693 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0215 10:12:15.981330 12693 net.cpp:165] Memory required for data: 843000028
I0215 10:12:15.981335 12693 layer_factory.hpp:77] Creating layer Concat5_Concat5_0_split
I0215 10:12:15.981345 12693 net.cpp:106] Creating Layer Concat5_Concat5_0_split
I0215 10:12:15.981353 12693 net.cpp:454] Concat5_Concat5_0_split <- Concat5
I0215 10:12:15.981361 12693 net.cpp:411] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_0
I0215 10:12:15.981370 12693 net.cpp:411] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_1
I0215 10:12:15.981426 12693 net.cpp:150] Setting up Concat5_Concat5_0_split
I0215 10:12:15.981436 12693 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0215 10:12:15.981443 12693 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0215 10:12:15.981448 12693 net.cpp:165] Memory required for data: 934200028
I0215 10:12:15.981453 12693 layer_factory.hpp:77] Creating layer BatchNorm6
I0215 10:12:15.981465 12693 net.cpp:106] Creating Layer BatchNorm6
I0215 10:12:15.981472 12693 net.cpp:454] BatchNorm6 <- Concat5_Concat5_0_split_0
I0215 10:12:15.981478 12693 net.cpp:411] BatchNorm6 -> BatchNorm6
I0215 10:12:15.982481 12693 net.cpp:150] Setting up BatchNorm6
I0215 10:12:15.982496 12693 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0215 10:12:15.982501 12693 net.cpp:165] Memory required for data: 979800028
I0215 10:12:15.982519 12693 layer_factory.hpp:77] Creating layer Scale6
I0215 10:12:15.982529 12693 net.cpp:106] Creating Layer Scale6
I0215 10:12:15.982535 12693 net.cpp:454] Scale6 <- BatchNorm6
I0215 10:12:15.982542 12693 net.cpp:397] Scale6 -> BatchNorm6 (in-place)
I0215 10:12:15.982602 12693 layer_factory.hpp:77] Creating layer Scale6
I0215 10:12:15.984221 12693 net.cpp:150] Setting up Scale6
I0215 10:12:15.984239 12693 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0215 10:12:15.984244 12693 net.cpp:165] Memory required for data: 1025400028
I0215 10:12:15.984252 12693 layer_factory.hpp:77] Creating layer ReLU6
I0215 10:12:15.984261 12693 net.cpp:106] Creating Layer ReLU6
I0215 10:12:15.984268 12693 net.cpp:454] ReLU6 <- BatchNorm6
I0215 10:12:15.984277 12693 net.cpp:397] ReLU6 -> BatchNorm6 (in-place)
I0215 10:12:15.984285 12693 net.cpp:150] Setting up ReLU6
I0215 10:12:15.984293 12693 net.cpp:157] Top shape: 1 76 300 500 (11400000)
I0215 10:12:15.984298 12693 net.cpp:165] Memory required for data: 1071000028
I0215 10:12:15.984303 12693 layer_factory.hpp:77] Creating layer Convolution7
I0215 10:12:15.984313 12693 net.cpp:106] Creating Layer Convolution7
I0215 10:12:15.984319 12693 net.cpp:454] Convolution7 <- BatchNorm6
I0215 10:12:15.984328 12693 net.cpp:411] Convolution7 -> Convolution7
I0215 10:12:15.984653 12693 net.cpp:150] Setting up Convolution7
I0215 10:12:15.984665 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.984671 12693 net.cpp:165] Memory required for data: 1078200028
I0215 10:12:15.984678 12693 layer_factory.hpp:77] Creating layer Dropout6
I0215 10:12:15.984686 12693 net.cpp:106] Creating Layer Dropout6
I0215 10:12:15.984693 12693 net.cpp:454] Dropout6 <- Convolution7
I0215 10:12:15.984704 12693 net.cpp:411] Dropout6 -> Dropout6
I0215 10:12:15.984758 12693 net.cpp:150] Setting up Dropout6
I0215 10:12:15.984769 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.984774 12693 net.cpp:165] Memory required for data: 1085400028
I0215 10:12:15.984779 12693 layer_factory.hpp:77] Creating layer Concat6
I0215 10:12:15.984789 12693 net.cpp:106] Creating Layer Concat6
I0215 10:12:15.984796 12693 net.cpp:454] Concat6 <- Concat5_Concat5_0_split_1
I0215 10:12:15.984803 12693 net.cpp:454] Concat6 <- Dropout6
I0215 10:12:15.984810 12693 net.cpp:411] Concat6 -> Concat6
I0215 10:12:15.984848 12693 net.cpp:150] Setting up Concat6
I0215 10:12:15.984858 12693 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0215 10:12:15.984864 12693 net.cpp:165] Memory required for data: 1138200028
I0215 10:12:15.984869 12693 layer_factory.hpp:77] Creating layer Concat6_Concat6_0_split
I0215 10:12:15.984876 12693 net.cpp:106] Creating Layer Concat6_Concat6_0_split
I0215 10:12:15.984882 12693 net.cpp:454] Concat6_Concat6_0_split <- Concat6
I0215 10:12:15.984891 12693 net.cpp:411] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_0
I0215 10:12:15.984900 12693 net.cpp:411] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_1
I0215 10:12:15.984953 12693 net.cpp:150] Setting up Concat6_Concat6_0_split
I0215 10:12:15.984963 12693 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0215 10:12:15.984971 12693 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0215 10:12:15.984975 12693 net.cpp:165] Memory required for data: 1243800028
I0215 10:12:15.984980 12693 layer_factory.hpp:77] Creating layer BatchNorm7
I0215 10:12:15.984993 12693 net.cpp:106] Creating Layer BatchNorm7
I0215 10:12:15.985002 12693 net.cpp:454] BatchNorm7 <- Concat6_Concat6_0_split_0
I0215 10:12:15.985010 12693 net.cpp:411] BatchNorm7 -> BatchNorm7
I0215 10:12:15.986035 12693 net.cpp:150] Setting up BatchNorm7
I0215 10:12:15.986050 12693 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0215 10:12:15.986055 12693 net.cpp:165] Memory required for data: 1296600028
I0215 10:12:15.986068 12693 layer_factory.hpp:77] Creating layer Scale7
I0215 10:12:15.986078 12693 net.cpp:106] Creating Layer Scale7
I0215 10:12:15.986085 12693 net.cpp:454] Scale7 <- BatchNorm7
I0215 10:12:15.986093 12693 net.cpp:397] Scale7 -> BatchNorm7 (in-place)
I0215 10:12:15.986155 12693 layer_factory.hpp:77] Creating layer Scale7
I0215 10:12:15.988132 12693 net.cpp:150] Setting up Scale7
I0215 10:12:15.988181 12693 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0215 10:12:15.988189 12693 net.cpp:165] Memory required for data: 1349400028
I0215 10:12:15.988205 12693 layer_factory.hpp:77] Creating layer ReLU7
I0215 10:12:15.988224 12693 net.cpp:106] Creating Layer ReLU7
I0215 10:12:15.988234 12693 net.cpp:454] ReLU7 <- BatchNorm7
I0215 10:12:15.988247 12693 net.cpp:397] ReLU7 -> BatchNorm7 (in-place)
I0215 10:12:15.988265 12693 net.cpp:150] Setting up ReLU7
I0215 10:12:15.988276 12693 net.cpp:157] Top shape: 1 88 300 500 (13200000)
I0215 10:12:15.988283 12693 net.cpp:165] Memory required for data: 1402200028
I0215 10:12:15.988289 12693 layer_factory.hpp:77] Creating layer Convolution8
I0215 10:12:15.988307 12693 net.cpp:106] Creating Layer Convolution8
I0215 10:12:15.988315 12693 net.cpp:454] Convolution8 <- BatchNorm7
I0215 10:12:15.988328 12693 net.cpp:411] Convolution8 -> Convolution8
I0215 10:12:15.988821 12693 net.cpp:150] Setting up Convolution8
I0215 10:12:15.988836 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.988842 12693 net.cpp:165] Memory required for data: 1409400028
I0215 10:12:15.988849 12693 layer_factory.hpp:77] Creating layer Dropout7
I0215 10:12:15.988863 12693 net.cpp:106] Creating Layer Dropout7
I0215 10:12:15.988872 12693 net.cpp:454] Dropout7 <- Convolution8
I0215 10:12:15.988879 12693 net.cpp:411] Dropout7 -> Dropout7
I0215 10:12:15.988942 12693 net.cpp:150] Setting up Dropout7
I0215 10:12:15.988955 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.988962 12693 net.cpp:165] Memory required for data: 1416600028
I0215 10:12:15.988967 12693 layer_factory.hpp:77] Creating layer Concat7
I0215 10:12:15.988978 12693 net.cpp:106] Creating Layer Concat7
I0215 10:12:15.988986 12693 net.cpp:454] Concat7 <- Concat6_Concat6_0_split_1
I0215 10:12:15.988992 12693 net.cpp:454] Concat7 <- Dropout7
I0215 10:12:15.989003 12693 net.cpp:411] Concat7 -> Concat7
I0215 10:12:15.989048 12693 net.cpp:150] Setting up Concat7
I0215 10:12:15.989074 12693 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0215 10:12:15.989083 12693 net.cpp:165] Memory required for data: 1476600028
I0215 10:12:15.989089 12693 layer_factory.hpp:77] Creating layer Concat7_Concat7_0_split
I0215 10:12:15.989101 12693 net.cpp:106] Creating Layer Concat7_Concat7_0_split
I0215 10:12:15.989110 12693 net.cpp:454] Concat7_Concat7_0_split <- Concat7
I0215 10:12:15.989118 12693 net.cpp:411] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_0
I0215 10:12:15.989130 12693 net.cpp:411] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_1
I0215 10:12:15.989198 12693 net.cpp:150] Setting up Concat7_Concat7_0_split
I0215 10:12:15.989212 12693 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0215 10:12:15.989220 12693 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0215 10:12:15.989228 12693 net.cpp:165] Memory required for data: 1596600028
I0215 10:12:15.989234 12693 layer_factory.hpp:77] Creating layer BatchNorm8
I0215 10:12:15.989248 12693 net.cpp:106] Creating Layer BatchNorm8
I0215 10:12:15.989256 12693 net.cpp:454] BatchNorm8 <- Concat7_Concat7_0_split_0
I0215 10:12:15.989265 12693 net.cpp:411] BatchNorm8 -> BatchNorm8
I0215 10:12:15.990507 12693 net.cpp:150] Setting up BatchNorm8
I0215 10:12:15.990526 12693 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0215 10:12:15.990535 12693 net.cpp:165] Memory required for data: 1656600028
I0215 10:12:15.990548 12693 layer_factory.hpp:77] Creating layer Scale8
I0215 10:12:15.990562 12693 net.cpp:106] Creating Layer Scale8
I0215 10:12:15.990571 12693 net.cpp:454] Scale8 <- BatchNorm8
I0215 10:12:15.990582 12693 net.cpp:397] Scale8 -> BatchNorm8 (in-place)
I0215 10:12:15.990664 12693 layer_factory.hpp:77] Creating layer Scale8
I0215 10:12:15.992542 12693 net.cpp:150] Setting up Scale8
I0215 10:12:15.992560 12693 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0215 10:12:15.992569 12693 net.cpp:165] Memory required for data: 1716600028
I0215 10:12:15.992580 12693 layer_factory.hpp:77] Creating layer ReLU8
I0215 10:12:15.992593 12693 net.cpp:106] Creating Layer ReLU8
I0215 10:12:15.992604 12693 net.cpp:454] ReLU8 <- BatchNorm8
I0215 10:12:15.992614 12693 net.cpp:397] ReLU8 -> BatchNorm8 (in-place)
I0215 10:12:15.992626 12693 net.cpp:150] Setting up ReLU8
I0215 10:12:15.992635 12693 net.cpp:157] Top shape: 1 100 300 500 (15000000)
I0215 10:12:15.992642 12693 net.cpp:165] Memory required for data: 1776600028
I0215 10:12:15.992650 12693 layer_factory.hpp:77] Creating layer Convolution9
I0215 10:12:15.992666 12693 net.cpp:106] Creating Layer Convolution9
I0215 10:12:15.992673 12693 net.cpp:454] Convolution9 <- BatchNorm8
I0215 10:12:15.992683 12693 net.cpp:411] Convolution9 -> Convolution9
I0215 10:12:15.993147 12693 net.cpp:150] Setting up Convolution9
I0215 10:12:15.993163 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.993170 12693 net.cpp:165] Memory required for data: 1783800028
I0215 10:12:15.993180 12693 layer_factory.hpp:77] Creating layer Dropout8
I0215 10:12:15.993191 12693 net.cpp:106] Creating Layer Dropout8
I0215 10:12:15.993199 12693 net.cpp:454] Dropout8 <- Convolution9
I0215 10:12:15.993207 12693 net.cpp:411] Dropout8 -> Dropout8
I0215 10:12:15.993280 12693 net.cpp:150] Setting up Dropout8
I0215 10:12:15.993297 12693 net.cpp:157] Top shape: 1 12 300 500 (1800000)
I0215 10:12:15.993305 12693 net.cpp:165] Memory required for data: 1791000028
I0215 10:12:15.993311 12693 layer_factory.hpp:77] Creating layer Concat8
I0215 10:12:15.993322 12693 net.cpp:106] Creating Layer Concat8
I0215 10:12:15.993330 12693 net.cpp:454] Concat8 <- Concat7_Concat7_0_split_1
I0215 10:12:15.993338 12693 net.cpp:454] Concat8 <- Dropout8
I0215 10:12:15.993348 12693 net.cpp:411] Concat8 -> Concat8
I0215 10:12:15.993401 12693 net.cpp:150] Setting up Concat8
I0215 10:12:15.993413 12693 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0215 10:12:15.993422 12693 net.cpp:165] Memory required for data: 1858200028
I0215 10:12:15.993427 12693 layer_factory.hpp:77] Creating layer BatchNorm9
I0215 10:12:15.993438 12693 net.cpp:106] Creating Layer BatchNorm9
I0215 10:12:15.993446 12693 net.cpp:454] BatchNorm9 <- Concat8
I0215 10:12:15.993458 12693 net.cpp:411] BatchNorm9 -> BatchNorm9
I0215 10:12:15.994678 12693 net.cpp:150] Setting up BatchNorm9
I0215 10:12:15.994698 12693 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0215 10:12:15.994705 12693 net.cpp:165] Memory required for data: 1925400028
I0215 10:12:15.994719 12693 layer_factory.hpp:77] Creating layer Scale9
I0215 10:12:15.994735 12693 net.cpp:106] Creating Layer Scale9
I0215 10:12:15.994742 12693 net.cpp:454] Scale9 <- BatchNorm9
I0215 10:12:15.994752 12693 net.cpp:397] Scale9 -> BatchNorm9 (in-place)
I0215 10:12:15.994834 12693 layer_factory.hpp:77] Creating layer Scale9
I0215 10:12:15.996717 12693 net.cpp:150] Setting up Scale9
I0215 10:12:15.996737 12693 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0215 10:12:15.996748 12693 net.cpp:165] Memory required for data: 1992600028
I0215 10:12:15.996759 12693 layer_factory.hpp:77] Creating layer ReLU9
I0215 10:12:15.996769 12693 net.cpp:106] Creating Layer ReLU9
I0215 10:12:15.996775 12693 net.cpp:454] ReLU9 <- BatchNorm9
I0215 10:12:15.996784 12693 net.cpp:397] ReLU9 -> BatchNorm9 (in-place)
I0215 10:12:15.996795 12693 net.cpp:150] Setting up ReLU9
I0215 10:12:15.996804 12693 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0215 10:12:15.996814 12693 net.cpp:165] Memory required for data: 2059800028
I0215 10:12:15.996819 12693 layer_factory.hpp:77] Creating layer Convolution10
I0215 10:12:15.996835 12693 net.cpp:106] Creating Layer Convolution10
I0215 10:12:15.996843 12693 net.cpp:454] Convolution10 <- BatchNorm9
I0215 10:12:15.996855 12693 net.cpp:411] Convolution10 -> Convolution10
I0215 10:12:15.997357 12693 net.cpp:150] Setting up Convolution10
I0215 10:12:15.997372 12693 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0215 10:12:15.997378 12693 net.cpp:165] Memory required for data: 2127000028
I0215 10:12:15.997387 12693 layer_factory.hpp:77] Creating layer Dropout9
I0215 10:12:15.997396 12693 net.cpp:106] Creating Layer Dropout9
I0215 10:12:15.997402 12693 net.cpp:454] Dropout9 <- Convolution10
I0215 10:12:15.997413 12693 net.cpp:411] Dropout9 -> Dropout9
I0215 10:12:15.997490 12693 net.cpp:150] Setting up Dropout9
I0215 10:12:15.997504 12693 net.cpp:157] Top shape: 1 112 300 500 (16800000)
I0215 10:12:15.997511 12693 net.cpp:165] Memory required for data: 2194200028
I0215 10:12:15.997517 12693 layer_factory.hpp:77] Creating layer Pooling1
I0215 10:12:15.997544 12693 net.cpp:106] Creating Layer Pooling1
I0215 10:12:15.997550 12693 net.cpp:454] Pooling1 <- Dropout9
I0215 10:12:15.997560 12693 net.cpp:411] Pooling1 -> Pooling1
I0215 10:12:15.997611 12693 net.cpp:150] Setting up Pooling1
I0215 10:12:15.997622 12693 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0215 10:12:15.997627 12693 net.cpp:165] Memory required for data: 2211000028
I0215 10:12:15.997633 12693 layer_factory.hpp:77] Creating layer Pooling1_Pooling1_0_split
I0215 10:12:15.997642 12693 net.cpp:106] Creating Layer Pooling1_Pooling1_0_split
I0215 10:12:15.997648 12693 net.cpp:454] Pooling1_Pooling1_0_split <- Pooling1
I0215 10:12:15.997658 12693 net.cpp:411] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_0
I0215 10:12:15.997668 12693 net.cpp:411] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_1
I0215 10:12:15.997735 12693 net.cpp:150] Setting up Pooling1_Pooling1_0_split
I0215 10:12:15.997746 12693 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0215 10:12:15.997753 12693 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0215 10:12:15.997759 12693 net.cpp:165] Memory required for data: 2244600028
I0215 10:12:15.997766 12693 layer_factory.hpp:77] Creating layer BatchNorm10
I0215 10:12:15.997787 12693 net.cpp:106] Creating Layer BatchNorm10
I0215 10:12:15.997793 12693 net.cpp:454] BatchNorm10 <- Pooling1_Pooling1_0_split_0
I0215 10:12:15.997802 12693 net.cpp:411] BatchNorm10 -> BatchNorm10
I0215 10:12:15.998955 12693 net.cpp:150] Setting up BatchNorm10
I0215 10:12:15.998970 12693 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0215 10:12:15.998977 12693 net.cpp:165] Memory required for data: 2261400028
I0215 10:12:15.998989 12693 layer_factory.hpp:77] Creating layer Scale10
I0215 10:12:15.999002 12693 net.cpp:106] Creating Layer Scale10
I0215 10:12:15.999009 12693 net.cpp:454] Scale10 <- BatchNorm10
I0215 10:12:15.999017 12693 net.cpp:397] Scale10 -> BatchNorm10 (in-place)
I0215 10:12:15.999096 12693 layer_factory.hpp:77] Creating layer Scale10
I0215 10:12:15.999418 12693 net.cpp:150] Setting up Scale10
I0215 10:12:15.999431 12693 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0215 10:12:15.999438 12693 net.cpp:165] Memory required for data: 2278200028
I0215 10:12:15.999447 12693 layer_factory.hpp:77] Creating layer ReLU10
I0215 10:12:15.999456 12693 net.cpp:106] Creating Layer ReLU10
I0215 10:12:15.999464 12693 net.cpp:454] ReLU10 <- BatchNorm10
I0215 10:12:15.999474 12693 net.cpp:397] ReLU10 -> BatchNorm10 (in-place)
I0215 10:12:15.999482 12693 net.cpp:150] Setting up ReLU10
I0215 10:12:15.999490 12693 net.cpp:157] Top shape: 1 112 150 250 (4200000)
I0215 10:12:15.999495 12693 net.cpp:165] Memory required for data: 2295000028
I0215 10:12:15.999500 12693 layer_factory.hpp:77] Creating layer Convolution11
I0215 10:12:15.999512 12693 net.cpp:106] Creating Layer Convolution11
I0215 10:12:15.999518 12693 net.cpp:454] Convolution11 <- BatchNorm10
I0215 10:12:15.999527 12693 net.cpp:411] Convolution11 -> Convolution11
I0215 10:12:16.000046 12693 net.cpp:150] Setting up Convolution11
I0215 10:12:16.000062 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.000072 12693 net.cpp:165] Memory required for data: 2296800028
I0215 10:12:16.000082 12693 layer_factory.hpp:77] Creating layer Dropout10
I0215 10:12:16.000093 12693 net.cpp:106] Creating Layer Dropout10
I0215 10:12:16.000102 12693 net.cpp:454] Dropout10 <- Convolution11
I0215 10:12:16.000114 12693 net.cpp:411] Dropout10 -> Dropout10
I0215 10:12:16.000203 12693 net.cpp:150] Setting up Dropout10
I0215 10:12:16.000216 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.000221 12693 net.cpp:165] Memory required for data: 2298600028
I0215 10:12:16.000226 12693 layer_factory.hpp:77] Creating layer Concat9
I0215 10:12:16.000232 12693 net.cpp:106] Creating Layer Concat9
I0215 10:12:16.000238 12693 net.cpp:454] Concat9 <- Pooling1_Pooling1_0_split_1
I0215 10:12:16.000246 12693 net.cpp:454] Concat9 <- Dropout10
I0215 10:12:16.000254 12693 net.cpp:411] Concat9 -> Concat9
I0215 10:12:16.000295 12693 net.cpp:150] Setting up Concat9
I0215 10:12:16.000305 12693 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0215 10:12:16.000310 12693 net.cpp:165] Memory required for data: 2317200028
I0215 10:12:16.000315 12693 layer_factory.hpp:77] Creating layer Concat9_Concat9_0_split
I0215 10:12:16.000322 12693 net.cpp:106] Creating Layer Concat9_Concat9_0_split
I0215 10:12:16.000337 12693 net.cpp:454] Concat9_Concat9_0_split <- Concat9
I0215 10:12:16.000346 12693 net.cpp:411] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_0
I0215 10:12:16.000363 12693 net.cpp:411] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_1
I0215 10:12:16.000423 12693 net.cpp:150] Setting up Concat9_Concat9_0_split
I0215 10:12:16.000437 12693 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0215 10:12:16.000442 12693 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0215 10:12:16.000447 12693 net.cpp:165] Memory required for data: 2354400028
I0215 10:12:16.000452 12693 layer_factory.hpp:77] Creating layer BatchNorm11
I0215 10:12:16.000463 12693 net.cpp:106] Creating Layer BatchNorm11
I0215 10:12:16.000469 12693 net.cpp:454] BatchNorm11 <- Concat9_Concat9_0_split_0
I0215 10:12:16.000481 12693 net.cpp:411] BatchNorm11 -> BatchNorm11
I0215 10:12:16.000865 12693 net.cpp:150] Setting up BatchNorm11
I0215 10:12:16.000877 12693 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0215 10:12:16.000882 12693 net.cpp:165] Memory required for data: 2373000028
I0215 10:12:16.000892 12693 layer_factory.hpp:77] Creating layer Scale11
I0215 10:12:16.000902 12693 net.cpp:106] Creating Layer Scale11
I0215 10:12:16.000907 12693 net.cpp:454] Scale11 <- BatchNorm11
I0215 10:12:16.000916 12693 net.cpp:397] Scale11 -> BatchNorm11 (in-place)
I0215 10:12:16.000990 12693 layer_factory.hpp:77] Creating layer Scale11
I0215 10:12:16.001231 12693 net.cpp:150] Setting up Scale11
I0215 10:12:16.001243 12693 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0215 10:12:16.001248 12693 net.cpp:165] Memory required for data: 2391600028
I0215 10:12:16.001268 12693 layer_factory.hpp:77] Creating layer ReLU11
I0215 10:12:16.001279 12693 net.cpp:106] Creating Layer ReLU11
I0215 10:12:16.001286 12693 net.cpp:454] ReLU11 <- BatchNorm11
I0215 10:12:16.001292 12693 net.cpp:397] ReLU11 -> BatchNorm11 (in-place)
I0215 10:12:16.001301 12693 net.cpp:150] Setting up ReLU11
I0215 10:12:16.001307 12693 net.cpp:157] Top shape: 1 124 150 250 (4650000)
I0215 10:12:16.001312 12693 net.cpp:165] Memory required for data: 2410200028
I0215 10:12:16.001317 12693 layer_factory.hpp:77] Creating layer Convolution12
I0215 10:12:16.001329 12693 net.cpp:106] Creating Layer Convolution12
I0215 10:12:16.001335 12693 net.cpp:454] Convolution12 <- BatchNorm11
I0215 10:12:16.001343 12693 net.cpp:411] Convolution12 -> Convolution12
I0215 10:12:16.001757 12693 net.cpp:150] Setting up Convolution12
I0215 10:12:16.001770 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.001773 12693 net.cpp:165] Memory required for data: 2412000028
I0215 10:12:16.001781 12693 layer_factory.hpp:77] Creating layer Dropout11
I0215 10:12:16.001790 12693 net.cpp:106] Creating Layer Dropout11
I0215 10:12:16.001794 12693 net.cpp:454] Dropout11 <- Convolution12
I0215 10:12:16.001803 12693 net.cpp:411] Dropout11 -> Dropout11
I0215 10:12:16.001866 12693 net.cpp:150] Setting up Dropout11
I0215 10:12:16.001878 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.001883 12693 net.cpp:165] Memory required for data: 2413800028
I0215 10:12:16.001888 12693 layer_factory.hpp:77] Creating layer Concat10
I0215 10:12:16.001896 12693 net.cpp:106] Creating Layer Concat10
I0215 10:12:16.001902 12693 net.cpp:454] Concat10 <- Concat9_Concat9_0_split_1
I0215 10:12:16.001909 12693 net.cpp:454] Concat10 <- Dropout11
I0215 10:12:16.001914 12693 net.cpp:411] Concat10 -> Concat10
I0215 10:12:16.001955 12693 net.cpp:150] Setting up Concat10
I0215 10:12:16.001966 12693 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0215 10:12:16.001971 12693 net.cpp:165] Memory required for data: 2434200028
I0215 10:12:16.001976 12693 layer_factory.hpp:77] Creating layer Concat10_Concat10_0_split
I0215 10:12:16.001983 12693 net.cpp:106] Creating Layer Concat10_Concat10_0_split
I0215 10:12:16.001988 12693 net.cpp:454] Concat10_Concat10_0_split <- Concat10
I0215 10:12:16.001998 12693 net.cpp:411] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_0
I0215 10:12:16.002007 12693 net.cpp:411] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_1
I0215 10:12:16.002063 12693 net.cpp:150] Setting up Concat10_Concat10_0_split
I0215 10:12:16.002074 12693 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0215 10:12:16.002079 12693 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0215 10:12:16.002084 12693 net.cpp:165] Memory required for data: 2475000028
I0215 10:12:16.002089 12693 layer_factory.hpp:77] Creating layer BatchNorm12
I0215 10:12:16.002099 12693 net.cpp:106] Creating Layer BatchNorm12
I0215 10:12:16.002104 12693 net.cpp:454] BatchNorm12 <- Concat10_Concat10_0_split_0
I0215 10:12:16.002113 12693 net.cpp:411] BatchNorm12 -> BatchNorm12
I0215 10:12:16.003089 12693 net.cpp:150] Setting up BatchNorm12
I0215 10:12:16.003103 12693 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0215 10:12:16.003108 12693 net.cpp:165] Memory required for data: 2495400028
I0215 10:12:16.003118 12693 layer_factory.hpp:77] Creating layer Scale12
I0215 10:12:16.003130 12693 net.cpp:106] Creating Layer Scale12
I0215 10:12:16.003136 12693 net.cpp:454] Scale12 <- BatchNorm12
I0215 10:12:16.003144 12693 net.cpp:397] Scale12 -> BatchNorm12 (in-place)
I0215 10:12:16.003209 12693 layer_factory.hpp:77] Creating layer Scale12
I0215 10:12:16.003482 12693 net.cpp:150] Setting up Scale12
I0215 10:12:16.003494 12693 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0215 10:12:16.003499 12693 net.cpp:165] Memory required for data: 2515800028
I0215 10:12:16.003509 12693 layer_factory.hpp:77] Creating layer ReLU12
I0215 10:12:16.003516 12693 net.cpp:106] Creating Layer ReLU12
I0215 10:12:16.003521 12693 net.cpp:454] ReLU12 <- BatchNorm12
I0215 10:12:16.003528 12693 net.cpp:397] ReLU12 -> BatchNorm12 (in-place)
I0215 10:12:16.003536 12693 net.cpp:150] Setting up ReLU12
I0215 10:12:16.003542 12693 net.cpp:157] Top shape: 1 136 150 250 (5100000)
I0215 10:12:16.003546 12693 net.cpp:165] Memory required for data: 2536200028
I0215 10:12:16.003551 12693 layer_factory.hpp:77] Creating layer Convolution13
I0215 10:12:16.003566 12693 net.cpp:106] Creating Layer Convolution13
I0215 10:12:16.003571 12693 net.cpp:454] Convolution13 <- BatchNorm12
I0215 10:12:16.003578 12693 net.cpp:411] Convolution13 -> Convolution13
I0215 10:12:16.004235 12693 net.cpp:150] Setting up Convolution13
I0215 10:12:16.004257 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.004262 12693 net.cpp:165] Memory required for data: 2538000028
I0215 10:12:16.004272 12693 layer_factory.hpp:77] Creating layer Dropout12
I0215 10:12:16.004284 12693 net.cpp:106] Creating Layer Dropout12
I0215 10:12:16.004292 12693 net.cpp:454] Dropout12 <- Convolution13
I0215 10:12:16.004300 12693 net.cpp:411] Dropout12 -> Dropout12
I0215 10:12:16.004371 12693 net.cpp:150] Setting up Dropout12
I0215 10:12:16.004381 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.004386 12693 net.cpp:165] Memory required for data: 2539800028
I0215 10:12:16.004391 12693 layer_factory.hpp:77] Creating layer Concat11
I0215 10:12:16.004398 12693 net.cpp:106] Creating Layer Concat11
I0215 10:12:16.004405 12693 net.cpp:454] Concat11 <- Concat10_Concat10_0_split_1
I0215 10:12:16.004410 12693 net.cpp:454] Concat11 <- Dropout12
I0215 10:12:16.004418 12693 net.cpp:411] Concat11 -> Concat11
I0215 10:12:16.004456 12693 net.cpp:150] Setting up Concat11
I0215 10:12:16.004467 12693 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0215 10:12:16.004470 12693 net.cpp:165] Memory required for data: 2562000028
I0215 10:12:16.004475 12693 layer_factory.hpp:77] Creating layer Concat11_Concat11_0_split
I0215 10:12:16.004484 12693 net.cpp:106] Creating Layer Concat11_Concat11_0_split
I0215 10:12:16.004489 12693 net.cpp:454] Concat11_Concat11_0_split <- Concat11
I0215 10:12:16.004495 12693 net.cpp:411] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_0
I0215 10:12:16.004503 12693 net.cpp:411] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_1
I0215 10:12:16.004559 12693 net.cpp:150] Setting up Concat11_Concat11_0_split
I0215 10:12:16.004568 12693 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0215 10:12:16.004575 12693 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0215 10:12:16.004578 12693 net.cpp:165] Memory required for data: 2606400028
I0215 10:12:16.004582 12693 layer_factory.hpp:77] Creating layer BatchNorm13
I0215 10:12:16.004591 12693 net.cpp:106] Creating Layer BatchNorm13
I0215 10:12:16.004596 12693 net.cpp:454] BatchNorm13 <- Concat11_Concat11_0_split_0
I0215 10:12:16.004606 12693 net.cpp:411] BatchNorm13 -> BatchNorm13
I0215 10:12:16.004961 12693 net.cpp:150] Setting up BatchNorm13
I0215 10:12:16.004971 12693 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0215 10:12:16.004976 12693 net.cpp:165] Memory required for data: 2628600028
I0215 10:12:16.004986 12693 layer_factory.hpp:77] Creating layer Scale13
I0215 10:12:16.004995 12693 net.cpp:106] Creating Layer Scale13
I0215 10:12:16.005000 12693 net.cpp:454] Scale13 <- BatchNorm13
I0215 10:12:16.005007 12693 net.cpp:397] Scale13 -> BatchNorm13 (in-place)
I0215 10:12:16.005071 12693 layer_factory.hpp:77] Creating layer Scale13
I0215 10:12:16.005319 12693 net.cpp:150] Setting up Scale13
I0215 10:12:16.005331 12693 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0215 10:12:16.005334 12693 net.cpp:165] Memory required for data: 2650800028
I0215 10:12:16.005342 12693 layer_factory.hpp:77] Creating layer ReLU13
I0215 10:12:16.005350 12693 net.cpp:106] Creating Layer ReLU13
I0215 10:12:16.005355 12693 net.cpp:454] ReLU13 <- BatchNorm13
I0215 10:12:16.005363 12693 net.cpp:397] ReLU13 -> BatchNorm13 (in-place)
I0215 10:12:16.005370 12693 net.cpp:150] Setting up ReLU13
I0215 10:12:16.005376 12693 net.cpp:157] Top shape: 1 148 150 250 (5550000)
I0215 10:12:16.005380 12693 net.cpp:165] Memory required for data: 2673000028
I0215 10:12:16.005385 12693 layer_factory.hpp:77] Creating layer Convolution14
I0215 10:12:16.005394 12693 net.cpp:106] Creating Layer Convolution14
I0215 10:12:16.005399 12693 net.cpp:454] Convolution14 <- BatchNorm13
I0215 10:12:16.005406 12693 net.cpp:411] Convolution14 -> Convolution14
I0215 10:12:16.005825 12693 net.cpp:150] Setting up Convolution14
I0215 10:12:16.005837 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.005841 12693 net.cpp:165] Memory required for data: 2674800028
I0215 10:12:16.005848 12693 layer_factory.hpp:77] Creating layer Dropout13
I0215 10:12:16.005857 12693 net.cpp:106] Creating Layer Dropout13
I0215 10:12:16.005862 12693 net.cpp:454] Dropout13 <- Convolution14
I0215 10:12:16.005869 12693 net.cpp:411] Dropout13 -> Dropout13
I0215 10:12:16.005928 12693 net.cpp:150] Setting up Dropout13
I0215 10:12:16.005937 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.005941 12693 net.cpp:165] Memory required for data: 2676600028
I0215 10:12:16.005946 12693 layer_factory.hpp:77] Creating layer Concat12
I0215 10:12:16.005954 12693 net.cpp:106] Creating Layer Concat12
I0215 10:12:16.005960 12693 net.cpp:454] Concat12 <- Concat11_Concat11_0_split_1
I0215 10:12:16.005965 12693 net.cpp:454] Concat12 <- Dropout13
I0215 10:12:16.005971 12693 net.cpp:411] Concat12 -> Concat12
I0215 10:12:16.006007 12693 net.cpp:150] Setting up Concat12
I0215 10:12:16.006016 12693 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0215 10:12:16.006021 12693 net.cpp:165] Memory required for data: 2700600028
I0215 10:12:16.006026 12693 layer_factory.hpp:77] Creating layer Concat12_Concat12_0_split
I0215 10:12:16.006033 12693 net.cpp:106] Creating Layer Concat12_Concat12_0_split
I0215 10:12:16.006038 12693 net.cpp:454] Concat12_Concat12_0_split <- Concat12
I0215 10:12:16.006045 12693 net.cpp:411] Concat12_Concat12_0_split -> Concat12_Concat12_0_split_0
I0215 10:12:16.006052 12693 net.cpp:411] Concat12_Concat12_0_split -> Concat12_Concat12_0_split_1
I0215 10:12:16.006108 12693 net.cpp:150] Setting up Concat12_Concat12_0_split
I0215 10:12:16.006116 12693 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0215 10:12:16.006122 12693 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0215 10:12:16.006126 12693 net.cpp:165] Memory required for data: 2748600028
I0215 10:12:16.006130 12693 layer_factory.hpp:77] Creating layer BatchNorm14
I0215 10:12:16.006139 12693 net.cpp:106] Creating Layer BatchNorm14
I0215 10:12:16.006145 12693 net.cpp:454] BatchNorm14 <- Concat12_Concat12_0_split_0
I0215 10:12:16.006152 12693 net.cpp:411] BatchNorm14 -> BatchNorm14
I0215 10:12:16.010035 12693 net.cpp:150] Setting up BatchNorm14
I0215 10:12:16.010051 12693 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0215 10:12:16.010056 12693 net.cpp:165] Memory required for data: 2772600028
I0215 10:12:16.010067 12693 layer_factory.hpp:77] Creating layer Scale14
I0215 10:12:16.010078 12693 net.cpp:106] Creating Layer Scale14
I0215 10:12:16.010083 12693 net.cpp:454] Scale14 <- BatchNorm14
I0215 10:12:16.010090 12693 net.cpp:397] Scale14 -> BatchNorm14 (in-place)
I0215 10:12:16.010165 12693 layer_factory.hpp:77] Creating layer Scale14
I0215 10:12:16.010393 12693 net.cpp:150] Setting up Scale14
I0215 10:12:16.010404 12693 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0215 10:12:16.010409 12693 net.cpp:165] Memory required for data: 2796600028
I0215 10:12:16.010417 12693 layer_factory.hpp:77] Creating layer ReLU14
I0215 10:12:16.010424 12693 net.cpp:106] Creating Layer ReLU14
I0215 10:12:16.010429 12693 net.cpp:454] ReLU14 <- BatchNorm14
I0215 10:12:16.010435 12693 net.cpp:397] ReLU14 -> BatchNorm14 (in-place)
I0215 10:12:16.010443 12693 net.cpp:150] Setting up ReLU14
I0215 10:12:16.010448 12693 net.cpp:157] Top shape: 1 160 150 250 (6000000)
I0215 10:12:16.010453 12693 net.cpp:165] Memory required for data: 2820600028
I0215 10:12:16.010457 12693 layer_factory.hpp:77] Creating layer Convolution15
I0215 10:12:16.010468 12693 net.cpp:106] Creating Layer Convolution15
I0215 10:12:16.010473 12693 net.cpp:454] Convolution15 <- BatchNorm14
I0215 10:12:16.010480 12693 net.cpp:411] Convolution15 -> Convolution15
I0215 10:12:16.010920 12693 net.cpp:150] Setting up Convolution15
I0215 10:12:16.010931 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.010936 12693 net.cpp:165] Memory required for data: 2822400028
I0215 10:12:16.010943 12693 layer_factory.hpp:77] Creating layer Dropout14
I0215 10:12:16.010951 12693 net.cpp:106] Creating Layer Dropout14
I0215 10:12:16.010956 12693 net.cpp:454] Dropout14 <- Convolution15
I0215 10:12:16.010965 12693 net.cpp:411] Dropout14 -> Dropout14
I0215 10:12:16.011026 12693 net.cpp:150] Setting up Dropout14
I0215 10:12:16.011036 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.011041 12693 net.cpp:165] Memory required for data: 2824200028
I0215 10:12:16.011045 12693 layer_factory.hpp:77] Creating layer Concat13
I0215 10:12:16.011052 12693 net.cpp:106] Creating Layer Concat13
I0215 10:12:16.011057 12693 net.cpp:454] Concat13 <- Concat12_Concat12_0_split_1
I0215 10:12:16.011063 12693 net.cpp:454] Concat13 <- Dropout14
I0215 10:12:16.011070 12693 net.cpp:411] Concat13 -> Concat13
I0215 10:12:16.011107 12693 net.cpp:150] Setting up Concat13
I0215 10:12:16.011116 12693 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0215 10:12:16.011121 12693 net.cpp:165] Memory required for data: 2850000028
I0215 10:12:16.011126 12693 layer_factory.hpp:77] Creating layer Concat13_Concat13_0_split
I0215 10:12:16.011132 12693 net.cpp:106] Creating Layer Concat13_Concat13_0_split
I0215 10:12:16.011137 12693 net.cpp:454] Concat13_Concat13_0_split <- Concat13
I0215 10:12:16.011144 12693 net.cpp:411] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_0
I0215 10:12:16.011152 12693 net.cpp:411] Concat13_Concat13_0_split -> Concat13_Concat13_0_split_1
I0215 10:12:16.011209 12693 net.cpp:150] Setting up Concat13_Concat13_0_split
I0215 10:12:16.011217 12693 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0215 10:12:16.011224 12693 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0215 10:12:16.011227 12693 net.cpp:165] Memory required for data: 2901600028
I0215 10:12:16.011232 12693 layer_factory.hpp:77] Creating layer BatchNorm15
I0215 10:12:16.011240 12693 net.cpp:106] Creating Layer BatchNorm15
I0215 10:12:16.011243 12693 net.cpp:454] BatchNorm15 <- Concat13_Concat13_0_split_0
I0215 10:12:16.011252 12693 net.cpp:411] BatchNorm15 -> BatchNorm15
I0215 10:12:16.011610 12693 net.cpp:150] Setting up BatchNorm15
I0215 10:12:16.011621 12693 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0215 10:12:16.011626 12693 net.cpp:165] Memory required for data: 2927400028
I0215 10:12:16.011634 12693 layer_factory.hpp:77] Creating layer Scale15
I0215 10:12:16.011644 12693 net.cpp:106] Creating Layer Scale15
I0215 10:12:16.011651 12693 net.cpp:454] Scale15 <- BatchNorm15
I0215 10:12:16.011658 12693 net.cpp:397] Scale15 -> BatchNorm15 (in-place)
I0215 10:12:16.011720 12693 layer_factory.hpp:77] Creating layer Scale15
I0215 10:12:16.011945 12693 net.cpp:150] Setting up Scale15
I0215 10:12:16.011955 12693 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0215 10:12:16.011960 12693 net.cpp:165] Memory required for data: 2953200028
I0215 10:12:16.011967 12693 layer_factory.hpp:77] Creating layer ReLU15
I0215 10:12:16.011975 12693 net.cpp:106] Creating Layer ReLU15
I0215 10:12:16.011979 12693 net.cpp:454] ReLU15 <- BatchNorm15
I0215 10:12:16.011984 12693 net.cpp:397] ReLU15 -> BatchNorm15 (in-place)
I0215 10:12:16.011992 12693 net.cpp:150] Setting up ReLU15
I0215 10:12:16.011997 12693 net.cpp:157] Top shape: 1 172 150 250 (6450000)
I0215 10:12:16.012002 12693 net.cpp:165] Memory required for data: 2979000028
I0215 10:12:16.012006 12693 layer_factory.hpp:77] Creating layer Convolution16
I0215 10:12:16.012019 12693 net.cpp:106] Creating Layer Convolution16
I0215 10:12:16.012024 12693 net.cpp:454] Convolution16 <- BatchNorm15
I0215 10:12:16.012032 12693 net.cpp:411] Convolution16 -> Convolution16
I0215 10:12:16.012441 12693 net.cpp:150] Setting up Convolution16
I0215 10:12:16.012452 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.012457 12693 net.cpp:165] Memory required for data: 2980800028
I0215 10:12:16.012464 12693 layer_factory.hpp:77] Creating layer Dropout15
I0215 10:12:16.012470 12693 net.cpp:106] Creating Layer Dropout15
I0215 10:12:16.012475 12693 net.cpp:454] Dropout15 <- Convolution16
I0215 10:12:16.012482 12693 net.cpp:411] Dropout15 -> Dropout15
I0215 10:12:16.012542 12693 net.cpp:150] Setting up Dropout15
I0215 10:12:16.012553 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.012557 12693 net.cpp:165] Memory required for data: 2982600028
I0215 10:12:16.012562 12693 layer_factory.hpp:77] Creating layer Concat14
I0215 10:12:16.012568 12693 net.cpp:106] Creating Layer Concat14
I0215 10:12:16.012573 12693 net.cpp:454] Concat14 <- Concat13_Concat13_0_split_1
I0215 10:12:16.012579 12693 net.cpp:454] Concat14 <- Dropout15
I0215 10:12:16.012585 12693 net.cpp:411] Concat14 -> Concat14
I0215 10:12:16.012624 12693 net.cpp:150] Setting up Concat14
I0215 10:12:16.012632 12693 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0215 10:12:16.012636 12693 net.cpp:165] Memory required for data: 3010200028
I0215 10:12:16.012641 12693 layer_factory.hpp:77] Creating layer Concat14_Concat14_0_split
I0215 10:12:16.012647 12693 net.cpp:106] Creating Layer Concat14_Concat14_0_split
I0215 10:12:16.012652 12693 net.cpp:454] Concat14_Concat14_0_split <- Concat14
I0215 10:12:16.012658 12693 net.cpp:411] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_0
I0215 10:12:16.012665 12693 net.cpp:411] Concat14_Concat14_0_split -> Concat14_Concat14_0_split_1
I0215 10:12:16.012719 12693 net.cpp:150] Setting up Concat14_Concat14_0_split
I0215 10:12:16.012730 12693 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0215 10:12:16.012735 12693 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0215 10:12:16.012740 12693 net.cpp:165] Memory required for data: 3065400028
I0215 10:12:16.012744 12693 layer_factory.hpp:77] Creating layer BatchNorm16
I0215 10:12:16.012751 12693 net.cpp:106] Creating Layer BatchNorm16
I0215 10:12:16.012756 12693 net.cpp:454] BatchNorm16 <- Concat14_Concat14_0_split_0
I0215 10:12:16.012764 12693 net.cpp:411] BatchNorm16 -> BatchNorm16
I0215 10:12:16.013928 12693 net.cpp:150] Setting up BatchNorm16
I0215 10:12:16.013942 12693 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0215 10:12:16.013947 12693 net.cpp:165] Memory required for data: 3093000028
I0215 10:12:16.013957 12693 layer_factory.hpp:77] Creating layer Scale16
I0215 10:12:16.013967 12693 net.cpp:106] Creating Layer Scale16
I0215 10:12:16.013972 12693 net.cpp:454] Scale16 <- BatchNorm16
I0215 10:12:16.013978 12693 net.cpp:397] Scale16 -> BatchNorm16 (in-place)
I0215 10:12:16.014041 12693 layer_factory.hpp:77] Creating layer Scale16
I0215 10:12:16.014271 12693 net.cpp:150] Setting up Scale16
I0215 10:12:16.014281 12693 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0215 10:12:16.014286 12693 net.cpp:165] Memory required for data: 3120600028
I0215 10:12:16.014293 12693 layer_factory.hpp:77] Creating layer ReLU16
I0215 10:12:16.014302 12693 net.cpp:106] Creating Layer ReLU16
I0215 10:12:16.014305 12693 net.cpp:454] ReLU16 <- BatchNorm16
I0215 10:12:16.014312 12693 net.cpp:397] ReLU16 -> BatchNorm16 (in-place)
I0215 10:12:16.014318 12693 net.cpp:150] Setting up ReLU16
I0215 10:12:16.014324 12693 net.cpp:157] Top shape: 1 184 150 250 (6900000)
I0215 10:12:16.014328 12693 net.cpp:165] Memory required for data: 3148200028
I0215 10:12:16.014333 12693 layer_factory.hpp:77] Creating layer Convolution17
I0215 10:12:16.014341 12693 net.cpp:106] Creating Layer Convolution17
I0215 10:12:16.014346 12693 net.cpp:454] Convolution17 <- BatchNorm16
I0215 10:12:16.014354 12693 net.cpp:411] Convolution17 -> Convolution17
I0215 10:12:16.014816 12693 net.cpp:150] Setting up Convolution17
I0215 10:12:16.014827 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.014832 12693 net.cpp:165] Memory required for data: 3150000028
I0215 10:12:16.014838 12693 layer_factory.hpp:77] Creating layer Dropout16
I0215 10:12:16.014848 12693 net.cpp:106] Creating Layer Dropout16
I0215 10:12:16.014853 12693 net.cpp:454] Dropout16 <- Convolution17
I0215 10:12:16.014860 12693 net.cpp:411] Dropout16 -> Dropout16
I0215 10:12:16.014920 12693 net.cpp:150] Setting up Dropout16
I0215 10:12:16.014930 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.014935 12693 net.cpp:165] Memory required for data: 3151800028
I0215 10:12:16.014940 12693 layer_factory.hpp:77] Creating layer Concat15
I0215 10:12:16.014945 12693 net.cpp:106] Creating Layer Concat15
I0215 10:12:16.014950 12693 net.cpp:454] Concat15 <- Concat14_Concat14_0_split_1
I0215 10:12:16.014956 12693 net.cpp:454] Concat15 <- Dropout16
I0215 10:12:16.014964 12693 net.cpp:411] Concat15 -> Concat15
I0215 10:12:16.015000 12693 net.cpp:150] Setting up Concat15
I0215 10:12:16.015010 12693 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0215 10:12:16.015014 12693 net.cpp:165] Memory required for data: 3181200028
I0215 10:12:16.015020 12693 layer_factory.hpp:77] Creating layer Concat15_Concat15_0_split
I0215 10:12:16.015028 12693 net.cpp:106] Creating Layer Concat15_Concat15_0_split
I0215 10:12:16.015033 12693 net.cpp:454] Concat15_Concat15_0_split <- Concat15
I0215 10:12:16.015039 12693 net.cpp:411] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_0
I0215 10:12:16.015046 12693 net.cpp:411] Concat15_Concat15_0_split -> Concat15_Concat15_0_split_1
I0215 10:12:16.015102 12693 net.cpp:150] Setting up Concat15_Concat15_0_split
I0215 10:12:16.015111 12693 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0215 10:12:16.015117 12693 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0215 10:12:16.015121 12693 net.cpp:165] Memory required for data: 3240000028
I0215 10:12:16.015126 12693 layer_factory.hpp:77] Creating layer BatchNorm17
I0215 10:12:16.015135 12693 net.cpp:106] Creating Layer BatchNorm17
I0215 10:12:16.015139 12693 net.cpp:454] BatchNorm17 <- Concat15_Concat15_0_split_0
I0215 10:12:16.015146 12693 net.cpp:411] BatchNorm17 -> BatchNorm17
I0215 10:12:16.015620 12693 net.cpp:150] Setting up BatchNorm17
I0215 10:12:16.015632 12693 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0215 10:12:16.015636 12693 net.cpp:165] Memory required for data: 3269400028
I0215 10:12:16.015645 12693 layer_factory.hpp:77] Creating layer Scale17
I0215 10:12:16.015655 12693 net.cpp:106] Creating Layer Scale17
I0215 10:12:16.015661 12693 net.cpp:454] Scale17 <- BatchNorm17
I0215 10:12:16.015667 12693 net.cpp:397] Scale17 -> BatchNorm17 (in-place)
I0215 10:12:16.015733 12693 layer_factory.hpp:77] Creating layer Scale17
I0215 10:12:16.015970 12693 net.cpp:150] Setting up Scale17
I0215 10:12:16.015982 12693 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0215 10:12:16.015986 12693 net.cpp:165] Memory required for data: 3298800028
I0215 10:12:16.015995 12693 layer_factory.hpp:77] Creating layer ReLU17
I0215 10:12:16.016001 12693 net.cpp:106] Creating Layer ReLU17
I0215 10:12:16.016006 12693 net.cpp:454] ReLU17 <- BatchNorm17
I0215 10:12:16.016012 12693 net.cpp:397] ReLU17 -> BatchNorm17 (in-place)
I0215 10:12:16.016019 12693 net.cpp:150] Setting up ReLU17
I0215 10:12:16.016026 12693 net.cpp:157] Top shape: 1 196 150 250 (7350000)
I0215 10:12:16.016029 12693 net.cpp:165] Memory required for data: 3328200028
I0215 10:12:16.016033 12693 layer_factory.hpp:77] Creating layer Convolution18
I0215 10:12:16.016042 12693 net.cpp:106] Creating Layer Convolution18
I0215 10:12:16.016047 12693 net.cpp:454] Convolution18 <- BatchNorm17
I0215 10:12:16.016057 12693 net.cpp:411] Convolution18 -> Convolution18
I0215 10:12:16.016495 12693 net.cpp:150] Setting up Convolution18
I0215 10:12:16.016506 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.016510 12693 net.cpp:165] Memory required for data: 3330000028
I0215 10:12:16.016516 12693 layer_factory.hpp:77] Creating layer Dropout17
I0215 10:12:16.016525 12693 net.cpp:106] Creating Layer Dropout17
I0215 10:12:16.016530 12693 net.cpp:454] Dropout17 <- Convolution18
I0215 10:12:16.016536 12693 net.cpp:411] Dropout17 -> Dropout17
I0215 10:12:16.016594 12693 net.cpp:150] Setting up Dropout17
I0215 10:12:16.016604 12693 net.cpp:157] Top shape: 1 12 150 250 (450000)
I0215 10:12:16.016608 12693 net.cpp:165] Memory required for data: 3331800028
I0215 10:12:16.016613 12693 layer_factory.hpp:77] Creating layer Concat16
I0215 10:12:16.016619 12693 net.cpp:106] Creating Layer Concat16
I0215 10:12:16.016628 12693 net.cpp:454] Concat16 <- Concat15_Concat15_0_split_1
I0215 10:12:16.016634 12693 net.cpp:454] Concat16 <- Dropout17
I0215 10:12:16.016640 12693 net.cpp:411] Concat16 -> Concat16
I0215 10:12:16.016676 12693 net.cpp:150] Setting up Concat16
I0215 10:12:16.016685 12693 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0215 10:12:16.016690 12693 net.cpp:165] Memory required for data: 3363000028
I0215 10:12:16.016695 12693 layer_factory.hpp:77] Creating layer BatchNorm18
I0215 10:12:16.016702 12693 net.cpp:106] Creating Layer BatchNorm18
I0215 10:12:16.016707 12693 net.cpp:454] BatchNorm18 <- Concat16
I0215 10:12:16.016713 12693 net.cpp:411] BatchNorm18 -> BatchNorm18
I0215 10:12:16.025073 12693 net.cpp:150] Setting up BatchNorm18
I0215 10:12:16.025097 12693 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0215 10:12:16.025105 12693 net.cpp:165] Memory required for data: 3394200028
I0215 10:12:16.025125 12693 layer_factory.hpp:77] Creating layer Scale18
I0215 10:12:16.025137 12693 net.cpp:106] Creating Layer Scale18
I0215 10:12:16.025146 12693 net.cpp:454] Scale18 <- BatchNorm18
I0215 10:12:16.025158 12693 net.cpp:397] Scale18 -> BatchNorm18 (in-place)
I0215 10:12:16.025282 12693 layer_factory.hpp:77] Creating layer Scale18
I0215 10:12:16.025737 12693 net.cpp:150] Setting up Scale18
I0215 10:12:16.025758 12693 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0215 10:12:16.025768 12693 net.cpp:165] Memory required for data: 3425400028
I0215 10:12:16.025782 12693 layer_factory.hpp:77] Creating layer ReLU18
I0215 10:12:16.025804 12693 net.cpp:106] Creating Layer ReLU18
I0215 10:12:16.025815 12693 net.cpp:454] ReLU18 <- BatchNorm18
I0215 10:12:16.025826 12693 net.cpp:397] ReLU18 -> BatchNorm18 (in-place)
I0215 10:12:16.025840 12693 net.cpp:150] Setting up ReLU18
I0215 10:12:16.025852 12693 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0215 10:12:16.025861 12693 net.cpp:165] Memory required for data: 3456600028
I0215 10:12:16.025868 12693 layer_factory.hpp:77] Creating layer Convolution19
I0215 10:12:16.025887 12693 net.cpp:106] Creating Layer Convolution19
I0215 10:12:16.025897 12693 net.cpp:454] Convolution19 <- BatchNorm18
I0215 10:12:16.025910 12693 net.cpp:411] Convolution19 -> Convolution19
I0215 10:12:16.026859 12693 net.cpp:150] Setting up Convolution19
I0215 10:12:16.026880 12693 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0215 10:12:16.026887 12693 net.cpp:165] Memory required for data: 3487800028
I0215 10:12:16.026901 12693 layer_factory.hpp:77] Creating layer Dropout18
I0215 10:12:16.026913 12693 net.cpp:106] Creating Layer Dropout18
I0215 10:12:16.026923 12693 net.cpp:454] Dropout18 <- Convolution19
I0215 10:12:16.026942 12693 net.cpp:411] Dropout18 -> Dropout18
I0215 10:12:16.027055 12693 net.cpp:150] Setting up Dropout18
I0215 10:12:16.027070 12693 net.cpp:157] Top shape: 1 208 150 250 (7800000)
I0215 10:12:16.027078 12693 net.cpp:165] Memory required for data: 3519000028
I0215 10:12:16.027086 12693 layer_factory.hpp:77] Creating layer Pooling2
I0215 10:12:16.027102 12693 net.cpp:106] Creating Layer Pooling2
I0215 10:12:16.027112 12693 net.cpp:454] Pooling2 <- Dropout18
I0215 10:12:16.027127 12693 net.cpp:411] Pooling2 -> Pooling2
I0215 10:12:16.027184 12693 net.cpp:150] Setting up Pooling2
I0215 10:12:16.027196 12693 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0215 10:12:16.027204 12693 net.cpp:165] Memory required for data: 3526800028
I0215 10:12:16.027212 12693 layer_factory.hpp:77] Creating layer Pooling2_Pooling2_0_split
I0215 10:12:16.027225 12693 net.cpp:106] Creating Layer Pooling2_Pooling2_0_split
I0215 10:12:16.027235 12693 net.cpp:454] Pooling2_Pooling2_0_split <- Pooling2
I0215 10:12:16.027254 12693 net.cpp:411] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_0
I0215 10:12:16.027269 12693 net.cpp:411] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_1
I0215 10:12:16.027367 12693 net.cpp:150] Setting up Pooling2_Pooling2_0_split
I0215 10:12:16.027384 12693 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0215 10:12:16.027392 12693 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0215 10:12:16.027400 12693 net.cpp:165] Memory required for data: 3542400028
I0215 10:12:16.027407 12693 layer_factory.hpp:77] Creating layer BatchNorm19
I0215 10:12:16.027442 12693 net.cpp:106] Creating Layer BatchNorm19
I0215 10:12:16.027451 12693 net.cpp:454] BatchNorm19 <- Pooling2_Pooling2_0_split_0
I0215 10:12:16.027462 12693 net.cpp:411] BatchNorm19 -> BatchNorm19
I0215 10:12:16.028044 12693 net.cpp:150] Setting up BatchNorm19
I0215 10:12:16.028069 12693 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0215 10:12:16.028077 12693 net.cpp:165] Memory required for data: 3550200028
I0215 10:12:16.028102 12693 layer_factory.hpp:77] Creating layer Scale19
I0215 10:12:16.028118 12693 net.cpp:106] Creating Layer Scale19
I0215 10:12:16.028128 12693 net.cpp:454] Scale19 <- BatchNorm19
I0215 10:12:16.028146 12693 net.cpp:397] Scale19 -> BatchNorm19 (in-place)
I0215 10:12:16.028254 12693 layer_factory.hpp:77] Creating layer Scale19
I0215 10:12:16.028604 12693 net.cpp:150] Setting up Scale19
I0215 10:12:16.028620 12693 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0215 10:12:16.028628 12693 net.cpp:165] Memory required for data: 3558000028
I0215 10:12:16.028642 12693 layer_factory.hpp:77] Creating layer ReLU19
I0215 10:12:16.028656 12693 net.cpp:106] Creating Layer ReLU19
I0215 10:12:16.028666 12693 net.cpp:454] ReLU19 <- BatchNorm19
I0215 10:12:16.028681 12693 net.cpp:397] ReLU19 -> BatchNorm19 (in-place)
I0215 10:12:16.028697 12693 net.cpp:150] Setting up ReLU19
I0215 10:12:16.028709 12693 net.cpp:157] Top shape: 1 208 75 125 (1950000)
I0215 10:12:16.028717 12693 net.cpp:165] Memory required for data: 3565800028
I0215 10:12:16.028725 12693 layer_factory.hpp:77] Creating layer Convolution20
I0215 10:12:16.028740 12693 net.cpp:106] Creating Layer Convolution20
I0215 10:12:16.028749 12693 net.cpp:454] Convolution20 <- BatchNorm19
I0215 10:12:16.028761 12693 net.cpp:411] Convolution20 -> Convolution20
I0215 10:12:16.029475 12693 net.cpp:150] Setting up Convolution20
I0215 10:12:16.029492 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.029500 12693 net.cpp:165] Memory required for data: 3566250028
I0215 10:12:16.029512 12693 layer_factory.hpp:77] Creating layer Dropout19
I0215 10:12:16.029525 12693 net.cpp:106] Creating Layer Dropout19
I0215 10:12:16.029532 12693 net.cpp:454] Dropout19 <- Convolution20
I0215 10:12:16.029546 12693 net.cpp:411] Dropout19 -> Dropout19
I0215 10:12:16.029642 12693 net.cpp:150] Setting up Dropout19
I0215 10:12:16.029659 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.029666 12693 net.cpp:165] Memory required for data: 3566700028
I0215 10:12:16.029675 12693 layer_factory.hpp:77] Creating layer Concat17
I0215 10:12:16.029686 12693 net.cpp:106] Creating Layer Concat17
I0215 10:12:16.029695 12693 net.cpp:454] Concat17 <- Pooling2_Pooling2_0_split_1
I0215 10:12:16.029706 12693 net.cpp:454] Concat17 <- Dropout19
I0215 10:12:16.029722 12693 net.cpp:411] Concat17 -> Concat17
I0215 10:12:16.029783 12693 net.cpp:150] Setting up Concat17
I0215 10:12:16.029798 12693 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0215 10:12:16.029804 12693 net.cpp:165] Memory required for data: 3574950028
I0215 10:12:16.029813 12693 layer_factory.hpp:77] Creating layer Concat17_Concat17_0_split
I0215 10:12:16.029822 12693 net.cpp:106] Creating Layer Concat17_Concat17_0_split
I0215 10:12:16.029830 12693 net.cpp:454] Concat17_Concat17_0_split <- Concat17
I0215 10:12:16.029844 12693 net.cpp:411] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_0
I0215 10:12:16.029856 12693 net.cpp:411] Concat17_Concat17_0_split -> Concat17_Concat17_0_split_1
I0215 10:12:16.029943 12693 net.cpp:150] Setting up Concat17_Concat17_0_split
I0215 10:12:16.029956 12693 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0215 10:12:16.029965 12693 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0215 10:12:16.029971 12693 net.cpp:165] Memory required for data: 3591450028
I0215 10:12:16.029978 12693 layer_factory.hpp:77] Creating layer BatchNorm20
I0215 10:12:16.029992 12693 net.cpp:106] Creating Layer BatchNorm20
I0215 10:12:16.030000 12693 net.cpp:454] BatchNorm20 <- Concat17_Concat17_0_split_0
I0215 10:12:16.030011 12693 net.cpp:411] BatchNorm20 -> BatchNorm20
I0215 10:12:16.030570 12693 net.cpp:150] Setting up BatchNorm20
I0215 10:12:16.030586 12693 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0215 10:12:16.030593 12693 net.cpp:165] Memory required for data: 3599700028
I0215 10:12:16.030608 12693 layer_factory.hpp:77] Creating layer Scale20
I0215 10:12:16.030625 12693 net.cpp:106] Creating Layer Scale20
I0215 10:12:16.030634 12693 net.cpp:454] Scale20 <- BatchNorm20
I0215 10:12:16.030644 12693 net.cpp:397] Scale20 -> BatchNorm20 (in-place)
I0215 10:12:16.030746 12693 layer_factory.hpp:77] Creating layer Scale20
I0215 10:12:16.031121 12693 net.cpp:150] Setting up Scale20
I0215 10:12:16.031137 12693 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0215 10:12:16.031146 12693 net.cpp:165] Memory required for data: 3607950028
I0215 10:12:16.031160 12693 layer_factory.hpp:77] Creating layer ReLU20
I0215 10:12:16.031173 12693 net.cpp:106] Creating Layer ReLU20
I0215 10:12:16.031183 12693 net.cpp:454] ReLU20 <- BatchNorm20
I0215 10:12:16.031193 12693 net.cpp:397] ReLU20 -> BatchNorm20 (in-place)
I0215 10:12:16.031208 12693 net.cpp:150] Setting up ReLU20
I0215 10:12:16.031219 12693 net.cpp:157] Top shape: 1 220 75 125 (2062500)
I0215 10:12:16.031227 12693 net.cpp:165] Memory required for data: 3616200028
I0215 10:12:16.031236 12693 layer_factory.hpp:77] Creating layer Convolution21
I0215 10:12:16.031255 12693 net.cpp:106] Creating Layer Convolution21
I0215 10:12:16.031265 12693 net.cpp:454] Convolution21 <- BatchNorm20
I0215 10:12:16.031287 12693 net.cpp:411] Convolution21 -> Convolution21
I0215 10:12:16.032086 12693 net.cpp:150] Setting up Convolution21
I0215 10:12:16.032109 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.032117 12693 net.cpp:165] Memory required for data: 3616650028
I0215 10:12:16.032140 12693 layer_factory.hpp:77] Creating layer Dropout20
I0215 10:12:16.032163 12693 net.cpp:106] Creating Layer Dropout20
I0215 10:12:16.032173 12693 net.cpp:454] Dropout20 <- Convolution21
I0215 10:12:16.032186 12693 net.cpp:411] Dropout20 -> Dropout20
I0215 10:12:16.032282 12693 net.cpp:150] Setting up Dropout20
I0215 10:12:16.032299 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.032307 12693 net.cpp:165] Memory required for data: 3617100028
I0215 10:12:16.032330 12693 layer_factory.hpp:77] Creating layer Concat18
I0215 10:12:16.032349 12693 net.cpp:106] Creating Layer Concat18
I0215 10:12:16.032361 12693 net.cpp:454] Concat18 <- Concat17_Concat17_0_split_1
I0215 10:12:16.032371 12693 net.cpp:454] Concat18 <- Dropout20
I0215 10:12:16.032387 12693 net.cpp:411] Concat18 -> Concat18
I0215 10:12:16.032449 12693 net.cpp:150] Setting up Concat18
I0215 10:12:16.032466 12693 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0215 10:12:16.032474 12693 net.cpp:165] Memory required for data: 3625800028
I0215 10:12:16.032482 12693 layer_factory.hpp:77] Creating layer Concat18_Concat18_0_split
I0215 10:12:16.032500 12693 net.cpp:106] Creating Layer Concat18_Concat18_0_split
I0215 10:12:16.032521 12693 net.cpp:454] Concat18_Concat18_0_split <- Concat18
I0215 10:12:16.032536 12693 net.cpp:411] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_0
I0215 10:12:16.032552 12693 net.cpp:411] Concat18_Concat18_0_split -> Concat18_Concat18_0_split_1
I0215 10:12:16.032649 12693 net.cpp:150] Setting up Concat18_Concat18_0_split
I0215 10:12:16.032665 12693 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0215 10:12:16.032676 12693 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0215 10:12:16.032691 12693 net.cpp:165] Memory required for data: 3643200028
I0215 10:12:16.032708 12693 layer_factory.hpp:77] Creating layer BatchNorm21
I0215 10:12:16.032726 12693 net.cpp:106] Creating Layer BatchNorm21
I0215 10:12:16.032737 12693 net.cpp:454] BatchNorm21 <- Concat18_Concat18_0_split_0
I0215 10:12:16.032757 12693 net.cpp:411] BatchNorm21 -> BatchNorm21
I0215 10:12:16.033303 12693 net.cpp:150] Setting up BatchNorm21
I0215 10:12:16.033327 12693 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0215 10:12:16.033346 12693 net.cpp:165] Memory required for data: 3651900028
I0215 10:12:16.033370 12693 layer_factory.hpp:77] Creating layer Scale21
I0215 10:12:16.033390 12693 net.cpp:106] Creating Layer Scale21
I0215 10:12:16.033404 12693 net.cpp:454] Scale21 <- BatchNorm21
I0215 10:12:16.033432 12693 net.cpp:397] Scale21 -> BatchNorm21 (in-place)
I0215 10:12:16.033534 12693 layer_factory.hpp:77] Creating layer Scale21
I0215 10:12:16.033887 12693 net.cpp:150] Setting up Scale21
I0215 10:12:16.033905 12693 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0215 10:12:16.033915 12693 net.cpp:165] Memory required for data: 3660600028
I0215 10:12:16.033931 12693 layer_factory.hpp:77] Creating layer ReLU21
I0215 10:12:16.033946 12693 net.cpp:106] Creating Layer ReLU21
I0215 10:12:16.033954 12693 net.cpp:454] ReLU21 <- BatchNorm21
I0215 10:12:16.033983 12693 net.cpp:397] ReLU21 -> BatchNorm21 (in-place)
I0215 10:12:16.033998 12693 net.cpp:150] Setting up ReLU21
I0215 10:12:16.034009 12693 net.cpp:157] Top shape: 1 232 75 125 (2175000)
I0215 10:12:16.034021 12693 net.cpp:165] Memory required for data: 3669300028
I0215 10:12:16.034029 12693 layer_factory.hpp:77] Creating layer Convolution22
I0215 10:12:16.034054 12693 net.cpp:106] Creating Layer Convolution22
I0215 10:12:16.034075 12693 net.cpp:454] Convolution22 <- BatchNorm21
I0215 10:12:16.034092 12693 net.cpp:411] Convolution22 -> Convolution22
I0215 10:12:16.034801 12693 net.cpp:150] Setting up Convolution22
I0215 10:12:16.034819 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.034828 12693 net.cpp:165] Memory required for data: 3669750028
I0215 10:12:16.034840 12693 layer_factory.hpp:77] Creating layer Dropout21
I0215 10:12:16.034857 12693 net.cpp:106] Creating Layer Dropout21
I0215 10:12:16.034868 12693 net.cpp:454] Dropout21 <- Convolution22
I0215 10:12:16.034891 12693 net.cpp:411] Dropout21 -> Dropout21
I0215 10:12:16.034991 12693 net.cpp:150] Setting up Dropout21
I0215 10:12:16.035006 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.035014 12693 net.cpp:165] Memory required for data: 3670200028
I0215 10:12:16.035023 12693 layer_factory.hpp:77] Creating layer Concat19
I0215 10:12:16.035035 12693 net.cpp:106] Creating Layer Concat19
I0215 10:12:16.035046 12693 net.cpp:454] Concat19 <- Concat18_Concat18_0_split_1
I0215 10:12:16.035056 12693 net.cpp:454] Concat19 <- Dropout21
I0215 10:12:16.035089 12693 net.cpp:411] Concat19 -> Concat19
I0215 10:12:16.035146 12693 net.cpp:150] Setting up Concat19
I0215 10:12:16.035172 12693 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0215 10:12:16.035179 12693 net.cpp:165] Memory required for data: 3679350028
I0215 10:12:16.035188 12693 layer_factory.hpp:77] Creating layer Concat19_Concat19_0_split
I0215 10:12:16.035205 12693 net.cpp:106] Creating Layer Concat19_Concat19_0_split
I0215 10:12:16.035215 12693 net.cpp:454] Concat19_Concat19_0_split <- Concat19
I0215 10:12:16.035229 12693 net.cpp:411] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_0
I0215 10:12:16.035271 12693 net.cpp:411] Concat19_Concat19_0_split -> Concat19_Concat19_0_split_1
I0215 10:12:16.035369 12693 net.cpp:150] Setting up Concat19_Concat19_0_split
I0215 10:12:16.035387 12693 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0215 10:12:16.035399 12693 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0215 10:12:16.035406 12693 net.cpp:165] Memory required for data: 3697650028
I0215 10:12:16.035418 12693 layer_factory.hpp:77] Creating layer BatchNorm22
I0215 10:12:16.035447 12693 net.cpp:106] Creating Layer BatchNorm22
I0215 10:12:16.035459 12693 net.cpp:454] BatchNorm22 <- Concat19_Concat19_0_split_0
I0215 10:12:16.035473 12693 net.cpp:411] BatchNorm22 -> BatchNorm22
I0215 10:12:16.035989 12693 net.cpp:150] Setting up BatchNorm22
I0215 10:12:16.036017 12693 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0215 10:12:16.036031 12693 net.cpp:165] Memory required for data: 3706800028
I0215 10:12:16.036085 12693 layer_factory.hpp:77] Creating layer Scale22
I0215 10:12:16.036106 12693 net.cpp:106] Creating Layer Scale22
I0215 10:12:16.036135 12693 net.cpp:454] Scale22 <- BatchNorm22
I0215 10:12:16.036149 12693 net.cpp:397] Scale22 -> BatchNorm22 (in-place)
I0215 10:12:16.036262 12693 layer_factory.hpp:77] Creating layer Scale22
I0215 10:12:16.036603 12693 net.cpp:150] Setting up Scale22
I0215 10:12:16.036622 12693 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0215 10:12:16.036631 12693 net.cpp:165] Memory required for data: 3715950028
I0215 10:12:16.036648 12693 layer_factory.hpp:77] Creating layer ReLU22
I0215 10:12:16.036660 12693 net.cpp:106] Creating Layer ReLU22
I0215 10:12:16.036689 12693 net.cpp:454] ReLU22 <- BatchNorm22
I0215 10:12:16.036702 12693 net.cpp:397] ReLU22 -> BatchNorm22 (in-place)
I0215 10:12:16.036715 12693 net.cpp:150] Setting up ReLU22
I0215 10:12:16.036726 12693 net.cpp:157] Top shape: 1 244 75 125 (2287500)
I0215 10:12:16.036733 12693 net.cpp:165] Memory required for data: 3725100028
I0215 10:12:16.036743 12693 layer_factory.hpp:77] Creating layer Convolution23
I0215 10:12:16.036767 12693 net.cpp:106] Creating Layer Convolution23
I0215 10:12:16.036785 12693 net.cpp:454] Convolution23 <- BatchNorm22
I0215 10:12:16.036801 12693 net.cpp:411] Convolution23 -> Convolution23
I0215 10:12:16.037536 12693 net.cpp:150] Setting up Convolution23
I0215 10:12:16.037554 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.037562 12693 net.cpp:165] Memory required for data: 3725550028
I0215 10:12:16.037573 12693 layer_factory.hpp:77] Creating layer Dropout22
I0215 10:12:16.037587 12693 net.cpp:106] Creating Layer Dropout22
I0215 10:12:16.037606 12693 net.cpp:454] Dropout22 <- Convolution23
I0215 10:12:16.037626 12693 net.cpp:411] Dropout22 -> Dropout22
I0215 10:12:16.037730 12693 net.cpp:150] Setting up Dropout22
I0215 10:12:16.037750 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.037760 12693 net.cpp:165] Memory required for data: 3726000028
I0215 10:12:16.037770 12693 layer_factory.hpp:77] Creating layer Concat20
I0215 10:12:16.037791 12693 net.cpp:106] Creating Layer Concat20
I0215 10:12:16.037807 12693 net.cpp:454] Concat20 <- Concat19_Concat19_0_split_1
I0215 10:12:16.037819 12693 net.cpp:454] Concat20 <- Dropout22
I0215 10:12:16.037837 12693 net.cpp:411] Concat20 -> Concat20
I0215 10:12:16.037904 12693 net.cpp:150] Setting up Concat20
I0215 10:12:16.037919 12693 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0215 10:12:16.037927 12693 net.cpp:165] Memory required for data: 3735600028
I0215 10:12:16.037945 12693 layer_factory.hpp:77] Creating layer Concat20_Concat20_0_split
I0215 10:12:16.037955 12693 net.cpp:106] Creating Layer Concat20_Concat20_0_split
I0215 10:12:16.037981 12693 net.cpp:454] Concat20_Concat20_0_split <- Concat20
I0215 10:12:16.037993 12693 net.cpp:411] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_0
I0215 10:12:16.038012 12693 net.cpp:411] Concat20_Concat20_0_split -> Concat20_Concat20_0_split_1
I0215 10:12:16.038103 12693 net.cpp:150] Setting up Concat20_Concat20_0_split
I0215 10:12:16.038117 12693 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0215 10:12:16.038130 12693 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0215 10:12:16.038139 12693 net.cpp:165] Memory required for data: 3754800028
I0215 10:12:16.038153 12693 layer_factory.hpp:77] Creating layer BatchNorm23
I0215 10:12:16.038177 12693 net.cpp:106] Creating Layer BatchNorm23
I0215 10:12:16.038188 12693 net.cpp:454] BatchNorm23 <- Concat20_Concat20_0_split_0
I0215 10:12:16.038209 12693 net.cpp:411] BatchNorm23 -> BatchNorm23
I0215 10:12:16.038704 12693 net.cpp:150] Setting up BatchNorm23
I0215 10:12:16.038733 12693 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0215 10:12:16.038739 12693 net.cpp:165] Memory required for data: 3764400028
I0215 10:12:16.038759 12693 layer_factory.hpp:77] Creating layer Scale23
I0215 10:12:16.038774 12693 net.cpp:106] Creating Layer Scale23
I0215 10:12:16.038784 12693 net.cpp:454] Scale23 <- BatchNorm23
I0215 10:12:16.038810 12693 net.cpp:397] Scale23 -> BatchNorm23 (in-place)
I0215 10:12:16.038936 12693 layer_factory.hpp:77] Creating layer Scale23
I0215 10:12:16.039247 12693 net.cpp:150] Setting up Scale23
I0215 10:12:16.039263 12693 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0215 10:12:16.039286 12693 net.cpp:165] Memory required for data: 3774000028
I0215 10:12:16.039302 12693 layer_factory.hpp:77] Creating layer ReLU23
I0215 10:12:16.039317 12693 net.cpp:106] Creating Layer ReLU23
I0215 10:12:16.039330 12693 net.cpp:454] ReLU23 <- BatchNorm23
I0215 10:12:16.039341 12693 net.cpp:397] ReLU23 -> BatchNorm23 (in-place)
I0215 10:12:16.039361 12693 net.cpp:150] Setting up ReLU23
I0215 10:12:16.039381 12693 net.cpp:157] Top shape: 1 256 75 125 (2400000)
I0215 10:12:16.039391 12693 net.cpp:165] Memory required for data: 3783600028
I0215 10:12:16.039400 12693 layer_factory.hpp:77] Creating layer Convolution24
I0215 10:12:16.039422 12693 net.cpp:106] Creating Layer Convolution24
I0215 10:12:16.039434 12693 net.cpp:454] Convolution24 <- BatchNorm23
I0215 10:12:16.039458 12693 net.cpp:411] Convolution24 -> Convolution24
I0215 10:12:16.040210 12693 net.cpp:150] Setting up Convolution24
I0215 10:12:16.040226 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.040235 12693 net.cpp:165] Memory required for data: 3784050028
I0215 10:12:16.040247 12693 layer_factory.hpp:77] Creating layer Dropout23
I0215 10:12:16.040261 12693 net.cpp:106] Creating Layer Dropout23
I0215 10:12:16.040271 12693 net.cpp:454] Dropout23 <- Convolution24
I0215 10:12:16.040295 12693 net.cpp:411] Dropout23 -> Dropout23
I0215 10:12:16.040390 12693 net.cpp:150] Setting up Dropout23
I0215 10:12:16.040408 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.040417 12693 net.cpp:165] Memory required for data: 3784500028
I0215 10:12:16.040426 12693 layer_factory.hpp:77] Creating layer Concat21
I0215 10:12:16.040438 12693 net.cpp:106] Creating Layer Concat21
I0215 10:12:16.040448 12693 net.cpp:454] Concat21 <- Concat20_Concat20_0_split_1
I0215 10:12:16.040460 12693 net.cpp:454] Concat21 <- Dropout23
I0215 10:12:16.040473 12693 net.cpp:411] Concat21 -> Concat21
I0215 10:12:16.040532 12693 net.cpp:150] Setting up Concat21
I0215 10:12:16.040546 12693 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0215 10:12:16.040555 12693 net.cpp:165] Memory required for data: 3794550028
I0215 10:12:16.040565 12693 layer_factory.hpp:77] Creating layer Concat21_Concat21_0_split
I0215 10:12:16.040577 12693 net.cpp:106] Creating Layer Concat21_Concat21_0_split
I0215 10:12:16.040587 12693 net.cpp:454] Concat21_Concat21_0_split <- Concat21
I0215 10:12:16.040608 12693 net.cpp:411] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_0
I0215 10:12:16.040622 12693 net.cpp:411] Concat21_Concat21_0_split -> Concat21_Concat21_0_split_1
I0215 10:12:16.040710 12693 net.cpp:150] Setting up Concat21_Concat21_0_split
I0215 10:12:16.040727 12693 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0215 10:12:16.040740 12693 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0215 10:12:16.040748 12693 net.cpp:165] Memory required for data: 3814650028
I0215 10:12:16.040756 12693 layer_factory.hpp:77] Creating layer BatchNorm24
I0215 10:12:16.040768 12693 net.cpp:106] Creating Layer BatchNorm24
I0215 10:12:16.040778 12693 net.cpp:454] BatchNorm24 <- Concat21_Concat21_0_split_0
I0215 10:12:16.040794 12693 net.cpp:411] BatchNorm24 -> BatchNorm24
I0215 10:12:16.042110 12693 net.cpp:150] Setting up BatchNorm24
I0215 10:12:16.042125 12693 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0215 10:12:16.042130 12693 net.cpp:165] Memory required for data: 3824700028
I0215 10:12:16.042141 12693 layer_factory.hpp:77] Creating layer Scale24
I0215 10:12:16.042152 12693 net.cpp:106] Creating Layer Scale24
I0215 10:12:16.042160 12693 net.cpp:454] Scale24 <- BatchNorm24
I0215 10:12:16.042168 12693 net.cpp:397] Scale24 -> BatchNorm24 (in-place)
I0215 10:12:16.042244 12693 layer_factory.hpp:77] Creating layer Scale24
I0215 10:12:16.042474 12693 net.cpp:150] Setting up Scale24
I0215 10:12:16.042486 12693 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0215 10:12:16.042491 12693 net.cpp:165] Memory required for data: 3834750028
I0215 10:12:16.042500 12693 layer_factory.hpp:77] Creating layer ReLU24
I0215 10:12:16.042508 12693 net.cpp:106] Creating Layer ReLU24
I0215 10:12:16.042513 12693 net.cpp:454] ReLU24 <- BatchNorm24
I0215 10:12:16.042520 12693 net.cpp:397] ReLU24 -> BatchNorm24 (in-place)
I0215 10:12:16.042528 12693 net.cpp:150] Setting up ReLU24
I0215 10:12:16.042534 12693 net.cpp:157] Top shape: 1 268 75 125 (2512500)
I0215 10:12:16.042539 12693 net.cpp:165] Memory required for data: 3844800028
I0215 10:12:16.042543 12693 layer_factory.hpp:77] Creating layer Convolution25
I0215 10:12:16.042554 12693 net.cpp:106] Creating Layer Convolution25
I0215 10:12:16.042559 12693 net.cpp:454] Convolution25 <- BatchNorm24
I0215 10:12:16.042569 12693 net.cpp:411] Convolution25 -> Convolution25
I0215 10:12:16.043817 12693 net.cpp:150] Setting up Convolution25
I0215 10:12:16.043833 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.043838 12693 net.cpp:165] Memory required for data: 3845250028
I0215 10:12:16.043845 12693 layer_factory.hpp:77] Creating layer Dropout24
I0215 10:12:16.043854 12693 net.cpp:106] Creating Layer Dropout24
I0215 10:12:16.043859 12693 net.cpp:454] Dropout24 <- Convolution25
I0215 10:12:16.043869 12693 net.cpp:411] Dropout24 -> Dropout24
I0215 10:12:16.043937 12693 net.cpp:150] Setting up Dropout24
I0215 10:12:16.043948 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.043953 12693 net.cpp:165] Memory required for data: 3845700028
I0215 10:12:16.043958 12693 layer_factory.hpp:77] Creating layer Concat22
I0215 10:12:16.043968 12693 net.cpp:106] Creating Layer Concat22
I0215 10:12:16.043974 12693 net.cpp:454] Concat22 <- Concat21_Concat21_0_split_1
I0215 10:12:16.043980 12693 net.cpp:454] Concat22 <- Dropout24
I0215 10:12:16.043988 12693 net.cpp:411] Concat22 -> Concat22
I0215 10:12:16.044030 12693 net.cpp:150] Setting up Concat22
I0215 10:12:16.044040 12693 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0215 10:12:16.044044 12693 net.cpp:165] Memory required for data: 3856200028
I0215 10:12:16.044049 12693 layer_factory.hpp:77] Creating layer Concat22_Concat22_0_split
I0215 10:12:16.044056 12693 net.cpp:106] Creating Layer Concat22_Concat22_0_split
I0215 10:12:16.044061 12693 net.cpp:454] Concat22_Concat22_0_split <- Concat22
I0215 10:12:16.044071 12693 net.cpp:411] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_0
I0215 10:12:16.044080 12693 net.cpp:411] Concat22_Concat22_0_split -> Concat22_Concat22_0_split_1
I0215 10:12:16.044143 12693 net.cpp:150] Setting up Concat22_Concat22_0_split
I0215 10:12:16.044154 12693 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0215 10:12:16.044160 12693 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0215 10:12:16.044164 12693 net.cpp:165] Memory required for data: 3877200028
I0215 10:12:16.044169 12693 layer_factory.hpp:77] Creating layer BatchNorm25
I0215 10:12:16.044179 12693 net.cpp:106] Creating Layer BatchNorm25
I0215 10:12:16.044185 12693 net.cpp:454] BatchNorm25 <- Concat22_Concat22_0_split_0
I0215 10:12:16.044193 12693 net.cpp:411] BatchNorm25 -> BatchNorm25
I0215 10:12:16.044570 12693 net.cpp:150] Setting up BatchNorm25
I0215 10:12:16.044582 12693 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0215 10:12:16.044586 12693 net.cpp:165] Memory required for data: 3887700028
I0215 10:12:16.044596 12693 layer_factory.hpp:77] Creating layer Scale25
I0215 10:12:16.044605 12693 net.cpp:106] Creating Layer Scale25
I0215 10:12:16.044610 12693 net.cpp:454] Scale25 <- BatchNorm25
I0215 10:12:16.044620 12693 net.cpp:397] Scale25 -> BatchNorm25 (in-place)
I0215 10:12:16.044697 12693 layer_factory.hpp:77] Creating layer Scale25
I0215 10:12:16.044919 12693 net.cpp:150] Setting up Scale25
I0215 10:12:16.044931 12693 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0215 10:12:16.044936 12693 net.cpp:165] Memory required for data: 3898200028
I0215 10:12:16.044945 12693 layer_factory.hpp:77] Creating layer ReLU25
I0215 10:12:16.044955 12693 net.cpp:106] Creating Layer ReLU25
I0215 10:12:16.044960 12693 net.cpp:454] ReLU25 <- BatchNorm25
I0215 10:12:16.044967 12693 net.cpp:397] ReLU25 -> BatchNorm25 (in-place)
I0215 10:12:16.044975 12693 net.cpp:150] Setting up ReLU25
I0215 10:12:16.044981 12693 net.cpp:157] Top shape: 1 280 75 125 (2625000)
I0215 10:12:16.044986 12693 net.cpp:165] Memory required for data: 3908700028
I0215 10:12:16.044991 12693 layer_factory.hpp:77] Creating layer Convolution26
I0215 10:12:16.045001 12693 net.cpp:106] Creating Layer Convolution26
I0215 10:12:16.045006 12693 net.cpp:454] Convolution26 <- BatchNorm25
I0215 10:12:16.045016 12693 net.cpp:411] Convolution26 -> Convolution26
I0215 10:12:16.045583 12693 net.cpp:150] Setting up Convolution26
I0215 10:12:16.045594 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.045599 12693 net.cpp:165] Memory required for data: 3909150028
I0215 10:12:16.045606 12693 layer_factory.hpp:77] Creating layer Dropout25
I0215 10:12:16.045614 12693 net.cpp:106] Creating Layer Dropout25
I0215 10:12:16.045622 12693 net.cpp:454] Dropout25 <- Convolution26
I0215 10:12:16.045629 12693 net.cpp:411] Dropout25 -> Dropout25
I0215 10:12:16.045697 12693 net.cpp:150] Setting up Dropout25
I0215 10:12:16.045707 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.045717 12693 net.cpp:165] Memory required for data: 3909600028
I0215 10:12:16.045722 12693 layer_factory.hpp:77] Creating layer Concat23
I0215 10:12:16.045730 12693 net.cpp:106] Creating Layer Concat23
I0215 10:12:16.045735 12693 net.cpp:454] Concat23 <- Concat22_Concat22_0_split_1
I0215 10:12:16.045742 12693 net.cpp:454] Concat23 <- Dropout25
I0215 10:12:16.045750 12693 net.cpp:411] Concat23 -> Concat23
I0215 10:12:16.045791 12693 net.cpp:150] Setting up Concat23
I0215 10:12:16.045801 12693 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0215 10:12:16.045805 12693 net.cpp:165] Memory required for data: 3920550028
I0215 10:12:16.045810 12693 layer_factory.hpp:77] Creating layer Concat23_Concat23_0_split
I0215 10:12:16.045819 12693 net.cpp:106] Creating Layer Concat23_Concat23_0_split
I0215 10:12:16.045824 12693 net.cpp:454] Concat23_Concat23_0_split <- Concat23
I0215 10:12:16.045832 12693 net.cpp:411] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_0
I0215 10:12:16.045840 12693 net.cpp:411] Concat23_Concat23_0_split -> Concat23_Concat23_0_split_1
I0215 10:12:16.045904 12693 net.cpp:150] Setting up Concat23_Concat23_0_split
I0215 10:12:16.045914 12693 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0215 10:12:16.045922 12693 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0215 10:12:16.045925 12693 net.cpp:165] Memory required for data: 3942450028
I0215 10:12:16.045930 12693 layer_factory.hpp:77] Creating layer BatchNorm26
I0215 10:12:16.045940 12693 net.cpp:106] Creating Layer BatchNorm26
I0215 10:12:16.045946 12693 net.cpp:454] BatchNorm26 <- Concat23_Concat23_0_split_0
I0215 10:12:16.045953 12693 net.cpp:411] BatchNorm26 -> BatchNorm26
I0215 10:12:16.046329 12693 net.cpp:150] Setting up BatchNorm26
I0215 10:12:16.046340 12693 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0215 10:12:16.046345 12693 net.cpp:165] Memory required for data: 3953400028
I0215 10:12:16.046355 12693 layer_factory.hpp:77] Creating layer Scale26
I0215 10:12:16.046366 12693 net.cpp:106] Creating Layer Scale26
I0215 10:12:16.046372 12693 net.cpp:454] Scale26 <- BatchNorm26
I0215 10:12:16.046380 12693 net.cpp:397] Scale26 -> BatchNorm26 (in-place)
I0215 10:12:16.046455 12693 layer_factory.hpp:77] Creating layer Scale26
I0215 10:12:16.046686 12693 net.cpp:150] Setting up Scale26
I0215 10:12:16.046699 12693 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0215 10:12:16.046703 12693 net.cpp:165] Memory required for data: 3964350028
I0215 10:12:16.046711 12693 layer_factory.hpp:77] Creating layer ReLU26
I0215 10:12:16.046721 12693 net.cpp:106] Creating Layer ReLU26
I0215 10:12:16.046727 12693 net.cpp:454] ReLU26 <- BatchNorm26
I0215 10:12:16.046735 12693 net.cpp:397] ReLU26 -> BatchNorm26 (in-place)
I0215 10:12:16.046742 12693 net.cpp:150] Setting up ReLU26
I0215 10:12:16.046748 12693 net.cpp:157] Top shape: 1 292 75 125 (2737500)
I0215 10:12:16.046753 12693 net.cpp:165] Memory required for data: 3975300028
I0215 10:12:16.046758 12693 layer_factory.hpp:77] Creating layer Convolution27
I0215 10:12:16.046768 12693 net.cpp:106] Creating Layer Convolution27
I0215 10:12:16.046773 12693 net.cpp:454] Convolution27 <- BatchNorm26
I0215 10:12:16.046783 12693 net.cpp:411] Convolution27 -> Convolution27
I0215 10:12:16.047344 12693 net.cpp:150] Setting up Convolution27
I0215 10:12:16.047356 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.047360 12693 net.cpp:165] Memory required for data: 3975750028
I0215 10:12:16.047369 12693 layer_factory.hpp:77] Creating layer Dropout26
I0215 10:12:16.047379 12693 net.cpp:106] Creating Layer Dropout26
I0215 10:12:16.047384 12693 net.cpp:454] Dropout26 <- Convolution27
I0215 10:12:16.047391 12693 net.cpp:411] Dropout26 -> Dropout26
I0215 10:12:16.047458 12693 net.cpp:150] Setting up Dropout26
I0215 10:12:16.047468 12693 net.cpp:157] Top shape: 1 12 75 125 (112500)
I0215 10:12:16.047473 12693 net.cpp:165] Memory required for data: 3976200028
I0215 10:12:16.047485 12693 layer_factory.hpp:77] Creating layer Concat24
I0215 10:12:16.047493 12693 net.cpp:106] Creating Layer Concat24
I0215 10:12:16.047498 12693 net.cpp:454] Concat24 <- Concat23_Concat23_0_split_1
I0215 10:12:16.047504 12693 net.cpp:454] Concat24 <- Dropout26
I0215 10:12:16.047513 12693 net.cpp:411] Concat24 -> Concat24
I0215 10:12:16.047555 12693 net.cpp:150] Setting up Concat24
I0215 10:12:16.047565 12693 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0215 10:12:16.047570 12693 net.cpp:165] Memory required for data: 3987600028
I0215 10:12:16.047575 12693 layer_factory.hpp:77] Creating layer BatchNorm27
I0215 10:12:16.047585 12693 net.cpp:106] Creating Layer BatchNorm27
I0215 10:12:16.047600 12693 net.cpp:454] BatchNorm27 <- Concat24
I0215 10:12:16.047615 12693 net.cpp:411] BatchNorm27 -> BatchNorm27
I0215 10:12:16.048001 12693 net.cpp:150] Setting up BatchNorm27
I0215 10:12:16.048015 12693 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0215 10:12:16.048020 12693 net.cpp:165] Memory required for data: 3999000028
I0215 10:12:16.048032 12693 layer_factory.hpp:77] Creating layer Scale27
I0215 10:12:16.048045 12693 net.cpp:106] Creating Layer Scale27
I0215 10:12:16.048053 12693 net.cpp:454] Scale27 <- BatchNorm27
I0215 10:12:16.048060 12693 net.cpp:397] Scale27 -> BatchNorm27 (in-place)
I0215 10:12:16.048138 12693 layer_factory.hpp:77] Creating layer Scale27
I0215 10:12:16.048372 12693 net.cpp:150] Setting up Scale27
I0215 10:12:16.048387 12693 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0215 10:12:16.048393 12693 net.cpp:165] Memory required for data: 4010400028
I0215 10:12:16.048403 12693 layer_factory.hpp:77] Creating layer ReLU27
I0215 10:12:16.048411 12693 net.cpp:106] Creating Layer ReLU27
I0215 10:12:16.048418 12693 net.cpp:454] ReLU27 <- BatchNorm27
I0215 10:12:16.048426 12693 net.cpp:397] ReLU27 -> BatchNorm27 (in-place)
I0215 10:12:16.048436 12693 net.cpp:150] Setting up ReLU27
I0215 10:12:16.048444 12693 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0215 10:12:16.048450 12693 net.cpp:165] Memory required for data: 4021800028
I0215 10:12:16.048455 12693 layer_factory.hpp:77] Creating layer Convolution28
I0215 10:12:16.048467 12693 net.cpp:106] Creating Layer Convolution28
I0215 10:12:16.048473 12693 net.cpp:454] Convolution28 <- BatchNorm27
I0215 10:12:16.048483 12693 net.cpp:411] Convolution28 -> Convolution28
I0215 10:12:16.049527 12693 net.cpp:150] Setting up Convolution28
I0215 10:12:16.049545 12693 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0215 10:12:16.049551 12693 net.cpp:165] Memory required for data: 4033200028
I0215 10:12:16.049558 12693 layer_factory.hpp:77] Creating layer Dropout27
I0215 10:12:16.049567 12693 net.cpp:106] Creating Layer Dropout27
I0215 10:12:16.049574 12693 net.cpp:454] Dropout27 <- Convolution28
I0215 10:12:16.049584 12693 net.cpp:411] Dropout27 -> Dropout27
I0215 10:12:16.049654 12693 net.cpp:150] Setting up Dropout27
I0215 10:12:16.049667 12693 net.cpp:157] Top shape: 1 304 75 125 (2850000)
I0215 10:12:16.049674 12693 net.cpp:165] Memory required for data: 4044600028
I0215 10:12:16.049679 12693 layer_factory.hpp:77] Creating layer Pooling3
I0215 10:12:16.049691 12693 net.cpp:106] Creating Layer Pooling3
I0215 10:12:16.049698 12693 net.cpp:454] Pooling3 <- Dropout27
I0215 10:12:16.049706 12693 net.cpp:411] Pooling3 -> Pooling3
I0215 10:12:16.049751 12693 net.cpp:150] Setting up Pooling3
I0215 10:12:16.049765 12693 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0215 10:12:16.049772 12693 net.cpp:165] Memory required for data: 4047511132
I0215 10:12:16.049777 12693 layer_factory.hpp:77] Creating layer Pooling3_Pooling3_0_split
I0215 10:12:16.049787 12693 net.cpp:106] Creating Layer Pooling3_Pooling3_0_split
I0215 10:12:16.049793 12693 net.cpp:454] Pooling3_Pooling3_0_split <- Pooling3
I0215 10:12:16.049799 12693 net.cpp:411] Pooling3_Pooling3_0_split -> Pooling3_Pooling3_0_split_0
I0215 10:12:16.049809 12693 net.cpp:411] Pooling3_Pooling3_0_split -> Pooling3_Pooling3_0_split_1
I0215 10:12:16.049875 12693 net.cpp:150] Setting up Pooling3_Pooling3_0_split
I0215 10:12:16.049886 12693 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0215 10:12:16.049896 12693 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0215 10:12:16.049902 12693 net.cpp:165] Memory required for data: 4053333340
I0215 10:12:16.049907 12693 layer_factory.hpp:77] Creating layer BatchNorm28
I0215 10:12:16.049916 12693 net.cpp:106] Creating Layer BatchNorm28
I0215 10:12:16.049923 12693 net.cpp:454] BatchNorm28 <- Pooling3_Pooling3_0_split_0
I0215 10:12:16.049934 12693 net.cpp:411] BatchNorm28 -> BatchNorm28
I0215 10:12:16.050317 12693 net.cpp:150] Setting up BatchNorm28
I0215 10:12:16.050330 12693 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0215 10:12:16.050336 12693 net.cpp:165] Memory required for data: 4056244444
I0215 10:12:16.050353 12693 layer_factory.hpp:77] Creating layer Scale28
I0215 10:12:16.050364 12693 net.cpp:106] Creating Layer Scale28
I0215 10:12:16.050371 12693 net.cpp:454] Scale28 <- BatchNorm28
I0215 10:12:16.050382 12693 net.cpp:397] Scale28 -> BatchNorm28 (in-place)
I0215 10:12:16.050458 12693 layer_factory.hpp:77] Creating layer Scale28
I0215 10:12:16.050686 12693 net.cpp:150] Setting up Scale28
I0215 10:12:16.050699 12693 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0215 10:12:16.050706 12693 net.cpp:165] Memory required for data: 4059155548
I0215 10:12:16.050715 12693 layer_factory.hpp:77] Creating layer ReLU28
I0215 10:12:16.050724 12693 net.cpp:106] Creating Layer ReLU28
I0215 10:12:16.050731 12693 net.cpp:454] ReLU28 <- BatchNorm28
I0215 10:12:16.050740 12693 net.cpp:397] ReLU28 -> BatchNorm28 (in-place)
I0215 10:12:16.050750 12693 net.cpp:150] Setting up ReLU28
I0215 10:12:16.050757 12693 net.cpp:157] Top shape: 1 304 38 63 (727776)
I0215 10:12:16.050763 12693 net.cpp:165] Memory required for data: 4062066652
I0215 10:12:16.050768 12693 layer_factory.hpp:77] Creating layer Convolution29
I0215 10:12:16.050783 12693 net.cpp:106] Creating Layer Convolution29
I0215 10:12:16.050791 12693 net.cpp:454] Convolution29 <- BatchNorm28
I0215 10:12:16.050798 12693 net.cpp:411] Convolution29 -> Convolution29
I0215 10:12:16.051380 12693 net.cpp:150] Setting up Convolution29
I0215 10:12:16.051394 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.051400 12693 net.cpp:165] Memory required for data: 4062181564
I0215 10:12:16.051409 12693 layer_factory.hpp:77] Creating layer Dropout28
I0215 10:12:16.051417 12693 net.cpp:106] Creating Layer Dropout28
I0215 10:12:16.051424 12693 net.cpp:454] Dropout28 <- Convolution29
I0215 10:12:16.051434 12693 net.cpp:411] Dropout28 -> Dropout28
I0215 10:12:16.051502 12693 net.cpp:150] Setting up Dropout28
I0215 10:12:16.051517 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.051523 12693 net.cpp:165] Memory required for data: 4062296476
I0215 10:12:16.051528 12693 layer_factory.hpp:77] Creating layer Concat25
I0215 10:12:16.051537 12693 net.cpp:106] Creating Layer Concat25
I0215 10:12:16.051544 12693 net.cpp:454] Concat25 <- Pooling3_Pooling3_0_split_1
I0215 10:12:16.051550 12693 net.cpp:454] Concat25 <- Dropout28
I0215 10:12:16.051559 12693 net.cpp:411] Concat25 -> Concat25
I0215 10:12:16.051618 12693 net.cpp:150] Setting up Concat25
I0215 10:12:16.051632 12693 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0215 10:12:16.051638 12693 net.cpp:165] Memory required for data: 4065322492
I0215 10:12:16.051645 12693 layer_factory.hpp:77] Creating layer Concat25_Concat25_0_split
I0215 10:12:16.051653 12693 net.cpp:106] Creating Layer Concat25_Concat25_0_split
I0215 10:12:16.051661 12693 net.cpp:454] Concat25_Concat25_0_split <- Concat25
I0215 10:12:16.051669 12693 net.cpp:411] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_0
I0215 10:12:16.051679 12693 net.cpp:411] Concat25_Concat25_0_split -> Concat25_Concat25_0_split_1
I0215 10:12:16.051749 12693 net.cpp:150] Setting up Concat25_Concat25_0_split
I0215 10:12:16.051764 12693 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0215 10:12:16.051772 12693 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0215 10:12:16.051779 12693 net.cpp:165] Memory required for data: 4071374524
I0215 10:12:16.051784 12693 layer_factory.hpp:77] Creating layer BatchNorm29
I0215 10:12:16.051792 12693 net.cpp:106] Creating Layer BatchNorm29
I0215 10:12:16.051800 12693 net.cpp:454] BatchNorm29 <- Concat25_Concat25_0_split_0
I0215 10:12:16.051808 12693 net.cpp:411] BatchNorm29 -> BatchNorm29
I0215 10:12:16.052192 12693 net.cpp:150] Setting up BatchNorm29
I0215 10:12:16.052206 12693 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0215 10:12:16.052213 12693 net.cpp:165] Memory required for data: 4074400540
I0215 10:12:16.052223 12693 layer_factory.hpp:77] Creating layer Scale29
I0215 10:12:16.052235 12693 net.cpp:106] Creating Layer Scale29
I0215 10:12:16.052243 12693 net.cpp:454] Scale29 <- BatchNorm29
I0215 10:12:16.052254 12693 net.cpp:397] Scale29 -> BatchNorm29 (in-place)
I0215 10:12:16.052332 12693 layer_factory.hpp:77] Creating layer Scale29
I0215 10:12:16.052557 12693 net.cpp:150] Setting up Scale29
I0215 10:12:16.052572 12693 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0215 10:12:16.052577 12693 net.cpp:165] Memory required for data: 4077426556
I0215 10:12:16.052587 12693 layer_factory.hpp:77] Creating layer ReLU29
I0215 10:12:16.052595 12693 net.cpp:106] Creating Layer ReLU29
I0215 10:12:16.052603 12693 net.cpp:454] ReLU29 <- BatchNorm29
I0215 10:12:16.052610 12693 net.cpp:397] ReLU29 -> BatchNorm29 (in-place)
I0215 10:12:16.052620 12693 net.cpp:150] Setting up ReLU29
I0215 10:12:16.052628 12693 net.cpp:157] Top shape: 1 316 38 63 (756504)
I0215 10:12:16.052634 12693 net.cpp:165] Memory required for data: 4080452572
I0215 10:12:16.052639 12693 layer_factory.hpp:77] Creating layer Convolution30
I0215 10:12:16.052654 12693 net.cpp:106] Creating Layer Convolution30
I0215 10:12:16.052661 12693 net.cpp:454] Convolution30 <- BatchNorm29
I0215 10:12:16.052670 12693 net.cpp:411] Convolution30 -> Convolution30
I0215 10:12:16.053269 12693 net.cpp:150] Setting up Convolution30
I0215 10:12:16.053283 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.053290 12693 net.cpp:165] Memory required for data: 4080567484
I0215 10:12:16.053297 12693 layer_factory.hpp:77] Creating layer Dropout29
I0215 10:12:16.053308 12693 net.cpp:106] Creating Layer Dropout29
I0215 10:12:16.053314 12693 net.cpp:454] Dropout29 <- Convolution30
I0215 10:12:16.053323 12693 net.cpp:411] Dropout29 -> Dropout29
I0215 10:12:16.053397 12693 net.cpp:150] Setting up Dropout29
I0215 10:12:16.053412 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.053419 12693 net.cpp:165] Memory required for data: 4080682396
I0215 10:12:16.053424 12693 layer_factory.hpp:77] Creating layer Concat26
I0215 10:12:16.053433 12693 net.cpp:106] Creating Layer Concat26
I0215 10:12:16.053441 12693 net.cpp:454] Concat26 <- Concat25_Concat25_0_split_1
I0215 10:12:16.053448 12693 net.cpp:454] Concat26 <- Dropout29
I0215 10:12:16.053457 12693 net.cpp:411] Concat26 -> Concat26
I0215 10:12:16.053503 12693 net.cpp:150] Setting up Concat26
I0215 10:12:16.053514 12693 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0215 10:12:16.053521 12693 net.cpp:165] Memory required for data: 4083823324
I0215 10:12:16.053526 12693 layer_factory.hpp:77] Creating layer Concat26_Concat26_0_split
I0215 10:12:16.053535 12693 net.cpp:106] Creating Layer Concat26_Concat26_0_split
I0215 10:12:16.053541 12693 net.cpp:454] Concat26_Concat26_0_split <- Concat26
I0215 10:12:16.053550 12693 net.cpp:411] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_0
I0215 10:12:16.053560 12693 net.cpp:411] Concat26_Concat26_0_split -> Concat26_Concat26_0_split_1
I0215 10:12:16.053627 12693 net.cpp:150] Setting up Concat26_Concat26_0_split
I0215 10:12:16.053639 12693 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0215 10:12:16.053647 12693 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0215 10:12:16.053653 12693 net.cpp:165] Memory required for data: 4090105180
I0215 10:12:16.053658 12693 layer_factory.hpp:77] Creating layer BatchNorm30
I0215 10:12:16.053668 12693 net.cpp:106] Creating Layer BatchNorm30
I0215 10:12:16.053674 12693 net.cpp:454] BatchNorm30 <- Concat26_Concat26_0_split_0
I0215 10:12:16.053683 12693 net.cpp:411] BatchNorm30 -> BatchNorm30
I0215 10:12:16.054235 12693 net.cpp:150] Setting up BatchNorm30
I0215 10:12:16.054270 12693 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0215 10:12:16.054276 12693 net.cpp:165] Memory required for data: 4093246108
I0215 10:12:16.054288 12693 layer_factory.hpp:77] Creating layer Scale30
I0215 10:12:16.054299 12693 net.cpp:106] Creating Layer Scale30
I0215 10:12:16.054307 12693 net.cpp:454] Scale30 <- BatchNorm30
I0215 10:12:16.054317 12693 net.cpp:397] Scale30 -> BatchNorm30 (in-place)
I0215 10:12:16.054400 12693 layer_factory.hpp:77] Creating layer Scale30
I0215 10:12:16.054610 12693 net.cpp:150] Setting up Scale30
I0215 10:12:16.054622 12693 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0215 10:12:16.054628 12693 net.cpp:165] Memory required for data: 4096387036
I0215 10:12:16.054636 12693 layer_factory.hpp:77] Creating layer ReLU30
I0215 10:12:16.054644 12693 net.cpp:106] Creating Layer ReLU30
I0215 10:12:16.054651 12693 net.cpp:454] ReLU30 <- BatchNorm30
I0215 10:12:16.054658 12693 net.cpp:397] ReLU30 -> BatchNorm30 (in-place)
I0215 10:12:16.054667 12693 net.cpp:150] Setting up ReLU30
I0215 10:12:16.054674 12693 net.cpp:157] Top shape: 1 328 38 63 (785232)
I0215 10:12:16.054680 12693 net.cpp:165] Memory required for data: 4099527964
I0215 10:12:16.054684 12693 layer_factory.hpp:77] Creating layer Convolution31
I0215 10:12:16.054697 12693 net.cpp:106] Creating Layer Convolution31
I0215 10:12:16.054704 12693 net.cpp:454] Convolution31 <- BatchNorm30
I0215 10:12:16.054713 12693 net.cpp:411] Convolution31 -> Convolution31
I0215 10:12:16.055290 12693 net.cpp:150] Setting up Convolution31
I0215 10:12:16.055304 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.055308 12693 net.cpp:165] Memory required for data: 4099642876
I0215 10:12:16.055315 12693 layer_factory.hpp:77] Creating layer Dropout30
I0215 10:12:16.055325 12693 net.cpp:106] Creating Layer Dropout30
I0215 10:12:16.055330 12693 net.cpp:454] Dropout30 <- Convolution31
I0215 10:12:16.055342 12693 net.cpp:411] Dropout30 -> Dropout30
I0215 10:12:16.055410 12693 net.cpp:150] Setting up Dropout30
I0215 10:12:16.055423 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.055428 12693 net.cpp:165] Memory required for data: 4099757788
I0215 10:12:16.055433 12693 layer_factory.hpp:77] Creating layer Concat27
I0215 10:12:16.055441 12693 net.cpp:106] Creating Layer Concat27
I0215 10:12:16.055449 12693 net.cpp:454] Concat27 <- Concat26_Concat26_0_split_1
I0215 10:12:16.055455 12693 net.cpp:454] Concat27 <- Dropout30
I0215 10:12:16.055464 12693 net.cpp:411] Concat27 -> Concat27
I0215 10:12:16.055506 12693 net.cpp:150] Setting up Concat27
I0215 10:12:16.055517 12693 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0215 10:12:16.055522 12693 net.cpp:165] Memory required for data: 4103013628
I0215 10:12:16.055527 12693 layer_factory.hpp:77] Creating layer Concat27_Concat27_0_split
I0215 10:12:16.055536 12693 net.cpp:106] Creating Layer Concat27_Concat27_0_split
I0215 10:12:16.055541 12693 net.cpp:454] Concat27_Concat27_0_split <- Concat27
I0215 10:12:16.055550 12693 net.cpp:411] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_0
I0215 10:12:16.055563 12693 net.cpp:411] Concat27_Concat27_0_split -> Concat27_Concat27_0_split_1
I0215 10:12:16.055649 12693 net.cpp:150] Setting up Concat27_Concat27_0_split
I0215 10:12:16.055662 12693 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0215 10:12:16.055670 12693 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0215 10:12:16.055675 12693 net.cpp:165] Memory required for data: 4109525308
I0215 10:12:16.055680 12693 layer_factory.hpp:77] Creating layer BatchNorm31
I0215 10:12:16.055688 12693 net.cpp:106] Creating Layer BatchNorm31
I0215 10:12:16.055696 12693 net.cpp:454] BatchNorm31 <- Concat27_Concat27_0_split_0
I0215 10:12:16.055704 12693 net.cpp:411] BatchNorm31 -> BatchNorm31
I0215 10:12:16.056124 12693 net.cpp:150] Setting up BatchNorm31
I0215 10:12:16.056138 12693 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0215 10:12:16.056143 12693 net.cpp:165] Memory required for data: 4112781148
I0215 10:12:16.056154 12693 layer_factory.hpp:77] Creating layer Scale31
I0215 10:12:16.056165 12693 net.cpp:106] Creating Layer Scale31
I0215 10:12:16.056172 12693 net.cpp:454] Scale31 <- BatchNorm31
I0215 10:12:16.056180 12693 net.cpp:397] Scale31 -> BatchNorm31 (in-place)
I0215 10:12:16.056253 12693 layer_factory.hpp:77] Creating layer Scale31
I0215 10:12:16.056455 12693 net.cpp:150] Setting up Scale31
I0215 10:12:16.056469 12693 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0215 10:12:16.056475 12693 net.cpp:165] Memory required for data: 4116036988
I0215 10:12:16.056483 12693 layer_factory.hpp:77] Creating layer ReLU31
I0215 10:12:16.056490 12693 net.cpp:106] Creating Layer ReLU31
I0215 10:12:16.056496 12693 net.cpp:454] ReLU31 <- BatchNorm31
I0215 10:12:16.056502 12693 net.cpp:397] ReLU31 -> BatchNorm31 (in-place)
I0215 10:12:16.056511 12693 net.cpp:150] Setting up ReLU31
I0215 10:12:16.056519 12693 net.cpp:157] Top shape: 1 340 38 63 (813960)
I0215 10:12:16.056524 12693 net.cpp:165] Memory required for data: 4119292828
I0215 10:12:16.056529 12693 layer_factory.hpp:77] Creating layer Convolution32
I0215 10:12:16.056540 12693 net.cpp:106] Creating Layer Convolution32
I0215 10:12:16.056547 12693 net.cpp:454] Convolution32 <- BatchNorm31
I0215 10:12:16.056555 12693 net.cpp:411] Convolution32 -> Convolution32
I0215 10:12:16.057134 12693 net.cpp:150] Setting up Convolution32
I0215 10:12:16.057147 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.057152 12693 net.cpp:165] Memory required for data: 4119407740
I0215 10:12:16.057159 12693 layer_factory.hpp:77] Creating layer Dropout31
I0215 10:12:16.057168 12693 net.cpp:106] Creating Layer Dropout31
I0215 10:12:16.057174 12693 net.cpp:454] Dropout31 <- Convolution32
I0215 10:12:16.057184 12693 net.cpp:411] Dropout31 -> Dropout31
I0215 10:12:16.057248 12693 net.cpp:150] Setting up Dropout31
I0215 10:12:16.057260 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.057265 12693 net.cpp:165] Memory required for data: 4119522652
I0215 10:12:16.057271 12693 layer_factory.hpp:77] Creating layer Concat28
I0215 10:12:16.057278 12693 net.cpp:106] Creating Layer Concat28
I0215 10:12:16.057284 12693 net.cpp:454] Concat28 <- Concat27_Concat27_0_split_1
I0215 10:12:16.057291 12693 net.cpp:454] Concat28 <- Dropout31
I0215 10:12:16.057298 12693 net.cpp:411] Concat28 -> Concat28
I0215 10:12:16.057341 12693 net.cpp:150] Setting up Concat28
I0215 10:12:16.057353 12693 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0215 10:12:16.057358 12693 net.cpp:165] Memory required for data: 4122893404
I0215 10:12:16.057363 12693 layer_factory.hpp:77] Creating layer Concat28_Concat28_0_split
I0215 10:12:16.057368 12693 net.cpp:106] Creating Layer Concat28_Concat28_0_split
I0215 10:12:16.057374 12693 net.cpp:454] Concat28_Concat28_0_split <- Concat28
I0215 10:12:16.057381 12693 net.cpp:411] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_0
I0215 10:12:16.057389 12693 net.cpp:411] Concat28_Concat28_0_split -> Concat28_Concat28_0_split_1
I0215 10:12:16.057448 12693 net.cpp:150] Setting up Concat28_Concat28_0_split
I0215 10:12:16.057459 12693 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0215 10:12:16.057466 12693 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0215 10:12:16.057471 12693 net.cpp:165] Memory required for data: 4129634908
I0215 10:12:16.057476 12693 layer_factory.hpp:77] Creating layer BatchNorm32
I0215 10:12:16.057484 12693 net.cpp:106] Creating Layer BatchNorm32
I0215 10:12:16.057490 12693 net.cpp:454] BatchNorm32 <- Concat28_Concat28_0_split_0
I0215 10:12:16.057499 12693 net.cpp:411] BatchNorm32 -> BatchNorm32
I0215 10:12:16.057838 12693 net.cpp:150] Setting up BatchNorm32
I0215 10:12:16.057850 12693 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0215 10:12:16.057857 12693 net.cpp:165] Memory required for data: 4133005660
I0215 10:12:16.057867 12693 layer_factory.hpp:77] Creating layer Scale32
I0215 10:12:16.057878 12693 net.cpp:106] Creating Layer Scale32
I0215 10:12:16.057883 12693 net.cpp:454] Scale32 <- BatchNorm32
I0215 10:12:16.057891 12693 net.cpp:397] Scale32 -> BatchNorm32 (in-place)
I0215 10:12:16.057960 12693 layer_factory.hpp:77] Creating layer Scale32
I0215 10:12:16.058161 12693 net.cpp:150] Setting up Scale32
I0215 10:12:16.058173 12693 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0215 10:12:16.058179 12693 net.cpp:165] Memory required for data: 4136376412
I0215 10:12:16.058187 12693 layer_factory.hpp:77] Creating layer ReLU32
I0215 10:12:16.058195 12693 net.cpp:106] Creating Layer ReLU32
I0215 10:12:16.058202 12693 net.cpp:454] ReLU32 <- BatchNorm32
I0215 10:12:16.058209 12693 net.cpp:397] ReLU32 -> BatchNorm32 (in-place)
I0215 10:12:16.058217 12693 net.cpp:150] Setting up ReLU32
I0215 10:12:16.058221 12693 net.cpp:157] Top shape: 1 352 38 63 (842688)
I0215 10:12:16.058228 12693 net.cpp:165] Memory required for data: 4139747164
I0215 10:12:16.058233 12693 layer_factory.hpp:77] Creating layer Convolution33
I0215 10:12:16.058244 12693 net.cpp:106] Creating Layer Convolution33
I0215 10:12:16.058250 12693 net.cpp:454] Convolution33 <- BatchNorm32
I0215 10:12:16.058259 12693 net.cpp:411] Convolution33 -> Convolution33
I0215 10:12:16.059444 12693 net.cpp:150] Setting up Convolution33
I0215 10:12:16.059459 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.059465 12693 net.cpp:165] Memory required for data: 4139862076
I0215 10:12:16.059473 12693 layer_factory.hpp:77] Creating layer Dropout32
I0215 10:12:16.059481 12693 net.cpp:106] Creating Layer Dropout32
I0215 10:12:16.059487 12693 net.cpp:454] Dropout32 <- Convolution33
I0215 10:12:16.059495 12693 net.cpp:411] Dropout32 -> Dropout32
I0215 10:12:16.059561 12693 net.cpp:150] Setting up Dropout32
I0215 10:12:16.059571 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.059577 12693 net.cpp:165] Memory required for data: 4139976988
I0215 10:12:16.059582 12693 layer_factory.hpp:77] Creating layer Concat29
I0215 10:12:16.059597 12693 net.cpp:106] Creating Layer Concat29
I0215 10:12:16.059604 12693 net.cpp:454] Concat29 <- Concat28_Concat28_0_split_1
I0215 10:12:16.059610 12693 net.cpp:454] Concat29 <- Dropout32
I0215 10:12:16.059618 12693 net.cpp:411] Concat29 -> Concat29
I0215 10:12:16.059660 12693 net.cpp:150] Setting up Concat29
I0215 10:12:16.059674 12693 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0215 10:12:16.059680 12693 net.cpp:165] Memory required for data: 4143462652
I0215 10:12:16.059685 12693 layer_factory.hpp:77] Creating layer Concat29_Concat29_0_split
I0215 10:12:16.059692 12693 net.cpp:106] Creating Layer Concat29_Concat29_0_split
I0215 10:12:16.059700 12693 net.cpp:454] Concat29_Concat29_0_split <- Concat29
I0215 10:12:16.059706 12693 net.cpp:411] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_0
I0215 10:12:16.059715 12693 net.cpp:411] Concat29_Concat29_0_split -> Concat29_Concat29_0_split_1
I0215 10:12:16.059775 12693 net.cpp:150] Setting up Concat29_Concat29_0_split
I0215 10:12:16.059788 12693 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0215 10:12:16.059795 12693 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0215 10:12:16.059801 12693 net.cpp:165] Memory required for data: 4150433980
I0215 10:12:16.059805 12693 layer_factory.hpp:77] Creating layer BatchNorm33
I0215 10:12:16.059813 12693 net.cpp:106] Creating Layer BatchNorm33
I0215 10:12:16.059820 12693 net.cpp:454] BatchNorm33 <- Concat29_Concat29_0_split_0
I0215 10:12:16.059830 12693 net.cpp:411] BatchNorm33 -> BatchNorm33
I0215 10:12:16.060214 12693 net.cpp:150] Setting up BatchNorm33
I0215 10:12:16.060226 12693 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0215 10:12:16.060231 12693 net.cpp:165] Memory required for data: 4153919644
I0215 10:12:16.060241 12693 layer_factory.hpp:77] Creating layer Scale33
I0215 10:12:16.060252 12693 net.cpp:106] Creating Layer Scale33
I0215 10:12:16.060259 12693 net.cpp:454] Scale33 <- BatchNorm33
I0215 10:12:16.060269 12693 net.cpp:397] Scale33 -> BatchNorm33 (in-place)
I0215 10:12:16.060338 12693 layer_factory.hpp:77] Creating layer Scale33
I0215 10:12:16.060544 12693 net.cpp:150] Setting up Scale33
I0215 10:12:16.060556 12693 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0215 10:12:16.060561 12693 net.cpp:165] Memory required for data: 4157405308
I0215 10:12:16.060570 12693 layer_factory.hpp:77] Creating layer ReLU33
I0215 10:12:16.060577 12693 net.cpp:106] Creating Layer ReLU33
I0215 10:12:16.060585 12693 net.cpp:454] ReLU33 <- BatchNorm33
I0215 10:12:16.060591 12693 net.cpp:397] ReLU33 -> BatchNorm33 (in-place)
I0215 10:12:16.060600 12693 net.cpp:150] Setting up ReLU33
I0215 10:12:16.060606 12693 net.cpp:157] Top shape: 1 364 38 63 (871416)
I0215 10:12:16.060611 12693 net.cpp:165] Memory required for data: 4160890972
I0215 10:12:16.060616 12693 layer_factory.hpp:77] Creating layer Convolution34
I0215 10:12:16.060628 12693 net.cpp:106] Creating Layer Convolution34
I0215 10:12:16.060634 12693 net.cpp:454] Convolution34 <- BatchNorm33
I0215 10:12:16.060643 12693 net.cpp:411] Convolution34 -> Convolution34
I0215 10:12:16.061197 12693 net.cpp:150] Setting up Convolution34
I0215 10:12:16.061210 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.061215 12693 net.cpp:165] Memory required for data: 4161005884
I0215 10:12:16.061223 12693 layer_factory.hpp:77] Creating layer Dropout33
I0215 10:12:16.061233 12693 net.cpp:106] Creating Layer Dropout33
I0215 10:12:16.061239 12693 net.cpp:454] Dropout33 <- Convolution34
I0215 10:12:16.061247 12693 net.cpp:411] Dropout33 -> Dropout33
I0215 10:12:16.061311 12693 net.cpp:150] Setting up Dropout33
I0215 10:12:16.061322 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.061327 12693 net.cpp:165] Memory required for data: 4161120796
I0215 10:12:16.061332 12693 layer_factory.hpp:77] Creating layer Concat30
I0215 10:12:16.061341 12693 net.cpp:106] Creating Layer Concat30
I0215 10:12:16.061347 12693 net.cpp:454] Concat30 <- Concat29_Concat29_0_split_1
I0215 10:12:16.061354 12693 net.cpp:454] Concat30 <- Dropout33
I0215 10:12:16.061363 12693 net.cpp:411] Concat30 -> Concat30
I0215 10:12:16.061403 12693 net.cpp:150] Setting up Concat30
I0215 10:12:16.061414 12693 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0215 10:12:16.061419 12693 net.cpp:165] Memory required for data: 4164721372
I0215 10:12:16.061424 12693 layer_factory.hpp:77] Creating layer Concat30_Concat30_0_split
I0215 10:12:16.061434 12693 net.cpp:106] Creating Layer Concat30_Concat30_0_split
I0215 10:12:16.061440 12693 net.cpp:454] Concat30_Concat30_0_split <- Concat30
I0215 10:12:16.061447 12693 net.cpp:411] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_0
I0215 10:12:16.061455 12693 net.cpp:411] Concat30_Concat30_0_split -> Concat30_Concat30_0_split_1
I0215 10:12:16.061513 12693 net.cpp:150] Setting up Concat30_Concat30_0_split
I0215 10:12:16.061524 12693 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0215 10:12:16.061532 12693 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0215 10:12:16.061537 12693 net.cpp:165] Memory required for data: 4171922524
I0215 10:12:16.061542 12693 layer_factory.hpp:77] Creating layer BatchNorm34
I0215 10:12:16.061553 12693 net.cpp:106] Creating Layer BatchNorm34
I0215 10:12:16.061558 12693 net.cpp:454] BatchNorm34 <- Concat30_Concat30_0_split_0
I0215 10:12:16.061566 12693 net.cpp:411] BatchNorm34 -> BatchNorm34
I0215 10:12:16.061909 12693 net.cpp:150] Setting up BatchNorm34
I0215 10:12:16.061921 12693 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0215 10:12:16.061928 12693 net.cpp:165] Memory required for data: 4175523100
I0215 10:12:16.061936 12693 layer_factory.hpp:77] Creating layer Scale34
I0215 10:12:16.061947 12693 net.cpp:106] Creating Layer Scale34
I0215 10:12:16.061954 12693 net.cpp:454] Scale34 <- BatchNorm34
I0215 10:12:16.061962 12693 net.cpp:397] Scale34 -> BatchNorm34 (in-place)
I0215 10:12:16.062034 12693 layer_factory.hpp:77] Creating layer Scale34
I0215 10:12:16.062237 12693 net.cpp:150] Setting up Scale34
I0215 10:12:16.062249 12693 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0215 10:12:16.062255 12693 net.cpp:165] Memory required for data: 4179123676
I0215 10:12:16.062263 12693 layer_factory.hpp:77] Creating layer ReLU34
I0215 10:12:16.062271 12693 net.cpp:106] Creating Layer ReLU34
I0215 10:12:16.062278 12693 net.cpp:454] ReLU34 <- BatchNorm34
I0215 10:12:16.062284 12693 net.cpp:397] ReLU34 -> BatchNorm34 (in-place)
I0215 10:12:16.062292 12693 net.cpp:150] Setting up ReLU34
I0215 10:12:16.062300 12693 net.cpp:157] Top shape: 1 376 38 63 (900144)
I0215 10:12:16.062305 12693 net.cpp:165] Memory required for data: 4182724252
I0215 10:12:16.062309 12693 layer_factory.hpp:77] Creating layer Convolution35
I0215 10:12:16.062319 12693 net.cpp:106] Creating Layer Convolution35
I0215 10:12:16.062325 12693 net.cpp:454] Convolution35 <- BatchNorm34
I0215 10:12:16.062335 12693 net.cpp:411] Convolution35 -> Convolution35
I0215 10:12:16.062909 12693 net.cpp:150] Setting up Convolution35
I0215 10:12:16.062922 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.062928 12693 net.cpp:165] Memory required for data: 4182839164
I0215 10:12:16.062935 12693 layer_factory.hpp:77] Creating layer Dropout34
I0215 10:12:16.062945 12693 net.cpp:106] Creating Layer Dropout34
I0215 10:12:16.062952 12693 net.cpp:454] Dropout34 <- Convolution35
I0215 10:12:16.062960 12693 net.cpp:411] Dropout34 -> Dropout34
I0215 10:12:16.063024 12693 net.cpp:150] Setting up Dropout34
I0215 10:12:16.063035 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.063040 12693 net.cpp:165] Memory required for data: 4182954076
I0215 10:12:16.063045 12693 layer_factory.hpp:77] Creating layer Concat31
I0215 10:12:16.063052 12693 net.cpp:106] Creating Layer Concat31
I0215 10:12:16.063061 12693 net.cpp:454] Concat31 <- Concat30_Concat30_0_split_1
I0215 10:12:16.063068 12693 net.cpp:454] Concat31 <- Dropout34
I0215 10:12:16.063076 12693 net.cpp:411] Concat31 -> Concat31
I0215 10:12:16.063115 12693 net.cpp:150] Setting up Concat31
I0215 10:12:16.063127 12693 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0215 10:12:16.063132 12693 net.cpp:165] Memory required for data: 4186669564
I0215 10:12:16.063136 12693 layer_factory.hpp:77] Creating layer Concat31_Concat31_0_split
I0215 10:12:16.063145 12693 net.cpp:106] Creating Layer Concat31_Concat31_0_split
I0215 10:12:16.063151 12693 net.cpp:454] Concat31_Concat31_0_split <- Concat31
I0215 10:12:16.063159 12693 net.cpp:411] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_0
I0215 10:12:16.063168 12693 net.cpp:411] Concat31_Concat31_0_split -> Concat31_Concat31_0_split_1
I0215 10:12:16.063226 12693 net.cpp:150] Setting up Concat31_Concat31_0_split
I0215 10:12:16.063237 12693 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0215 10:12:16.063244 12693 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0215 10:12:16.063249 12693 net.cpp:165] Memory required for data: 4194100540
I0215 10:12:16.063254 12693 layer_factory.hpp:77] Creating layer BatchNorm35
I0215 10:12:16.063264 12693 net.cpp:106] Creating Layer BatchNorm35
I0215 10:12:16.063271 12693 net.cpp:454] BatchNorm35 <- Concat31_Concat31_0_split_0
I0215 10:12:16.063278 12693 net.cpp:411] BatchNorm35 -> BatchNorm35
I0215 10:12:16.063635 12693 net.cpp:150] Setting up BatchNorm35
I0215 10:12:16.063647 12693 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0215 10:12:16.063653 12693 net.cpp:165] Memory required for data: 4197816028
I0215 10:12:16.063663 12693 layer_factory.hpp:77] Creating layer Scale35
I0215 10:12:16.063674 12693 net.cpp:106] Creating Layer Scale35
I0215 10:12:16.063681 12693 net.cpp:454] Scale35 <- BatchNorm35
I0215 10:12:16.063688 12693 net.cpp:397] Scale35 -> BatchNorm35 (in-place)
I0215 10:12:16.063760 12693 layer_factory.hpp:77] Creating layer Scale35
I0215 10:12:16.063964 12693 net.cpp:150] Setting up Scale35
I0215 10:12:16.063978 12693 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0215 10:12:16.063984 12693 net.cpp:165] Memory required for data: 4201531516
I0215 10:12:16.063992 12693 layer_factory.hpp:77] Creating layer ReLU35
I0215 10:12:16.064000 12693 net.cpp:106] Creating Layer ReLU35
I0215 10:12:16.064007 12693 net.cpp:454] ReLU35 <- BatchNorm35
I0215 10:12:16.064014 12693 net.cpp:397] ReLU35 -> BatchNorm35 (in-place)
I0215 10:12:16.064023 12693 net.cpp:150] Setting up ReLU35
I0215 10:12:16.064029 12693 net.cpp:157] Top shape: 1 388 38 63 (928872)
I0215 10:12:16.064035 12693 net.cpp:165] Memory required for data: 4205247004
I0215 10:12:16.064039 12693 layer_factory.hpp:77] Creating layer Convolution36
I0215 10:12:16.064049 12693 net.cpp:106] Creating Layer Convolution36
I0215 10:12:16.064055 12693 net.cpp:454] Convolution36 <- BatchNorm35
I0215 10:12:16.064065 12693 net.cpp:411] Convolution36 -> Convolution36
I0215 10:12:16.064651 12693 net.cpp:150] Setting up Convolution36
I0215 10:12:16.064664 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.064671 12693 net.cpp:165] Memory required for data: 4205361916
I0215 10:12:16.064677 12693 layer_factory.hpp:77] Creating layer Dropout35
I0215 10:12:16.064684 12693 net.cpp:106] Creating Layer Dropout35
I0215 10:12:16.064690 12693 net.cpp:454] Dropout35 <- Convolution36
I0215 10:12:16.064698 12693 net.cpp:411] Dropout35 -> Dropout35
I0215 10:12:16.064761 12693 net.cpp:150] Setting up Dropout35
I0215 10:12:16.064772 12693 net.cpp:157] Top shape: 1 12 38 63 (28728)
I0215 10:12:16.064779 12693 net.cpp:165] Memory required for data: 4205476828
I0215 10:12:16.064784 12693 layer_factory.hpp:77] Creating layer Concat32
I0215 10:12:16.064793 12693 net.cpp:106] Creating Layer Concat32
I0215 10:12:16.064800 12693 net.cpp:454] Concat32 <- Concat31_Concat31_0_split_1
I0215 10:12:16.064806 12693 net.cpp:454] Concat32 <- Dropout35
I0215 10:12:16.064813 12693 net.cpp:411] Concat32 -> Concat32
I0215 10:12:16.064852 12693 net.cpp:150] Setting up Concat32
I0215 10:12:16.064865 12693 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0215 10:12:16.064872 12693 net.cpp:165] Memory required for data: 4209307228
I0215 10:12:16.064877 12693 layer_factory.hpp:77] Creating layer BatchNorm36
I0215 10:12:16.064884 12693 net.cpp:106] Creating Layer BatchNorm36
I0215 10:12:16.064890 12693 net.cpp:454] BatchNorm36 <- Concat32
I0215 10:12:16.064899 12693 net.cpp:411] BatchNorm36 -> BatchNorm36
I0215 10:12:16.065243 12693 net.cpp:150] Setting up BatchNorm36
I0215 10:12:16.065254 12693 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0215 10:12:16.065260 12693 net.cpp:165] Memory required for data: 4213137628
I0215 10:12:16.065269 12693 layer_factory.hpp:77] Creating layer Scale36
I0215 10:12:16.065281 12693 net.cpp:106] Creating Layer Scale36
I0215 10:12:16.065289 12693 net.cpp:454] Scale36 <- BatchNorm36
I0215 10:12:16.065297 12693 net.cpp:397] Scale36 -> BatchNorm36 (in-place)
I0215 10:12:16.065366 12693 layer_factory.hpp:77] Creating layer Scale36
I0215 10:12:16.065567 12693 net.cpp:150] Setting up Scale36
I0215 10:12:16.065580 12693 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0215 10:12:16.065585 12693 net.cpp:165] Memory required for data: 4216968028
I0215 10:12:16.065593 12693 layer_factory.hpp:77] Creating layer ReLU36
I0215 10:12:16.065601 12693 net.cpp:106] Creating Layer ReLU36
I0215 10:12:16.065608 12693 net.cpp:454] ReLU36 <- BatchNorm36
I0215 10:12:16.065615 12693 net.cpp:397] ReLU36 -> BatchNorm36 (in-place)
I0215 10:12:16.065623 12693 net.cpp:150] Setting up ReLU36
I0215 10:12:16.065630 12693 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0215 10:12:16.065635 12693 net.cpp:165] Memory required for data: 4220798428
I0215 10:12:16.065640 12693 layer_factory.hpp:77] Creating layer Convolution37
I0215 10:12:16.065654 12693 net.cpp:106] Creating Layer Convolution37
I0215 10:12:16.065659 12693 net.cpp:454] Convolution37 <- BatchNorm36
I0215 10:12:16.065668 12693 net.cpp:411] Convolution37 -> Convolution37
I0215 10:12:16.067703 12693 net.cpp:150] Setting up Convolution37
I0215 10:12:16.067718 12693 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0215 10:12:16.067724 12693 net.cpp:165] Memory required for data: 4224628828
I0215 10:12:16.067731 12693 layer_factory.hpp:77] Creating layer Dropout36
I0215 10:12:16.067742 12693 net.cpp:106] Creating Layer Dropout36
I0215 10:12:16.067749 12693 net.cpp:454] Dropout36 <- Convolution37
I0215 10:12:16.067757 12693 net.cpp:411] Dropout36 -> Dropout36
I0215 10:12:16.067821 12693 net.cpp:150] Setting up Dropout36
I0215 10:12:16.067833 12693 net.cpp:157] Top shape: 1 400 38 63 (957600)
I0215 10:12:16.067839 12693 net.cpp:165] Memory required for data: 4228459228
I0215 10:12:16.067844 12693 layer_factory.hpp:77] Creating layer Pooling4
I0215 10:12:16.067862 12693 net.cpp:106] Creating Layer Pooling4
I0215 10:12:16.067868 12693 net.cpp:454] Pooling4 <- Dropout36
I0215 10:12:16.067878 12693 net.cpp:411] Pooling4 -> Pooling4
I0215 10:12:16.067921 12693 net.cpp:150] Setting up Pooling4
I0215 10:12:16.067932 12693 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0215 10:12:16.067939 12693 net.cpp:165] Memory required for data: 4229432028
I0215 10:12:16.067944 12693 layer_factory.hpp:77] Creating layer BatchNorm37
I0215 10:12:16.067953 12693 net.cpp:106] Creating Layer BatchNorm37
I0215 10:12:16.067960 12693 net.cpp:454] BatchNorm37 <- Pooling4
I0215 10:12:16.067967 12693 net.cpp:411] BatchNorm37 -> BatchNorm37
I0215 10:12:16.068308 12693 net.cpp:150] Setting up BatchNorm37
I0215 10:12:16.068320 12693 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0215 10:12:16.068325 12693 net.cpp:165] Memory required for data: 4230404828
I0215 10:12:16.068336 12693 layer_factory.hpp:77] Creating layer Scale37
I0215 10:12:16.068346 12693 net.cpp:106] Creating Layer Scale37
I0215 10:12:16.068353 12693 net.cpp:454] Scale37 <- BatchNorm37
I0215 10:12:16.068362 12693 net.cpp:397] Scale37 -> BatchNorm37 (in-place)
I0215 10:12:16.068433 12693 layer_factory.hpp:77] Creating layer Scale37
I0215 10:12:16.068632 12693 net.cpp:150] Setting up Scale37
I0215 10:12:16.068645 12693 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0215 10:12:16.068650 12693 net.cpp:165] Memory required for data: 4231377628
I0215 10:12:16.068658 12693 layer_factory.hpp:77] Creating layer ReLU37
I0215 10:12:16.068666 12693 net.cpp:106] Creating Layer ReLU37
I0215 10:12:16.068673 12693 net.cpp:454] ReLU37 <- BatchNorm37
I0215 10:12:16.068681 12693 net.cpp:397] ReLU37 -> BatchNorm37 (in-place)
I0215 10:12:16.068688 12693 net.cpp:150] Setting up ReLU37
I0215 10:12:16.068696 12693 net.cpp:157] Top shape: 1 400 19 32 (243200)
I0215 10:12:16.068701 12693 net.cpp:165] Memory required for data: 4232350428
I0215 10:12:16.068706 12693 layer_factory.hpp:77] Creating layer rpn_conv/3x3
I0215 10:12:16.068718 12693 net.cpp:106] Creating Layer rpn_conv/3x3
I0215 10:12:16.068724 12693 net.cpp:454] rpn_conv/3x3 <- BatchNorm37
I0215 10:12:16.068733 12693 net.cpp:411] rpn_conv/3x3 -> rpn/output
I0215 10:12:16.125646 12693 net.cpp:150] Setting up rpn_conv/3x3
I0215 10:12:16.125679 12693 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0215 10:12:16.125684 12693 net.cpp:165] Memory required for data: 4233595612
I0215 10:12:16.125694 12693 layer_factory.hpp:77] Creating layer rpn_relu/3x3
I0215 10:12:16.125705 12693 net.cpp:106] Creating Layer rpn_relu/3x3
I0215 10:12:16.125710 12693 net.cpp:454] rpn_relu/3x3 <- rpn/output
I0215 10:12:16.125718 12693 net.cpp:397] rpn_relu/3x3 -> rpn/output (in-place)
I0215 10:12:16.125727 12693 net.cpp:150] Setting up rpn_relu/3x3
I0215 10:12:16.125735 12693 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0215 10:12:16.125741 12693 net.cpp:165] Memory required for data: 4234840796
I0215 10:12:16.125744 12693 layer_factory.hpp:77] Creating layer rpn/output_rpn_relu/3x3_0_split
I0215 10:12:16.125790 12693 net.cpp:106] Creating Layer rpn/output_rpn_relu/3x3_0_split
I0215 10:12:16.125797 12693 net.cpp:454] rpn/output_rpn_relu/3x3_0_split <- rpn/output
I0215 10:12:16.125807 12693 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_0
I0215 10:12:16.125815 12693 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_1
I0215 10:12:16.125882 12693 net.cpp:150] Setting up rpn/output_rpn_relu/3x3_0_split
I0215 10:12:16.125893 12693 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0215 10:12:16.125900 12693 net.cpp:157] Top shape: 1 512 19 32 (311296)
I0215 10:12:16.125905 12693 net.cpp:165] Memory required for data: 4237331164
I0215 10:12:16.125911 12693 layer_factory.hpp:77] Creating layer rpn_cls_score
I0215 10:12:16.125926 12693 net.cpp:106] Creating Layer rpn_cls_score
I0215 10:12:16.125932 12693 net.cpp:454] rpn_cls_score <- rpn/output_rpn_relu/3x3_0_split_0
I0215 10:12:16.125942 12693 net.cpp:411] rpn_cls_score -> rpn_cls_score
I0215 10:12:16.127120 12693 net.cpp:150] Setting up rpn_cls_score
I0215 10:12:16.127135 12693 net.cpp:157] Top shape: 1 50 19 32 (30400)
I0215 10:12:16.127140 12693 net.cpp:165] Memory required for data: 4237452764
I0215 10:12:16.127147 12693 layer_factory.hpp:77] Creating layer rpn_cls_score_rpn_cls_score_0_split
I0215 10:12:16.127156 12693 net.cpp:106] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I0215 10:12:16.127162 12693 net.cpp:454] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I0215 10:12:16.127169 12693 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I0215 10:12:16.127179 12693 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I0215 10:12:16.127249 12693 net.cpp:150] Setting up rpn_cls_score_rpn_cls_score_0_split
I0215 10:12:16.127261 12693 net.cpp:157] Top shape: 1 50 19 32 (30400)
I0215 10:12:16.127269 12693 net.cpp:157] Top shape: 1 50 19 32 (30400)
I0215 10:12:16.127274 12693 net.cpp:165] Memory required for data: 4237695964
I0215 10:12:16.127279 12693 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I0215 10:12:16.127291 12693 net.cpp:106] Creating Layer rpn_bbox_pred
I0215 10:12:16.127298 12693 net.cpp:454] rpn_bbox_pred <- rpn/output_rpn_relu/3x3_0_split_1
I0215 10:12:16.127307 12693 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I0215 10:12:16.129202 12693 net.cpp:150] Setting up rpn_bbox_pred
I0215 10:12:16.129216 12693 net.cpp:157] Top shape: 1 100 19 32 (60800)
I0215 10:12:16.129221 12693 net.cpp:165] Memory required for data: 4237939164
I0215 10:12:16.129230 12693 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I0215 10:12:16.129242 12693 net.cpp:106] Creating Layer rpn_cls_score_reshape
I0215 10:12:16.129250 12693 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I0215 10:12:16.129257 12693 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0215 10:12:16.129309 12693 net.cpp:150] Setting up rpn_cls_score_reshape
I0215 10:12:16.129320 12693 net.cpp:157] Top shape: 1 2 475 32 (30400)
I0215 10:12:16.129326 12693 net.cpp:165] Memory required for data: 4238060764
I0215 10:12:16.129331 12693 layer_factory.hpp:77] Creating layer rpn-data
I0215 10:12:16.130020 12693 net.cpp:106] Creating Layer rpn-data
I0215 10:12:16.130034 12693 net.cpp:454] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I0215 10:12:16.130043 12693 net.cpp:454] rpn-data <- gt_boxes
I0215 10:12:16.130049 12693 net.cpp:454] rpn-data <- im_info
I0215 10:12:16.130056 12693 net.cpp:454] rpn-data <- data_input-data_0_split_1
I0215 10:12:16.130064 12693 net.cpp:411] rpn-data -> rpn_labels
I0215 10:12:16.130074 12693 net.cpp:411] rpn-data -> rpn_bbox_targets
I0215 10:12:16.130084 12693 net.cpp:411] rpn-data -> rpn_bbox_inside_weights
I0215 10:12:16.130095 12693 net.cpp:411] rpn-data -> rpn_bbox_outside_weights
I0215 10:12:16.134621 12693 net.cpp:150] Setting up rpn-data
I0215 10:12:16.134639 12693 net.cpp:157] Top shape: 1 1 475 32 (15200)
I0215 10:12:16.134646 12693 net.cpp:157] Top shape: 1 100 19 32 (60800)
I0215 10:12:16.134654 12693 net.cpp:157] Top shape: 1 100 19 32 (60800)
I0215 10:12:16.134660 12693 net.cpp:157] Top shape: 1 100 19 32 (60800)
I0215 10:12:16.134665 12693 net.cpp:165] Memory required for data: 4238851164
I0215 10:12:16.134670 12693 layer_factory.hpp:77] Creating layer rpn_loss_cls
I0215 10:12:16.134680 12693 net.cpp:106] Creating Layer rpn_loss_cls
I0215 10:12:16.134686 12693 net.cpp:454] rpn_loss_cls <- rpn_cls_score_reshape
I0215 10:12:16.134694 12693 net.cpp:454] rpn_loss_cls <- rpn_labels
I0215 10:12:16.134704 12693 net.cpp:411] rpn_loss_cls -> rpn_cls_loss
I0215 10:12:16.134717 12693 layer_factory.hpp:77] Creating layer rpn_loss_cls
I0215 10:12:16.136569 12693 net.cpp:150] Setting up rpn_loss_cls
I0215 10:12:16.136587 12693 net.cpp:157] Top shape: (1)
I0215 10:12:16.136595 12693 net.cpp:160]     with loss weight 1
I0215 10:12:16.136606 12693 net.cpp:165] Memory required for data: 4238851168
I0215 10:12:16.136611 12693 layer_factory.hpp:77] Creating layer rpn_loss_bbox
I0215 10:12:16.136622 12693 net.cpp:106] Creating Layer rpn_loss_bbox
I0215 10:12:16.136629 12693 net.cpp:454] rpn_loss_bbox <- rpn_bbox_pred
I0215 10:12:16.136636 12693 net.cpp:454] rpn_loss_bbox <- rpn_bbox_targets
I0215 10:12:16.136643 12693 net.cpp:454] rpn_loss_bbox <- rpn_bbox_inside_weights
I0215 10:12:16.136651 12693 net.cpp:454] rpn_loss_bbox <- rpn_bbox_outside_weights
I0215 10:12:16.136660 12693 net.cpp:411] rpn_loss_bbox -> rpn_loss_bbox
I0215 10:12:16.137171 12693 net.cpp:150] Setting up rpn_loss_bbox
I0215 10:12:16.137183 12693 net.cpp:157] Top shape: (1)
I0215 10:12:16.137189 12693 net.cpp:160]     with loss weight 1
I0215 10:12:16.137197 12693 net.cpp:165] Memory required for data: 4238851172
I0215 10:12:16.137203 12693 net.cpp:226] rpn_loss_bbox needs backward computation.
I0215 10:12:16.137209 12693 net.cpp:226] rpn_loss_cls needs backward computation.
I0215 10:12:16.137214 12693 net.cpp:226] rpn-data needs backward computation.
I0215 10:12:16.137222 12693 net.cpp:226] rpn_cls_score_reshape needs backward computation.
I0215 10:12:16.137228 12693 net.cpp:226] rpn_bbox_pred needs backward computation.
I0215 10:12:16.137233 12693 net.cpp:226] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I0215 10:12:16.137238 12693 net.cpp:226] rpn_cls_score needs backward computation.
I0215 10:12:16.137243 12693 net.cpp:226] rpn/output_rpn_relu/3x3_0_split needs backward computation.
I0215 10:12:16.137248 12693 net.cpp:226] rpn_relu/3x3 needs backward computation.
I0215 10:12:16.137254 12693 net.cpp:226] rpn_conv/3x3 needs backward computation.
I0215 10:12:16.137259 12693 net.cpp:226] ReLU37 needs backward computation.
I0215 10:12:16.137264 12693 net.cpp:226] Scale37 needs backward computation.
I0215 10:12:16.137269 12693 net.cpp:226] BatchNorm37 needs backward computation.
I0215 10:12:16.137275 12693 net.cpp:226] Pooling4 needs backward computation.
I0215 10:12:16.137281 12693 net.cpp:226] Dropout36 needs backward computation.
I0215 10:12:16.137287 12693 net.cpp:226] Convolution37 needs backward computation.
I0215 10:12:16.137293 12693 net.cpp:226] ReLU36 needs backward computation.
I0215 10:12:16.137298 12693 net.cpp:226] Scale36 needs backward computation.
I0215 10:12:16.137303 12693 net.cpp:226] BatchNorm36 needs backward computation.
I0215 10:12:16.137308 12693 net.cpp:226] Concat32 needs backward computation.
I0215 10:12:16.137315 12693 net.cpp:226] Dropout35 needs backward computation.
I0215 10:12:16.137331 12693 net.cpp:226] Convolution36 needs backward computation.
I0215 10:12:16.137336 12693 net.cpp:226] ReLU35 needs backward computation.
I0215 10:12:16.137342 12693 net.cpp:226] Scale35 needs backward computation.
I0215 10:12:16.137346 12693 net.cpp:226] BatchNorm35 needs backward computation.
I0215 10:12:16.137352 12693 net.cpp:226] Concat31_Concat31_0_split needs backward computation.
I0215 10:12:16.137357 12693 net.cpp:226] Concat31 needs backward computation.
I0215 10:12:16.137363 12693 net.cpp:226] Dropout34 needs backward computation.
I0215 10:12:16.137369 12693 net.cpp:226] Convolution35 needs backward computation.
I0215 10:12:16.137375 12693 net.cpp:226] ReLU34 needs backward computation.
I0215 10:12:16.137382 12693 net.cpp:226] Scale34 needs backward computation.
I0215 10:12:16.137385 12693 net.cpp:226] BatchNorm34 needs backward computation.
I0215 10:12:16.137392 12693 net.cpp:226] Concat30_Concat30_0_split needs backward computation.
I0215 10:12:16.137395 12693 net.cpp:226] Concat30 needs backward computation.
I0215 10:12:16.137401 12693 net.cpp:226] Dropout33 needs backward computation.
I0215 10:12:16.137408 12693 net.cpp:226] Convolution34 needs backward computation.
I0215 10:12:16.137413 12693 net.cpp:226] ReLU33 needs backward computation.
I0215 10:12:16.137418 12693 net.cpp:226] Scale33 needs backward computation.
I0215 10:12:16.137421 12693 net.cpp:226] BatchNorm33 needs backward computation.
I0215 10:12:16.137426 12693 net.cpp:226] Concat29_Concat29_0_split needs backward computation.
I0215 10:12:16.137431 12693 net.cpp:226] Concat29 needs backward computation.
I0215 10:12:16.137436 12693 net.cpp:226] Dropout32 needs backward computation.
I0215 10:12:16.137442 12693 net.cpp:226] Convolution33 needs backward computation.
I0215 10:12:16.137447 12693 net.cpp:226] ReLU32 needs backward computation.
I0215 10:12:16.137452 12693 net.cpp:226] Scale32 needs backward computation.
I0215 10:12:16.137456 12693 net.cpp:226] BatchNorm32 needs backward computation.
I0215 10:12:16.137462 12693 net.cpp:226] Concat28_Concat28_0_split needs backward computation.
I0215 10:12:16.137468 12693 net.cpp:226] Concat28 needs backward computation.
I0215 10:12:16.137473 12693 net.cpp:226] Dropout31 needs backward computation.
I0215 10:12:16.137477 12693 net.cpp:226] Convolution32 needs backward computation.
I0215 10:12:16.137481 12693 net.cpp:226] ReLU31 needs backward computation.
I0215 10:12:16.137487 12693 net.cpp:226] Scale31 needs backward computation.
I0215 10:12:16.137491 12693 net.cpp:226] BatchNorm31 needs backward computation.
I0215 10:12:16.137497 12693 net.cpp:226] Concat27_Concat27_0_split needs backward computation.
I0215 10:12:16.137502 12693 net.cpp:226] Concat27 needs backward computation.
I0215 10:12:16.137508 12693 net.cpp:226] Dropout30 needs backward computation.
I0215 10:12:16.137513 12693 net.cpp:226] Convolution31 needs backward computation.
I0215 10:12:16.137519 12693 net.cpp:226] ReLU30 needs backward computation.
I0215 10:12:16.137524 12693 net.cpp:226] Scale30 needs backward computation.
I0215 10:12:16.137529 12693 net.cpp:226] BatchNorm30 needs backward computation.
I0215 10:12:16.137534 12693 net.cpp:226] Concat26_Concat26_0_split needs backward computation.
I0215 10:12:16.137540 12693 net.cpp:226] Concat26 needs backward computation.
I0215 10:12:16.137548 12693 net.cpp:226] Dropout29 needs backward computation.
I0215 10:12:16.137555 12693 net.cpp:226] Convolution30 needs backward computation.
I0215 10:12:16.137560 12693 net.cpp:226] ReLU29 needs backward computation.
I0215 10:12:16.137565 12693 net.cpp:226] Scale29 needs backward computation.
I0215 10:12:16.137569 12693 net.cpp:226] BatchNorm29 needs backward computation.
I0215 10:12:16.137576 12693 net.cpp:226] Concat25_Concat25_0_split needs backward computation.
I0215 10:12:16.137583 12693 net.cpp:226] Concat25 needs backward computation.
I0215 10:12:16.137588 12693 net.cpp:226] Dropout28 needs backward computation.
I0215 10:12:16.137593 12693 net.cpp:226] Convolution29 needs backward computation.
I0215 10:12:16.137598 12693 net.cpp:226] ReLU28 needs backward computation.
I0215 10:12:16.137603 12693 net.cpp:226] Scale28 needs backward computation.
I0215 10:12:16.137608 12693 net.cpp:226] BatchNorm28 needs backward computation.
I0215 10:12:16.137614 12693 net.cpp:226] Pooling3_Pooling3_0_split needs backward computation.
I0215 10:12:16.137619 12693 net.cpp:226] Pooling3 needs backward computation.
I0215 10:12:16.137625 12693 net.cpp:226] Dropout27 needs backward computation.
I0215 10:12:16.137631 12693 net.cpp:226] Convolution28 needs backward computation.
I0215 10:12:16.137637 12693 net.cpp:226] ReLU27 needs backward computation.
I0215 10:12:16.137642 12693 net.cpp:226] Scale27 needs backward computation.
I0215 10:12:16.137647 12693 net.cpp:226] BatchNorm27 needs backward computation.
I0215 10:12:16.137651 12693 net.cpp:226] Concat24 needs backward computation.
I0215 10:12:16.137657 12693 net.cpp:226] Dropout26 needs backward computation.
I0215 10:12:16.137665 12693 net.cpp:226] Convolution27 needs backward computation.
I0215 10:12:16.137670 12693 net.cpp:226] ReLU26 needs backward computation.
I0215 10:12:16.137675 12693 net.cpp:226] Scale26 needs backward computation.
I0215 10:12:16.137678 12693 net.cpp:226] BatchNorm26 needs backward computation.
I0215 10:12:16.137684 12693 net.cpp:226] Concat23_Concat23_0_split needs backward computation.
I0215 10:12:16.137691 12693 net.cpp:226] Concat23 needs backward computation.
I0215 10:12:16.137696 12693 net.cpp:226] Dropout25 needs backward computation.
I0215 10:12:16.137702 12693 net.cpp:226] Convolution26 needs backward computation.
I0215 10:12:16.137708 12693 net.cpp:226] ReLU25 needs backward computation.
I0215 10:12:16.137713 12693 net.cpp:226] Scale25 needs backward computation.
I0215 10:12:16.137718 12693 net.cpp:226] BatchNorm25 needs backward computation.
I0215 10:12:16.137723 12693 net.cpp:226] Concat22_Concat22_0_split needs backward computation.
I0215 10:12:16.137728 12693 net.cpp:226] Concat22 needs backward computation.
I0215 10:12:16.137733 12693 net.cpp:226] Dropout24 needs backward computation.
I0215 10:12:16.137739 12693 net.cpp:226] Convolution25 needs backward computation.
I0215 10:12:16.137744 12693 net.cpp:226] ReLU24 needs backward computation.
I0215 10:12:16.137748 12693 net.cpp:226] Scale24 needs backward computation.
I0215 10:12:16.137753 12693 net.cpp:226] BatchNorm24 needs backward computation.
I0215 10:12:16.137758 12693 net.cpp:226] Concat21_Concat21_0_split needs backward computation.
I0215 10:12:16.137763 12693 net.cpp:226] Concat21 needs backward computation.
I0215 10:12:16.137768 12693 net.cpp:226] Dropout23 needs backward computation.
I0215 10:12:16.137773 12693 net.cpp:226] Convolution24 needs backward computation.
I0215 10:12:16.137778 12693 net.cpp:226] ReLU23 needs backward computation.
I0215 10:12:16.137784 12693 net.cpp:226] Scale23 needs backward computation.
I0215 10:12:16.137789 12693 net.cpp:226] BatchNorm23 needs backward computation.
I0215 10:12:16.137794 12693 net.cpp:226] Concat20_Concat20_0_split needs backward computation.
I0215 10:12:16.137800 12693 net.cpp:226] Concat20 needs backward computation.
I0215 10:12:16.137807 12693 net.cpp:226] Dropout22 needs backward computation.
I0215 10:12:16.137812 12693 net.cpp:226] Convolution23 needs backward computation.
I0215 10:12:16.137817 12693 net.cpp:226] ReLU22 needs backward computation.
I0215 10:12:16.137822 12693 net.cpp:226] Scale22 needs backward computation.
I0215 10:12:16.137827 12693 net.cpp:226] BatchNorm22 needs backward computation.
I0215 10:12:16.137833 12693 net.cpp:226] Concat19_Concat19_0_split needs backward computation.
I0215 10:12:16.137840 12693 net.cpp:226] Concat19 needs backward computation.
I0215 10:12:16.137845 12693 net.cpp:226] Dropout21 needs backward computation.
I0215 10:12:16.137850 12693 net.cpp:226] Convolution22 needs backward computation.
I0215 10:12:16.137854 12693 net.cpp:226] ReLU21 needs backward computation.
I0215 10:12:16.137861 12693 net.cpp:226] Scale21 needs backward computation.
I0215 10:12:16.137864 12693 net.cpp:226] BatchNorm21 needs backward computation.
I0215 10:12:16.137871 12693 net.cpp:226] Concat18_Concat18_0_split needs backward computation.
I0215 10:12:16.137877 12693 net.cpp:226] Concat18 needs backward computation.
I0215 10:12:16.137884 12693 net.cpp:226] Dropout20 needs backward computation.
I0215 10:12:16.137890 12693 net.cpp:226] Convolution21 needs backward computation.
I0215 10:12:16.137895 12693 net.cpp:226] ReLU20 needs backward computation.
I0215 10:12:16.137900 12693 net.cpp:226] Scale20 needs backward computation.
I0215 10:12:16.137904 12693 net.cpp:226] BatchNorm20 needs backward computation.
I0215 10:12:16.137910 12693 net.cpp:226] Concat17_Concat17_0_split needs backward computation.
I0215 10:12:16.137917 12693 net.cpp:226] Concat17 needs backward computation.
I0215 10:12:16.137923 12693 net.cpp:226] Dropout19 needs backward computation.
I0215 10:12:16.137929 12693 net.cpp:226] Convolution20 needs backward computation.
I0215 10:12:16.137934 12693 net.cpp:226] ReLU19 needs backward computation.
I0215 10:12:16.137940 12693 net.cpp:226] Scale19 needs backward computation.
I0215 10:12:16.137945 12693 net.cpp:226] BatchNorm19 needs backward computation.
I0215 10:12:16.137950 12693 net.cpp:226] Pooling2_Pooling2_0_split needs backward computation.
I0215 10:12:16.137955 12693 net.cpp:226] Pooling2 needs backward computation.
I0215 10:12:16.137961 12693 net.cpp:226] Dropout18 needs backward computation.
I0215 10:12:16.137967 12693 net.cpp:226] Convolution19 needs backward computation.
I0215 10:12:16.137974 12693 net.cpp:226] ReLU18 needs backward computation.
I0215 10:12:16.137979 12693 net.cpp:226] Scale18 needs backward computation.
I0215 10:12:16.137984 12693 net.cpp:226] BatchNorm18 needs backward computation.
I0215 10:12:16.137989 12693 net.cpp:226] Concat16 needs backward computation.
I0215 10:12:16.137995 12693 net.cpp:226] Dropout17 needs backward computation.
I0215 10:12:16.138001 12693 net.cpp:226] Convolution18 needs backward computation.
I0215 10:12:16.138015 12693 net.cpp:226] ReLU17 needs backward computation.
I0215 10:12:16.138020 12693 net.cpp:226] Scale17 needs backward computation.
I0215 10:12:16.138025 12693 net.cpp:226] BatchNorm17 needs backward computation.
I0215 10:12:16.138029 12693 net.cpp:226] Concat15_Concat15_0_split needs backward computation.
I0215 10:12:16.138034 12693 net.cpp:226] Concat15 needs backward computation.
I0215 10:12:16.138039 12693 net.cpp:226] Dropout16 needs backward computation.
I0215 10:12:16.138046 12693 net.cpp:226] Convolution17 needs backward computation.
I0215 10:12:16.138049 12693 net.cpp:226] ReLU16 needs backward computation.
I0215 10:12:16.138054 12693 net.cpp:226] Scale16 needs backward computation.
I0215 10:12:16.138059 12693 net.cpp:226] BatchNorm16 needs backward computation.
I0215 10:12:16.138065 12693 net.cpp:226] Concat14_Concat14_0_split needs backward computation.
I0215 10:12:16.138070 12693 net.cpp:226] Concat14 needs backward computation.
I0215 10:12:16.138077 12693 net.cpp:226] Dropout15 needs backward computation.
I0215 10:12:16.138083 12693 net.cpp:226] Convolution16 needs backward computation.
I0215 10:12:16.138089 12693 net.cpp:226] ReLU15 needs backward computation.
I0215 10:12:16.138095 12693 net.cpp:226] Scale15 needs backward computation.
I0215 10:12:16.138099 12693 net.cpp:226] BatchNorm15 needs backward computation.
I0215 10:12:16.138105 12693 net.cpp:226] Concat13_Concat13_0_split needs backward computation.
I0215 10:12:16.138110 12693 net.cpp:226] Concat13 needs backward computation.
I0215 10:12:16.138118 12693 net.cpp:226] Dropout14 needs backward computation.
I0215 10:12:16.138124 12693 net.cpp:226] Convolution15 needs backward computation.
I0215 10:12:16.138130 12693 net.cpp:226] ReLU14 needs backward computation.
I0215 10:12:16.138136 12693 net.cpp:226] Scale14 needs backward computation.
I0215 10:12:16.138141 12693 net.cpp:226] BatchNorm14 needs backward computation.
I0215 10:12:16.138146 12693 net.cpp:226] Concat12_Concat12_0_split needs backward computation.
I0215 10:12:16.138151 12693 net.cpp:226] Concat12 needs backward computation.
I0215 10:12:16.138157 12693 net.cpp:226] Dropout13 needs backward computation.
I0215 10:12:16.138162 12693 net.cpp:226] Convolution14 needs backward computation.
I0215 10:12:16.138169 12693 net.cpp:226] ReLU13 needs backward computation.
I0215 10:12:16.138172 12693 net.cpp:226] Scale13 needs backward computation.
I0215 10:12:16.138177 12693 net.cpp:226] BatchNorm13 needs backward computation.
I0215 10:12:16.138182 12693 net.cpp:226] Concat11_Concat11_0_split needs backward computation.
I0215 10:12:16.138188 12693 net.cpp:226] Concat11 needs backward computation.
I0215 10:12:16.138200 12693 net.cpp:226] Dropout12 needs backward computation.
I0215 10:12:16.138205 12693 net.cpp:226] Convolution13 needs backward computation.
I0215 10:12:16.138211 12693 net.cpp:226] ReLU12 needs backward computation.
I0215 10:12:16.138216 12693 net.cpp:226] Scale12 needs backward computation.
I0215 10:12:16.138221 12693 net.cpp:226] BatchNorm12 needs backward computation.
I0215 10:12:16.138226 12693 net.cpp:226] Concat10_Concat10_0_split needs backward computation.
I0215 10:12:16.138233 12693 net.cpp:226] Concat10 needs backward computation.
I0215 10:12:16.138240 12693 net.cpp:226] Dropout11 needs backward computation.
I0215 10:12:16.138247 12693 net.cpp:226] Convolution12 needs backward computation.
I0215 10:12:16.138252 12693 net.cpp:226] ReLU11 needs backward computation.
I0215 10:12:16.138258 12693 net.cpp:226] Scale11 needs backward computation.
I0215 10:12:16.138263 12693 net.cpp:226] BatchNorm11 needs backward computation.
I0215 10:12:16.138269 12693 net.cpp:226] Concat9_Concat9_0_split needs backward computation.
I0215 10:12:16.138274 12693 net.cpp:226] Concat9 needs backward computation.
I0215 10:12:16.138280 12693 net.cpp:226] Dropout10 needs backward computation.
I0215 10:12:16.138286 12693 net.cpp:226] Convolution11 needs backward computation.
I0215 10:12:16.138293 12693 net.cpp:226] ReLU10 needs backward computation.
I0215 10:12:16.138296 12693 net.cpp:226] Scale10 needs backward computation.
I0215 10:12:16.138303 12693 net.cpp:226] BatchNorm10 needs backward computation.
I0215 10:12:16.138308 12693 net.cpp:226] Pooling1_Pooling1_0_split needs backward computation.
I0215 10:12:16.138312 12693 net.cpp:226] Pooling1 needs backward computation.
I0215 10:12:16.138317 12693 net.cpp:226] Dropout9 needs backward computation.
I0215 10:12:16.138324 12693 net.cpp:226] Convolution10 needs backward computation.
I0215 10:12:16.138330 12693 net.cpp:226] ReLU9 needs backward computation.
I0215 10:12:16.138336 12693 net.cpp:226] Scale9 needs backward computation.
I0215 10:12:16.138341 12693 net.cpp:226] BatchNorm9 needs backward computation.
I0215 10:12:16.138346 12693 net.cpp:226] Concat8 needs backward computation.
I0215 10:12:16.138353 12693 net.cpp:226] Dropout8 needs backward computation.
I0215 10:12:16.138360 12693 net.cpp:226] Convolution9 needs backward computation.
I0215 10:12:16.138366 12693 net.cpp:226] ReLU8 needs backward computation.
I0215 10:12:16.138371 12693 net.cpp:226] Scale8 needs backward computation.
I0215 10:12:16.138376 12693 net.cpp:226] BatchNorm8 needs backward computation.
I0215 10:12:16.138382 12693 net.cpp:226] Concat7_Concat7_0_split needs backward computation.
I0215 10:12:16.138388 12693 net.cpp:226] Concat7 needs backward computation.
I0215 10:12:16.138396 12693 net.cpp:226] Dropout7 needs backward computation.
I0215 10:12:16.138402 12693 net.cpp:226] Convolution8 needs backward computation.
I0215 10:12:16.138408 12693 net.cpp:226] ReLU7 needs backward computation.
I0215 10:12:16.138414 12693 net.cpp:226] Scale7 needs backward computation.
I0215 10:12:16.138420 12693 net.cpp:226] BatchNorm7 needs backward computation.
I0215 10:12:16.138427 12693 net.cpp:226] Concat6_Concat6_0_split needs backward computation.
I0215 10:12:16.138432 12693 net.cpp:226] Concat6 needs backward computation.
I0215 10:12:16.138444 12693 net.cpp:226] Dropout6 needs backward computation.
I0215 10:12:16.138449 12693 net.cpp:226] Convolution7 needs backward computation.
I0215 10:12:16.138454 12693 net.cpp:226] ReLU6 needs backward computation.
I0215 10:12:16.138460 12693 net.cpp:226] Scale6 needs backward computation.
I0215 10:12:16.138464 12693 net.cpp:226] BatchNorm6 needs backward computation.
I0215 10:12:16.138473 12693 net.cpp:226] Concat5_Concat5_0_split needs backward computation.
I0215 10:12:16.138480 12693 net.cpp:226] Concat5 needs backward computation.
I0215 10:12:16.138486 12693 net.cpp:226] Dropout5 needs backward computation.
I0215 10:12:16.138492 12693 net.cpp:226] Convolution6 needs backward computation.
I0215 10:12:16.138497 12693 net.cpp:226] ReLU5 needs backward computation.
I0215 10:12:16.138502 12693 net.cpp:226] Scale5 needs backward computation.
I0215 10:12:16.138507 12693 net.cpp:226] BatchNorm5 needs backward computation.
I0215 10:12:16.138514 12693 net.cpp:226] Concat4_Concat4_0_split needs backward computation.
I0215 10:12:16.138519 12693 net.cpp:226] Concat4 needs backward computation.
I0215 10:12:16.138525 12693 net.cpp:226] Dropout4 needs backward computation.
I0215 10:12:16.138530 12693 net.cpp:226] Convolution5 needs backward computation.
I0215 10:12:16.138536 12693 net.cpp:226] ReLU4 needs backward computation.
I0215 10:12:16.138542 12693 net.cpp:226] Scale4 needs backward computation.
I0215 10:12:16.138546 12693 net.cpp:226] BatchNorm4 needs backward computation.
I0215 10:12:16.138552 12693 net.cpp:226] Concat3_Concat3_0_split needs backward computation.
I0215 10:12:16.138559 12693 net.cpp:226] Concat3 needs backward computation.
I0215 10:12:16.138564 12693 net.cpp:226] Dropout3 needs backward computation.
I0215 10:12:16.138571 12693 net.cpp:226] Convolution4 needs backward computation.
I0215 10:12:16.138576 12693 net.cpp:226] ReLU3 needs backward computation.
I0215 10:12:16.138581 12693 net.cpp:226] Scale3 needs backward computation.
I0215 10:12:16.138586 12693 net.cpp:226] BatchNorm3 needs backward computation.
I0215 10:12:16.138592 12693 net.cpp:226] Concat2_Concat2_0_split needs backward computation.
I0215 10:12:16.138597 12693 net.cpp:226] Concat2 needs backward computation.
I0215 10:12:16.138604 12693 net.cpp:226] Dropout2 needs backward computation.
I0215 10:12:16.138610 12693 net.cpp:226] Convolution3 needs backward computation.
I0215 10:12:16.138615 12693 net.cpp:226] ReLU2 needs backward computation.
I0215 10:12:16.138620 12693 net.cpp:226] Scale2 needs backward computation.
I0215 10:12:16.138624 12693 net.cpp:226] BatchNorm2 needs backward computation.
I0215 10:12:16.138630 12693 net.cpp:226] Concat1_Concat1_0_split needs backward computation.
I0215 10:12:16.138636 12693 net.cpp:226] Concat1 needs backward computation.
I0215 10:12:16.138643 12693 net.cpp:226] Dropout1 needs backward computation.
I0215 10:12:16.138649 12693 net.cpp:226] Convolution2 needs backward computation.
I0215 10:12:16.138653 12693 net.cpp:226] ReLU1 needs backward computation.
I0215 10:12:16.138659 12693 net.cpp:226] Scale1 needs backward computation.
I0215 10:12:16.138664 12693 net.cpp:226] BatchNorm1 needs backward computation.
I0215 10:12:16.138669 12693 net.cpp:226] Convolution1_Convolution1_0_split needs backward computation.
I0215 10:12:16.138675 12693 net.cpp:226] Convolution1 needs backward computation.
I0215 10:12:16.138681 12693 net.cpp:228] data_input-data_0_split does not need backward computation.
I0215 10:12:16.138687 12693 net.cpp:228] input-data does not need backward computation.
I0215 10:12:16.138694 12693 net.cpp:270] This network produces output rpn_cls_loss
I0215 10:12:16.138698 12693 net.cpp:270] This network produces output rpn_loss_bbox
I0215 10:12:16.138859 12693 net.cpp:283] Network initialization done.
I0215 10:12:16.139381 12693 solver.cpp:60] Solver scaffolding done.
Init model: None
Using config:
{'DATA_DIR': '/home/duchenting/dct/py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'default',
 'GPU_ID': 1,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/duchenting/dct/py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/duchenting/dct/py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': False,
          'MAX_SIZE': 500,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [300],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.1,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 500,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 128,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [300],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for training
Set proposal method: gt
Appending horizontally-flipped training examples...
voc_2007_trainval gt roidb loaded from /home/duchenting/dct/py-faster-rcnn/data/cache/voc_2007_trainval_gt_roidb.pkl
done
Preparing training data...
done
roidb len: 10022
Output will be saved to `/home/duchenting/dct/py-faster-rcnn/output/default/voc_2007_trainval`
Filtered 0 roidb entries: 10022 -> 10022
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
anchors:
[[ -26.5  -10.    41.5   25. ]
 [ -61.   -28.    76.    43. ]
 [ -95.5  -46.   110.5   61. ]
 [-176.   -88.   191.   103. ]
 [-233.5 -118.   248.5  133. ]
 [ -22.   -11.5   37.    26.5]
 [ -52.   -31.    67.    46. ]
 [ -82.   -50.5   97.    65.5]
 [-152.   -96.   167.   111. ]
 [-202.  -128.5  217.   143.5]
 [ -16.   -16.    31.    31. ]
 [ -40.   -40.    55.    55. ]
 [ -64.   -64.    79.    79. ]
 [-120.  -120.   135.   135. ]
 [-160.  -160.   175.   175. ]
 [ -11.5  -22.    26.5   37. ]
 [ -31.   -52.    46.    67. ]
 [ -50.5  -82.    65.5   97. ]
 [ -96.  -152.   111.   167. ]
 [-128.5 -202.   143.5  217. ]
 [  -8.5  -25.    23.5   40. ]
 [ -25.   -58.    40.    73. ]
 [ -41.5  -91.    56.5  106. ]
 [ -80.  -168.    95.   183. ]
 [-107.5 -223.   122.5  238. ]]
anchor shapes:
[[  68.   35.]
 [ 137.   71.]
 [ 206.  107.]
 [ 367.  191.]
 [ 482.  251.]
 [  59.   38.]
 [ 119.   77.]
 [ 179.  116.]
 [ 319.  207.]
 [ 419.  272.]
 [  47.   47.]
 [  95.   95.]
 [ 143.  143.]
 [ 255.  255.]
 [ 335.  335.]
 [  38.   59.]
 [  77.  119.]
 [ 116.  179.]
 [ 207.  319.]
 [ 272.  419.]
 [  32.   65.]
 [  65.  131.]
 [  98.  197.]
 [ 175.  351.]
 [ 230.  461.]]
AnchorTargetLayer: height 19 width 32
Solving...

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  30.39999962   48.          299.20001221  399.20001221   15.        ]
 [  35.20000076   79.19999695  291.20001221  399.20001221    2.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.01463694  0.02879135  0.10447038  0.12671I0215 10:12:17.520196 12693 solver.cpp:229] Iteration 0, loss = 0.721138
I0215 10:12:17.520264 12693 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.689044 (* 1 = 0.689044 loss)
I0215 10:12:17.520279 12693 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320941 (* 1 = 0.0320941 loss)
I0215 10:12:17.520297 12693 sgd_solver.cpp:106] Iteration 0, lr = 0.001
131]]
stdevs:
[[ 0.06413897  0.06345132  0.10724091  0.10766785]]
rpn: max max_overlap 0.774602120176
rpn: num_positive 17
rpn: num_negative 111
rpn: num_positive avg 17
rpn: num_negative avg 111

im_size: (452.0, 300.0)
scale: 0.903614461422
height, width: (29, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  53.3132515   117.46987915  299.09637451  450.90362549   14.        ]
 [ 117.46987915   55.1204834   299.09637451  396.68673706   15.        ]]
total_anchors 13775
inds_inside 4404
anchors.shape (4404, 4)
means:
[[ 0.03919197  0.02338206  0.06984138  0.09668593]]
stdevs:
[[ 0.06029734  0.06378521  0.10545394  0.11457016]]
rpn: max max_overlap 0.879571022493
rpn: num_positive 21
rpn: num_negative 107
rpn: num_positive avg 19
rpn: num_negative avg 109

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[   0.           69.59999847   84.          253.6000061     9.        ]
 [ 106.40000153   86.40000153  178.3999939   201.6000061     9.        ]
 [ 187.19999695   99.19999695  255.19999695  182.3999939    12.        ]
 [ 124.80000305  185.6000061   216.          299.20001221   12.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.037263    0.02367284  0.04159595  0.06943057]]
stdevs:
[[ 0.06522841  0.06385662  0.1312273   0.12592016]]
rpn: max max_overlap 0.793984113904
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 16
rpn: num_negative avg 112

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 209.6000061   177.6000061   230.3999939   239.19999695   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.03928547  0.02099429  0.03228587  0.06693424]]
stdevs:
[[ 0.06606248  0.0658698   0.14501617  0.12582295]]
rpn: max max_overlap 0.543166999158
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 12
rpn: num_negative avg 115

im_size: (300.0, 453.0)
scale: 0.906344413757
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 309.06344604   79.75830841  396.07250977  290.93655396   15.        ]
 [  83.38368225  160.42295837  144.10876465  278.2477417     9.        ]]
total_anchors 13775
inds_inside 4428
anchors.shape (4428, 4)
means:
[[ 0.03379537  0.02030486  0.00587803  0.06155001]]
stdevs:
[[ 0.07112451  0.06430219  0.14836072  0.11857563]]
rpn: max max_overlap 0.811069968167
rpn: num_positive 9
rpn: num_negative 119
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (259.0, 500.0)
scale: 1.0
height, width: (17, 32)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[   5.   12.  492.  258.    7.]
 [  99.    3.  139.   79.   15.]
 [  66.   13.  100.   83.   15.]]
total_anchors 13600
inds_inside 3958
anchors.shape (3958, 4)
means:
[[ 0.02332089  0.01262956  0.07666178  0.11453642]]
stdevs:
[[ 0.07762032  0.06707505  0.17531512  0.13126314]]
rpn: max max_overlap 0.741210009503
rpn: num_positive 23
rpn: num_negative 105
rpn: num_positive avg 13
rpn: num_negative avg 114

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  20.          204.          218.3999939   299.20001221   11.        ]
 [ 164.           56.          332.          299.20001221   15.        ]
 [  74.40000153   81.59999847  226.3999939   210.3999939    15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.02120211  0.01573581  0.03957828  0.09357547]]
stdevs:
[[ 0.0773178   0.06734969  0.19482314  0.13430721]]
rpn: max max_overlap 0.853315812539
rpn: num_positive 16
rpn: num_negative 112
rpn: num_positive avg 13
rpn: num_negative avg 114

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[ 232.          170.3999939   326.3999939   272.            9.        ]
 [ 303.20001221  158.3999939   376.79998779  244.            9.        ]
 [ 194.3999939   138.3999939   235.19999695  189.6000061     9.        ]
 [ 144.          156.80000305  202.3999939   221.6000061     9.        ]
 [  64.80000305  138.3999939   102.40000153  177.6000061     9.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01919065  0.01587221  0.03525609  0.08602669]]
stdevs:
[[ 0.07605802  0.06875229  0.1933231   0.13881816]]
rpn: max max_overlap 0.886331969475
rpn: num_positive 8
rpn: num_negative 120
rpn: num_positive avg 13
rpn: num_negative avg 114

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (6, 5)
rpn: gt_boxes [[   1.60000002  197.6000061   354.3999939   299.20001221   14.        ]
 [  86.40000153  135.19999695  184.80000305  298.3999939    15.        ]
 [  85.59999847  109.59999847  226.3999939   299.20001221   15.        ]
 [ 226.3999939   175.19999695  277.6000061   271.20001221   15.        ]
 [ 187.19999695  128.80000305  288.          299.20001221   15.        ]
 [ 224.          116.80000305  336.          298.3999939    15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01624282  0.02240163  0.02738598  0.06667148]]
stdevs:
[[ 0.07459002  0.07119637  0.1857841   0.1612373 ]]
rpn: max max_overlap 0.884704064936
rpn: num_positive 17
rpn: num_negative 111
rpn: num_positive avg 13
rpn: num_negative avg 114

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  76.80000305  100.80000305  312.79998779  297.6000061    12.        ]
 [  66.40000153   28.          399.20001221  241.6000061    15.        ]
 [   0.            0.          399.20001221  297.6000061    18.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.02404755  0.01513278  0.0102688   0.07688993]]
stdevs:
[[ 0.07178428  0.08236555  0.17997565  0.16030176]]
rpn: max max_overlap 0.929174402488
rpn: num_positive 34
rpn: num_negative 94
rpn: num_positive avg 15
rpn: num_negative avg 112

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 176.94610596   45.80838394  328.74252319  299.10180664   15.        ]
 [ 277.54492188   32.33533096  425.74850464  299.10180664   15.        ]
 [ 140.11976624   17.96407127  253.29341125  160.77844238    9.        ]]
total_anchors 13775
inds_inside 4396
anchors.shape (4396, 4)
means:
[[ 0.02253849  0.01455319 -0.01649756  0.07048702]]
stdevs:
[[ 0.07459405  0.0799564   0.20583786  0.15771377]]
rpn: max max_overlap 0.774474426435
rpn: num_positive 11
rpn: num_negative 117
rpn: num_positive avg 15
rpn: num_negative avg 112

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  20.79999924  140.80000305  183.19999695  273.6000061     7.        ]
 [   1.60000002  154.3999939    28.79999924  212.            7.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.0184129   0.0134979  -0.01521364  0.06653713]]
stdevs:
[[ 0.07746286  0.07889233  0.20276647  0.15543584]]
rpn: max max_overlap 0.803096113887
rpn: num_positive 11
rpn: num_negative 117
rpn: num_positive avg 14
rpn: num_negative avg 113

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  53.89221573    0.          356.58682251  299.10180664   15.        ]]
total_anchors 13775
inds_inside 4396
anchors.shape (4396, 4)
means:
[[ 0.01669229  0.01216454 -0.00346255  0.07237337]]
stdevs:
[[ 0.07729048  0.07759993  0.20140967  0.15211718]]
rpn: max max_overlap 0.719075084881
rpn: num_positive 12
rpn: num_negative 116
rpn: num_positive avg 14
rpn: num_negative avg 113

im_size: (300.0, 343.0)
scale: 0.686498880386
height, width: (19, 22)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   7.55148745  164.75971985  188.78718567  275.97253418    2.        ]
 [ 175.74371338  172.99771118  340.50341797  276.6590271     2.        ]]
total_anchors 10450
inds_inside 3001
anchors.shape (3001, 4)
means:
[[ 0.01493581  0.01166168 -0.0058987   0.0682703 ]]
stdevs:
[[ 0.07714949  0.07726815  0.19804155  0.15081188]]
rpn: max max_overlap 0.868247447544
rpn: num_positive 8
rpn: I0215 10:12:34.075147 12693 solver.cpp:229] Iteration 20, loss = 0.575227
I0215 10:12:34.075217 12693 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.571588 (* 1 = 0.571588 loss)
I0215 10:12:34.075228 12693 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00363882 (* 1 = 0.00363882 loss)
I0215 10:12:34.075237 12693 sgd_solver.cpp:106] Iteration 20, lr = 0.001
num_negative 120
rpn: num_positive avg 14
rpn: num_negative avg 113

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 119.19999695   48.          314.3999939   251.19999695   20.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01450722  0.01083196 -0.01720932  0.05547257]]
stdevs:
[[ 0.07622681  0.07641437  0.20082251  0.15924367]]
rpn: max max_overlap 0.611328716706
rpn: num_positive 9
rpn: num_negative 119
rpn: num_positive avg 13
rpn: num_negative avg 114

im_size: (300.0, 343.0)
scale: 0.686498880386
height, width: (19, 22)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 248.5125885   187.41418457  330.20596313  251.25857544    3.        ]]
total_anchors 10450
inds_inside 3001
anchors.shape (3001, 4)
means:
[[ 0.01483247  0.01101634 -0.01891673  0.0543178 ]]
stdevs:
[[ 0.0761872   0.07627661  0.20183966  0.15972684]]
rpn: max max_overlap 0.572884776393
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 13
rpn: num_negative avg 115

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 100.          108.          320.          210.3999939     6.        ]
 [ 327.20001221  164.          339.20001221  199.19999695   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01390926  0.01100989 -0.01880134  0.0409655 ]]
stdevs:
[[ 0.07755134  0.07699392  0.21731029  0.16942113]]
rpn: max max_overlap 0.813999257585
rpn: num_positive 14
rpn: num_negative 114
rpn: num_positive avg 13
rpn: num_negative avg 114

im_size: (300.0, 448.0)
scale: 0.895522415638
height, width: (19, 28)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 313.43283081  174.62686157  385.97015381  203.28358459    7.        ]
 [ 224.77612305  172.83581543  329.55224609  212.23880005    7.        ]
 [ 171.94029236  174.62686157  224.77612305  214.92536926    7.        ]]
total_anchors 13300
inds_inside 4396
anchors.shape (4396, 4)
means:
[[ 0.01336333  0.00782592 -0.01176518  0.0397961 ]]
stdevs:
[[ 0.08002132  0.08070737  0.22108628  0.16916311]]
rpn: max max_overlap 0.734000446269
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 12
rpn: num_negative avg 115

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 113.59999847   48.          259.20001221  245.6000061    12.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01238138  0.00800709 -0.00767382  0.04080541]]
stdevs:
[[ 0.08017841  0.08023281  0.22133806  0.16787151]]
rpn: max max_overlap 0.72334435798
rpn: num_positive 4
rpn: num_negative 124
rpn: num_positive avg 12
rpn: num_negative avg 115

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 159.19999695   70.40000153  324.          299.20001221   15.        ]
 [  94.40000153   88.80000305  191.19999695  213.6000061    15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01276903  0.00916542 -0.01464381  0.03774652]]
stdevs:
[[ 0.08068928  0.07951469  0.22753967  0.16676178]]
rpn: max max_overlap 0.760775227586
rpn: num_positive 7
rpn: num_negative 121
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 374.0)
scale: 0.748129665852
height, width: (19, 24)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 225.93516541   94.26433563  372.568573    227.431427     13.        ]
 [   0.           76.30922699  240.89775085  299.25186157   15.        ]]
total_anchors 11400
inds_inside 3408
anchors.shape (3408, 4)
means:
[[ 0.01349185  0.00895666 -0.01508197  0.03611623]]
stdevs:
[[ 0.08063007  0.07917587  0.225561    0.16595943]]
rpn: max max_overlap 0.865812787895
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 129.6000061   119.19999695  243.19999695  291.20001221    8.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01314077  0.00966977 -0.0144994   0.03450964]]
stdevs:
[[ 0.08038488  0.07888703  0.22396932  0.16517633]]
rpn: max max_overlap 0.892643739605
rpn: num_positive 4
rpn: num_negative 124
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 140.           15.19999981  196.80000305   53.59999847    4.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.01314824  0.00910576 -0.0145912   0.03441202]]
stdevs:
[[ 0.08022339  0.07922792  0.2235238   0.16485148]]
rpn: max max_overlap 0.744794695924
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (327.0, 300.0)
scale: 0.680272102356
height, width: (21, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 114.96598816   46.25850296  174.1496582   200.           15.        ]
 [  51.02040863   82.99319458  262.58502197  274.1496582    13.        ]]
total_anchors 9975
inds_inside 2779
anchors.shape (2779, 4)
means:
[[ 0.01295425  0.01036316 -0.02084496  0.02372873]]
stdevs:
[[ 0.07958526  0.07852024  0.22144558  0.17221879]]
rpn: max max_overlap 0.698355386592
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 110.40000153    1.60000002  272.          266.3999939    14.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01263441  0.00999585 -0.03064962  0.02404204]]
stdevs:
[[ 0.08032921  0.07766374  0.22820499  0.17027043]]
rpn: max max_overlap 0.620079289106
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 171.1711731    82.88288116  328.8288269   298.19821167    8.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.01282777  0.00965653 -0.02322768  0.02757868]]
stdevs:
[[ 0.08116403  0.07711637  0.2309923   0.17000752]]
rpn: max max_overlap 0.613634882816
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 189.18919373   87.38739014  264.86486816  172.07206726   15.        ]
 [ 149.54954529  108.10810852  304.5045166   208.10810852   14.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.01271054  0.0090914  -0.02569936  0.024536  ]]
stdevs:
[[ 0.08068661  0.07701301  0.22965438  0.16995285]]
rpn: max max_overlap 0.747932198607
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 401.0)
scale: 0.802139043808
height, width: (19, 26)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  53.74331665   61.76470566  164.43850708  230.21389771   12.        ]
 [ 143.58288574   92.24598694  247.86096191  210.96257019   12.        ]]
total_anchors 12350
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01211616  0.00826822 -0.02086403  0.02044082]]
stdevs:
[[ 0.0806938   0.07628773  0.22694735  0.16850258]]
rpn: max max_overlap 0.852647966474
rpn: num_positive 13
rpn: num_negative 115
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (11, 5)
rpn: gt_boxes [[  97.59999847  102.40000153  371.20001221  299.20001221   11.        ]
 [ 182.3999939    44.79999924  250.3999939   132.80000305   15.        ]
 [ 136.           67.19999695  176.          120.80000305   15.        ]
 [  69.59999847   56.          112.80000305   88.80000305   15.        ]
 [   8.           98.40000153  151.19999695  299.20001221   15.        ]
 [  36.79999924   72.80000305  179.19999695  228.           15.        ]
 [   3.20000005   56.79999924   40.          144.80000305   15.        ]
 [ 200.           51.20000076  284.79998779  149.6000061    15.        ]
 [ 267.20001221   64.          336.79998779  172.80000305   15.        ]
 [ 275.20001221   84.80000305  399.20001221  269.6000061    15.        ]
 [ 222.3999939   180.          345.6000061   299.20001221   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01195651  0.00937839 -0.01310895  0.01723587]]
stdevs:
[[ 0.08042247  0.07457584  0.2235654   0.16532863]]
rpn: max max_overlap 0.868105579296
rpn: num_positive 38
rpn: num_negative 90
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 375.0)
scale: 0.75
height, width: (19, 24)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 210.75  148.5   291.75  193.5     3.  ]
 [ 204.    129.75  235.5   185.25    3.  ]]
total_anchors 11400
inds_inside 3408
anchors.shape (3408, 4)
means:
[[ 0.01238025  0.00992084 -0.01305878  0.01769346]]
stdevs:
[[ 0.08038872  0.07467174  0.22331484  0.16535784]]
rpn: max max_overlap 0.658536585366
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   0.           56.          101.59999847  155.19999695   20.        ]
 [ 127.19999695   24.79999924  392.79998779  299.20001221   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01287653  0.01050727 -0.01155406  0.01907795]]
stdevs:
[[ 0.07985553  0.07413938  0.22051342  0.16336344]]
rpn: max max_overlap 0.892597865754
rpn: num_positive 9
rpn: num_negative 119
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 164.          159.19999695  325.6000061   294.3999939    12.        ]
 [  28.79999924  171.19999695  176.          263.20001221   12.        ]
 [  44.           45.59999847  352.          298.3999939    15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01191771  0.01237082 -0.00808738  0.02547717]]
stdevs:
[[ 0.07867749  0.07351346  0.21508187  0.16147462]]
rpn: max max_overlap 0.802706834393
rpn: num_positive 26
rpn: num_negative 102
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  27.20000076   98.40000153  120.80000305  144.80000305   14.        ]
 [ 262.3999939   112.          370.3999939   167.19999695   14.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01206315  0.01260622 -0.00744372  0.02519399]]
stdevs:
[[ 0.07848694  0.07338138  0.21520774  0.1626122 ]]
rpn: max max_overlap 0.654465776427
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  48.            8.          331.20001221  297.6000061    14.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01144458  0.01236724 -0.00388992  0.02841864]]
stdevs:
[[ 0.07830274  0.07278938  0.21267175  0.16099822]]
rpn: max max_overlap 0.793524279266
rpn: num_positive 12
rpn: num_negative 116
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 343.0)
scale: 0.686498880386
height, width: (19, 22)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 153.77574158  164.75971985  335.01144409  275.97253418    2.        ]
 [   2.05949664  172.99771118  166.81922913  276.6590271     2.        ]]
total_anchors 10450
inds_inside 3001
anchors.shape (3001, 4)
means:
[[ 0.01129896  0.01137524 -0.00513069  0.02722493]]
stdevs:
[[ 0.07812867  0.0729099   0.21097762  0.15966622]]
rpn: max max_overlap 0.889000076352
rpn: num_positive 8
rpn: num_negative 120
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   8.80000019    0.800000I0215 10:12:50.780627 12693 solver.cpp:229] Iteration 40, loss = 0.476247
I0215 10:12:50.780690 12693 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.425195 (* 1 = 0.425195 loss)
I0215 10:12:50.780702 12693 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0510519 (* 1 = 0.0510519 loss)
I0215 10:12:50.780714 12693 sgd_solver.cpp:106] Iteration 40, lr = 0.001
01  399.20001221  299.20001221   19.        ]
 [ 249.6000061   105.59999847  364.          299.20001221   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.01050167  0.01119073 -0.00172129  0.04165228]]
stdevs:
[[ 0.07686364  0.07786957  0.2065115   0.1733809 ]]
rpn: max max_overlap 0.855978872213
rpn: num_positive 20
rpn: num_negative 108
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  66.40000153   58.40000153  226.3999939   141.6000061     4.        ]
 [ 186.3999939    79.19999695  225.6000061   111.19999695    3.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.01085849  0.01126316 -0.00138845  0.04119606]]
stdevs:
[[ 0.07693947  0.07804564  0.20622387  0.17415104]]
rpn: max max_overlap 0.732948756337
rpn: num_positive 3
rpn: num_negative 125
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[   0.           93.59999847  164.          397.6000061     6.        ]
 [ 217.6000061   231.19999695  241.6000061   300.           15.        ]
 [ 255.19999695  229.6000061   286.3999939   321.6000061    15.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.01122917  0.01210418 -0.00302472  0.04158171]]
stdevs:
[[ 0.07809272  0.07833928  0.20553374  0.17467838]]
rpn: max max_overlap 0.781015521093
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (450.0, 300.0)
scale: 0.900900900364
height, width: (29, 19)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  25.22522545  214.41441345  206.30630493  449.54956055   13.        ]
 [  90.99098969  280.18017578  299.09909058  449.54956055   15.        ]
 [  84.68468475   72.97297668  241.44143677  370.27026367   15.        ]]
total_anchors 13775
inds_inside 4390
anchors.shape (4390, 4)
means:
[[ 0.0104767   0.01143968 -0.00768483  0.04134086]]
stdevs:
[[ 0.07725511  0.07963754  0.20498053  0.17867769]]
rpn: max max_overlap 0.759594704445
rpn: num_positive 25
rpn: num_negative 103
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (397.0, 300.0)
scale: 0.79365080595
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 103.17460632   69.84127045  264.28570557  224.60317993   12.        ]]
total_anchors 11875
inds_inside 3706
anchors.shape (3706, 4)
means:
[[ 0.01012725  0.01147674 -0.00626737  0.04175855]]
stdevs:
[[ 0.07739327  0.07939956  0.20425897  0.17771424]]
rpn: max max_overlap 0.821203839016
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  84.80000305  107.19999695  281.6000061   291.20001221   13.        ]
 [ 150.3999939    44.          208.          201.6000061    15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00952992  0.01097326  0.00015337  0.04677817]]
stdevs:
[[ 0.07917943  0.08110201  0.20938036  0.18087397]]
rpn: max max_overlap 0.739712908975
rpn: num_positive 12
rpn: num_negative 116
rpn: num_positive avg 11
rpn: num_negative avg 116

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  12.           63.20000076   89.59999847  188.80000305   16.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00887687  0.01077452  0.00094327  0.04642336]]
stdevs:
[[ 0.07914524  0.08099504  0.20878593  0.18018217]]
rpn: max max_overlap 0.802338522815
rpn: num_positive 4
rpn: num_negative 124
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (6, 5)
rpn: gt_boxes [[ 301.80178833   43.24324417  375.6756897    75.67567444    1.        ]
 [ 177.47747803   54.05405426  249.54954529   93.69369507    1.        ]
 [ 188.2882843   120.72071838  261.26126099  152.2522583     1.        ]
 [  41.44144058   73.87387085  118.01802063  107.20720673    1.        ]
 [  72.07207489  125.22522736  143.24324036  156.75675964    1.        ]
 [  77.47747803  197.29730225  146.84684753  227.92793274    1.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00818674  0.01154677  0.00232819  0.04479501]]
stdevs:
[[ 0.07936929  0.08095539  0.20737063  0.17937773]]
rpn: max max_overlap 0.834166286737
rpn: num_positive 8
rpn: num_negative 120
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 444.0)
scale: 0.887573957443
height, width: (19, 28)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   6.21301794    4.43787003  436.68640137  294.67456055    1.        ]]
total_anchors 13300
inds_inside 4325
anchors.shape (4325, 4)
means:
[[ 0.00813125  0.01146751  0.00243274  0.04487938]]
stdevs:
[[ 0.07921489  0.08079244  0.20693608  0.17900114]]
rpn: max max_overlap 0.911571162559
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[ 229.6000061   134.3999939   398.3999939   299.20001221    9.        ]
 [ 208.           36.          264.79998779   92.80000305   16.        ]
 [ 127.19999695  120.80000305  188.          177.6000061    20.        ]
 [  67.19999695  116.          128.          202.3999939    20.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00803211  0.01094008  0.00303631  0.04435579]]
stdevs:
[[ 0.07934476  0.08077847  0.20694899  0.17974742]]
rpn: max max_overlap 0.736550884411
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 241.61676025  149.10179138  273.9520874   167.96406555    7.        ]
 [ 181.43711853  150.89820862  212.87425232  167.96406555    7.        ]
 [ 141.01795959  157.18562317  193.11376953  188.62275696    7.        ]]
total_anchors 13775
inds_inside 4396
anchors.shape (4396, 4)
means:
[[ 0.00723703  0.01102453 -0.00038774  0.03579739]]
stdevs:
[[ 0.07943924  0.0820148   0.20883583  0.19975686]]
rpn: max max_overlap 0.656240125696
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 353.0)
scale: 0.705882370472
height, width: (19, 23)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 117.1764679   120.          197.64706421  212.47058105   15.        ]
 [  39.52941132   21.88235283  242.8235321   107.29412079    7.        ]]
total_anchors 10925
inds_inside 3152
anchors.shape (3152, 4)
means:
[[ 0.0069669   0.01091758 -0.00116339  0.03341989]]
stdevs:
[[ 0.07924565  0.08179912  0.20781428  0.1999289 ]]
rpn: max max_overlap 0.826291664094
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[  31.20000076   17.60000038  362.3999939   285.6000061     7.        ]
 [   0.           59.20000076   39.20000076  100.80000305    7.        ]
 [  47.20000076   30.39999962   69.59999847  104.           15.        ]
 [  32.           40.79999924   82.40000153   84.            7.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00702526  0.01038911  0.00129754  0.03764619]]
stdevs:
[[ 0.07861802  0.08241029  0.20652748  0.19902799]]
rpn: max max_overlap 0.744837279016
rpn: num_positive 18
rpn: num_negative 110
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 304.5045166    21.62162209  360.36035156   66.66666412    1.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00721408  0.01055945  0.00162895  0.03748993]]
stdevs:
[[ 0.07865522  0.08241809  0.20645833  0.19886268]]
rpn: max max_overlap 0.691880133446
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[ 176.           46.40000153  396.79998779  296.79998779   15.        ]
 [ 116.           84.80000305  208.          296.           15.        ]
 [  53.59999847   62.40000153  190.3999939   299.20001221   15.        ]
 [   1.60000002   58.40000153  138.3999939   296.79998779   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00747547  0.01063968  0.00263316  0.04087059]]
stdevs:
[[ 0.07817112  0.08224831  0.20482061  0.19824037]]
rpn: max max_overlap 0.836648375049
rpn: num_positive 14
rpn: num_negative 114
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 330.3999939   175.19999695  382.3999939   211.19999695    3.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00733743  0.01036786  0.00281794  0.04029366]]
stdevs:
[[ 0.07815974  0.08240339  0.20466779  0.1984877 ]]
rpn: max max_overlap 0.673126172289
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 452.0)
scale: 0.903614461422
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 190.66264343   83.13253021  305.42169189  248.49397278   19.        ]]
total_anchors 13775
inds_inside 4428
anchors.shape (4428, 4)
means:
[[ 0.00707924  0.00999283  0.00272598  0.03835659]]
stdevs:
[[ 0.07803325  0.08200183  0.20351911  0.19770247]]
rpn: max max_overlap 0.914427459268
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 153.6000061    4.         274.3999939  116.          20.       ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00700591  0.00978735  0.00277153  0.03960125]]
stdevs:
[[ 0.07789617  0.08215683  0.20313899  0.19837327]]
rpn: max max_overlap 0.674371126638
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 446.0)
scale: 0.892857134342
height, width: (19, 28)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[ 389.28570557   94.64286041  410.71429443  122.32142639   15.        ]
 [ 230.35714722  135.71427917  268.75        216.07142639   15.        ]
 [ 315.17855835   70.5357132   444.64285278  299.10714722   15.        ]
 [ 105.35713959   48.2142868   165.17857361  152.67857361   15.        ]
 [   0.           50.          185.71427917  298.21429443   15.        ]]
total_anchors 13300
inds_inside 4340
anchors.shape (4340, 4)
means:
[[  6.93131845e-03   1.00212754e-02  -3.11254316e-05   3.79009338e-02]]
stdevs:
[[ 0.07833459  0.0821287   0.20749132  0.20094016]]
rpn: max max_overlap 0.736282043323
rpn: num_positive 9
rpn: num_negative 119
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 452.0)
scale: 0.903614461422
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 236.74699402   36.14457703  298.19277954  206.02409363   15.        ]
 [ 190.66264343   74.09638214  351.50601196  236.74699402   13.        ]]
total_anchors 13775
inds_inside 4428
anchors.shape (4428, 4)
means:
[[ 0.0071502   0.00975323  0.0005169   0.03975702]]
stdevs:
[[ 0.07808174  0.08213277  0.20644708  0.20045738]]
rpn: max max_overlap 0.782911084176
rpn: num_positive 7
rpn: num_negative 121
rpn: num_positive avg 10
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 156.          195.19999695  275.20001221  282.3999939     2.        ]
 [ 136.          189.6000061   240.80000305  279.20001221    2.        ]
 [ 259.20001221  191.19999695  399.20001221  299.20001221    7.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, I0215 10:13:08.428647 12693 solver.cpp:229] Iteration 60, loss = 0.319914
I0215 10:13:08.428720 12693 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.303362 (* 1 = 0.303362 loss)
I0215 10:13:08.428730 12693 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016552 (* 1 = 0.016552 loss)
I0215 10:13:08.428738 12693 sgd_solver.cpp:106] Iteration 60, lr = 0.001
4)
means:
[[  6.87420855e-03   9.56474743e-03   3.29275753e-05   4.26841102e-02]]
stdevs:
[[ 0.07763759  0.08298872  0.20480641  0.20083599]]
rpn: max max_overlap 0.857288307622
rpn: num_positive 11
rpn: num_negative 117
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  25.22522545   21.62162209  411.71170044  296.39639282   12.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00680723  0.00962848 -0.00025342  0.0425684 ]]
stdevs:
[[ 0.077516    0.08284806  0.20449855  0.20048831]]
rpn: max max_overlap 0.875575189176
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 410.0)
scale: 0.819672107697
height, width: (19, 26)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  71.31147766   19.67213058  245.08197021  158.19671631    4.        ]
 [  49.18032837  185.24589539   77.04917908  243.44262695   15.        ]]
total_anchors 12350
inds_inside 3894
anchors.shape (3894, 4)
means:
[[ 0.00698039  0.00936469  0.00032068  0.04311794]]
stdevs:
[[ 0.07805173  0.08271709  0.20375741  0.19923136]]
rpn: max max_overlap 0.802722741991
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  95.49549866   52.25225067  444.14413452  299.09909058   15.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00724608  0.00976686  0.00132526  0.04659607]]
stdevs:
[[ 0.07736708  0.08260909  0.20182379  0.19843403]]
rpn: max max_overlap 0.768065201563
rpn: num_positive 13
rpn: num_negative 115
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   0.9009009    16.21621704  416.21621704  272.07208252   14.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00710043  0.00964199  0.00129078  0.04623024]]
stdevs:
[[ 0.0772836   0.08249618  0.20148113  0.19819522]]
rpn: max max_overlap 0.910783306367
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   1.60000002   31.20000076  345.6000061   256.79998779   12.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00557669  0.0092118   0.00199323  0.04838714]]
stdevs:
[[ 0.0769387   0.0824185   0.19869955  0.19555669]]
rpn: max max_overlap 0.851401341018
rpn: num_positive 19
rpn: num_negative 109
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 349.0)
scale: 0.697674393654
height, width: (19, 22)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 175.11627197   87.20930481  343.25582886  281.86047363   12.        ]
 [   0.           12.5581398   130.46511841  211.39535522   12.        ]
 [   0.            0.          348.13952637  296.5116272    15.        ]]
total_anchors 10450
inds_inside 3103
anchors.shape (3103, 4)
means:
[[ 0.00502982  0.00928513  0.00664813  0.05454261]]
stdevs:
[[ 0.07651398  0.08327462  0.19834262  0.19723393]]
rpn: max max_overlap 0.801625382995
rpn: num_positive 19
rpn: num_negative 109
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   0.            0.          351.20001221  299.20001221   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00494109  0.00966209  0.00832644  0.06041787]]
stdevs:
[[ 0.07587121  0.08447634  0.19684207  0.19992565]]
rpn: max max_overlap 0.629525375914
rpn: num_positive 12
rpn: num_negative 116
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  51.20000076    0.80000001  399.20001221  240.            7.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00571892  0.00928059  0.00874916  0.0633178 ]]
stdevs:
[[ 0.07529226  0.0842849   0.19473981  0.19840944]]
rpn: max max_overlap 0.793989700807
rpn: num_positive 16
rpn: num_negative 112
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (442.0, 300.0)
scale: 0.884955763817
height, width: (28, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   1.76991153    0.          269.91149902  441.59292603   15.        ]]
total_anchors 13300
inds_inside 4291
anchors.shape (4291, 4)
means:
[[ 0.00552661  0.00923267  0.00867909  0.06328451]]
stdevs:
[[ 0.07525779  0.08416735  0.19444685  0.19810767]]
rpn: max max_overlap 0.847957402401
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 10
rpn: num_negative avg 117

im_size: (409.0, 300.0)
scale: 0.817438721657
height, width: (26, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[   4.90463209  165.94004822   40.05449677  227.24795532   16.        ]
 [ 250.13624573  152.86103821  285.28610229  231.33514404   16.        ]]
total_anchors 12350
inds_inside 3845
anchors.shape (3845, 4)
means:
[[ 0.00560847  0.00888014  0.0088011   0.06324824]]
stdevs:
[[ 0.07526023  0.0841634   0.19408489  0.19777236]]
rpn: max max_overlap 0.846744367317
rpn: num_positive 3
rpn: num_negative 125
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (452.0, 300.0)
scale: 0.903614461422
height, width: (29, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 140.96385193  210.54217529  163.55421448  239.45782471   19.        ]]
total_anchors 13775
inds_inside 4404
anchors.shape (4404, 4)
means:
[[ 0.00566109  0.00892204  0.00775883  0.0606627 ]]
stdevs:
[[ 0.07515236  0.08430349  0.19471239  0.20297357]]
rpn: max max_overlap 0.324022504849
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 116.          76.         162.3999939  152.          16.       ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.00579585  0.0087701   0.00804176  0.06094789]]
stdevs:
[[ 0.07517545  0.08433033  0.1947012   0.20295271]]
rpn: max max_overlap 0.641131103434
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (7, 5)
rpn: gt_boxes [[ 143.19999695   72.80000305  342.3999939   129.6000061    11.        ]
 [  82.40000153  129.6000061   399.20001221  299.20001221   11.        ]
 [  62.40000153    0.          179.19999695  140.80000305   15.        ]
 [ 213.6000061    25.60000038  252.           58.40000153   15.        ]
 [ 249.6000061     8.80000019  289.6000061    85.59999847   15.        ]
 [ 288.79998779    1.60000002  340.79998779   98.40000153   15.        ]
 [ 338.3999939    28.          364.           60.           15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00633744  0.00877938  0.00693696  0.0563984 ]]
stdevs:
[[ 0.07628911  0.08412938  0.19863654  0.20478794]]
rpn: max max_overlap 0.805557470601
rpn: num_positive 13
rpn: num_negative 115
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  16.79999924   73.59999847  228.80000305  164.80000305    8.        ]
 [ 165.6000061    90.40000153  398.3999939   299.20001221   12.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00650477  0.00883823  0.00695243  0.05444793]]
stdevs:
[[ 0.07611392  0.08423118  0.19780364  0.20493567]]
rpn: max max_overlap 0.830878366036
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 128.80000305  138.3999939   205.6000061   270.3999939     9.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00642729  0.00888392  0.00737008  0.05452289]]
stdevs:
[[ 0.07584143  0.08434002  0.1972655   0.20422824]]
rpn: max max_overlap 0.895780924729
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 279.20001221  130.3999939   348.79998779  167.19999695    7.        ]
 [ 251.19999695  132.80000305  282.3999939   159.19999695    7.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00661214  0.0084696   0.00702794  0.05349722]]
stdevs:
[[ 0.07573356  0.08439958  0.19753339  0.20514997]]
rpn: max max_overlap 0.807310873084
rpn: num_positive 3
rpn: num_negative 125
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (500.0, 252.0)
scale: 1.0
height, width: (32, 16)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  48.   70.  202.  254.   15.]
 [  94.  184.  201.  465.   14.]]
total_anchors 12800
inds_inside 3790
anchors.shape (3790, 4)
means:
[[ 0.00633132  0.0079887   0.00867045  0.05641518]]
stdevs:
[[ 0.07542645  0.0846616   0.19682091  0.2056426 ]]
rpn: max max_overlap 0.723138622493
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (436.0, 300.0)
scale: 0.87209302187
height, width: (28, 19)
rpn: gt_boxes.shape (8, 5)
rpn: gt_boxes [[ 102.03488159  284.30233765  177.03488159  394.18603516   15.        ]
 [ 177.03488159  299.12789917  210.17442322  395.05813599   15.        ]
 [ 209.30232239  295.63952637  236.33720398  398.54650879   15.        ]
 [ 234.59301758  299.12789917  255.52325439  397.67440796   15.        ]
 [  70.639534    294.76745605  108.139534    396.80233765   15.        ]
 [  36.6279068   289.53488159   69.7674408   407.26745605   15.        ]
 [ 262.5         238.08139038  293.89535522  311.33721924   15.        ]
 [   5.23255825  252.03488159   24.4186039   308.72091675   15.        ]]
total_anchors 13300
inds_inside 4196
anchors.shape (4196, 4)
means:
[[ 0.00603381  0.00775292  0.00678129  0.0623209 ]]
stdevs:
[[ 0.07686253  0.0867588   0.19780027  0.21038588]]
rpn: max max_overlap 0.834386258421
rpn: num_positive 19
rpn: num_negative 109
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  33.59999847  121.59999847  170.3999939   254.3999939    16.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00592005  0.00791541  0.00643004  0.06138184]]
stdevs:
[[ 0.076831    0.08667744  0.19716006  0.20995892]]
rpn: max max_overlap 0.889160819672
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[   0.           92.79279327   30.63063049  125.22522736   16.        ]
 [ 114.41441345   82.88288116  215.31532288  170.27026367   16.        ]
 [  34.23423386  116.21621704  449.54956055  299.09909058   11.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00566447  0.00819528  0.00633147  0.05987206]]
stdevs:
[[ 0.07680637  0.08650831  0.19689141  0.20944085]]
rpn: max max_overlap 0.816303394326
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (439.0, 300.0)
scale: 0.877192974091
height, width: (28, 19)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[  42.10526276  306.14035034  298.24560547  437.71929932   12.        ]
 [ 107.01754761   67.54386139  225.43859863  357.89474487   15.        ]
 [  85.08772278   68.42105103  146.4912262   134.21052551   15.        ]
 [ 193.85964966   12.28070164  253.5087738   100.           16.        ]]
total_anchors 13300
inds_inside 4213
anchors.shape (4213, 4)
means:
[[ 0.00539334  0.00774365  0.00668448  0.06410432]]
stdevs:
[[ 0.0764684   0.08754652  0.19783865  0.21187534]]
rpn: max max_overlap 0.65576I0215 10:13:25.641969 12693 solver.cpp:229] Iteration 80, loss = 0.447859
I0215 10:13:25.642036 12693 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.346531 (* 1 = 0.346531 loss)
I0215 10:13:25.642047 12693 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.101328 (* 1 = 0.101328 loss)
I0215 10:13:25.642055 12693 sgd_solver.cpp:106] Iteration 80, lr = 0.001
6626675
rpn: num_positive 13
rpn: num_negative 115
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  33.59999847  204.          219.19999695  377.6000061    10.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.00532383  0.00782457  0.00803098  0.06479008]]
stdevs:
[[ 0.07637796  0.08741395  0.19816539  0.21151701]]
rpn: max max_overlap 0.636457038679
rpn: num_positive 4
rpn: num_negative 124
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 448.0)
scale: 0.895522415638
height, width: (19, 28)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[ 383.28356934  115.52238464  416.41790771  154.92536926    3.        ]
 [ 324.17910767  139.70149231  374.32836914  187.16418457    3.        ]
 [ 224.77612305  151.34327698  306.26864624  215.82089233    3.        ]
 [ 156.71641541  142.38806152  231.04476929  189.85075378    3.        ]
 [ 247.16418457  119.10447693  304.4776001   162.9850769     3.        ]]
total_anchors 13300
inds_inside 4396
anchors.shape (4396, 4)
means:
[[ 0.00505891  0.00779546  0.00767318  0.06411244]]
stdevs:
[[ 0.0765988   0.08734204  0.19817672  0.21171845]]
rpn: max max_overlap 0.687247438626
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 308.79998779   36.79999924  388.79998779  127.19999695   15.        ]
 [ 315.20001221   63.20000076  381.6000061   270.3999939    15.        ]
 [ 200.           61.59999847  324.          260.           15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00571497  0.00768493  0.00856387  0.06394829]]
stdevs:
[[ 0.07663367  0.08706399  0.19797058  0.21016366]]
rpn: max max_overlap 0.844934797948
rpn: num_positive 12
rpn: num_negative 116
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[ 114.40000153  148.80000305  196.          228.80000305   20.        ]
 [ 220.80000305   55.20000076  230.3999939    88.            5.        ]
 [ 231.19999695   53.59999847  243.19999695   84.80000305    5.        ]
 [ 172.          230.3999939   303.20001221  299.20001221    9.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00751386  0.00758462  0.00016835  0.05754604]]
stdevs:
[[ 0.07917686  0.08834862  0.21722116  0.21942909]]
rpn: max max_overlap 0.881517678316
rpn: num_positive 9
rpn: num_negative 119
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 455.0)
scale: 0.909090936184
height, width: (19, 29)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 237.27272034  142.72727966  300.90908813  182.72727966    2.        ]
 [ 243.6363678   122.72727203  280.          173.6363678    15.        ]
 [ 157.27272034  118.1818161   178.18182373  170.90908813   15.        ]]
total_anchors 13775
inds_inside 4445
anchors.shape (4445, 4)
means:
[[ 0.00746741  0.0071053  -0.00033714  0.0566603 ]]
stdevs:
[[ 0.07906596  0.08840887  0.21722263  0.2194611 ]]
rpn: max max_overlap 0.794472437897
rpn: num_positive 4
rpn: num_negative 124
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 397.0)
scale: 0.79365080595
height, width: (19, 25)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[ 320.63491821  178.57142639  355.55554199  222.222229     10.        ]
 [ 240.47619629  176.98413086  289.68252563  206.34921265   10.        ]
 [ 182.53968811  167.46031189  226.98413086  193.65078735   10.        ]
 [  96.82539368  146.82539368  179.36508179  197.61904907   10.        ]
 [  54.76190567  140.47619629   94.44444275  187.30158997   10.        ]]
total_anchors 11875
inds_inside 3717
anchors.shape (3717, 4)
means:
[[ 0.00759255  0.00705879 -0.0009097   0.05496985]]
stdevs:
[[ 0.07897583  0.08868053  0.21706304  0.22098367]]
rpn: max max_overlap 0.736427449622
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 136.03604126  132.43243408  162.16215515  151.35134888   20.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00728705  0.00740019 -0.00165153  0.05020576]]
stdevs:
[[ 0.0789805   0.08952007  0.21698248  0.23365486]]
rpn: max max_overlap 0.248082071315
rpn: num_positive 3
rpn: num_negative 125
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (203.0, 500.0)
scale: 1.0
height, width: (13, 32)
rpn: gt_boxes.shape (6, 5)
rpn: gt_boxes [[ 392.   44.  495.  200.   15.]
 [ 304.   27.  383.   82.   15.]
 [ 223.   10.  301.   89.   15.]
 [  78.   30.  210.  136.   15.]
 [   4.   51.  138.  202.   15.]
 [ 101.   83.  421.  202.   11.]]
total_anchors 10400
inds_inside 2591
anchors.shape (2591, 4)
means:
[[ 0.00680851  0.00792574  0.00076473  0.04978975]]
stdevs:
[[ 0.07984304  0.08901185  0.21969899  0.2323971 ]]
rpn: max max_overlap 0.856622114216
rpn: num_positive 16
rpn: num_negative 112
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 113.59999847  107.19999695  211.19999695  250.3999939    15.        ]
 [   0.80000001   13.60000038  344.          279.20001221   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00638675  0.00780621  0.00320693  0.05099435]]
stdevs:
[[ 0.07969507  0.0886107   0.21927355  0.23134025]]
rpn: max max_overlap 0.725341093952
rpn: num_positive 16
rpn: num_negative 112
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   2.4000001    0.         334.3999939  233.6000061    8.       ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00531065  0.00721729  0.00290498  0.05289381]]
stdevs:
[[ 0.07946828  0.0884394   0.21737979  0.22958462]]
rpn: max max_overlap 0.852002891733
rpn: num_positive 16
rpn: num_negative 112
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 452.0)
scale: 0.903614461422
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   1.80722892    0.90361446  450.          299.09637451   13.        ]]
total_anchors 13775
inds_inside 4428
anchors.shape (4428, 4)
means:
[[ 0.00531163  0.00718698  0.00305824  0.05298612]]
stdevs:
[[ 0.07937896  0.0883361   0.21714318  0.22931859]]
rpn: max max_overlap 0.853155311306
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  30.39999962   64.          398.3999939   299.20001221   18.        ]
 [ 144.           61.59999847  354.3999939   291.20001221   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.0055931   0.00771907  0.00367494  0.05456465]]
stdevs:
[[ 0.07879218  0.08796465  0.2154533   0.22755151]]
rpn: max max_overlap 0.808277420652
rpn: num_positive 19
rpn: num_negative 109
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 154.3999939    71.19999695  395.20001221  299.20001221   15.        ]
 [  98.40000153  266.3999939   130.3999939   299.20001221   15.        ]
 [  79.19999695  217.6000061   110.40000153  288.           15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00606432  0.00813303  0.0019139   0.05386015]]
stdevs:
[[ 0.07917836  0.0878792   0.2168855   0.22726666]]
rpn: max max_overlap 0.791346140449
rpn: num_positive 5
rpn: num_negative 123
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (450.0, 300.0)
scale: 0.900900900364
height, width: (29, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  63.96396255   27.92792702  245.94595337  325.22521973   15.        ]
 [  81.08107758  179.27928162  245.94595337  414.41442871    2.        ]]
total_anchors 13775
inds_inside 4390
anchors.shape (4390, 4)
means:
[[ 0.00597728  0.007712    0.00065329  0.05031067]]
stdevs:
[[ 0.07882089  0.08755864  0.21533931  0.22714741]]
rpn: max max_overlap 0.820057579243
rpn: num_positive 16
rpn: num_negative 112
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (452.0, 300.0)
scale: 0.903614461422
height, width: (29, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   0.90361446    9.93975925  299.09637451  450.90362549   15.        ]]
total_anchors 13775
inds_inside 4404
anchors.shape (4404, 4)
means:
[[ 0.00595124  0.0077319   0.00085979  0.05031218]]
stdevs:
[[ 0.07873327  0.0874649   0.21513831  0.22688944]]
rpn: max max_overlap 0.867109797256
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 455.0)
scale: 0.909090936184
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  69.09091187  129.09091187  260.          285.45455933    2.        ]
 [  92.72727203   33.63636398  257.2727356   255.45454407   15.        ]]
total_anchors 13775
inds_inside 4445
anchors.shape (4445, 4)
means:
[[ 0.0062934   0.00769715  0.00333862  0.05196647]]
stdevs:
[[ 0.07906402  0.08720866  0.21605508  0.22655808]]
rpn: max max_overlap 0.697362213756
rpn: num_positive 8
rpn: num_negative 120
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[ 251.19999695  136.80000305  399.20001221  299.20001221   18.        ]
 [ 131.19999695  129.6000061   200.          235.19999695    9.        ]
 [ 198.3999939   138.3999939   263.20001221  231.19999695    9.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00613513  0.00778792  0.00346851  0.04982523]]
stdevs:
[[ 0.07872451  0.08719942  0.21491312  0.22706022]]
rpn: max max_overlap 0.851699897096
rpn: num_positive 10
rpn: num_negative 118
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 449.0)
scale: 0.898203611374
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   0.            0.89820361  336.82635498  299.10180664    8.        ]]
total_anchors 13775
inds_inside 4396
anchors.shape (4396, 4)
means:
[[ 0.00600375  0.00777317  0.00322275  0.04987171]]
stdevs:
[[ 0.07877932  0.08715209  0.21492006  0.22693832]]
rpn: max max_overlap 0.723838602789
rpn: num_positive 1
rpn: num_negative 127
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   0.80000001    2.4000001   397.6000061   297.6000061    18.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00595112  0.00765288  0.00379883  0.05283276]]
stdevs:
[[ 0.078477    0.08805174  0.21418873  0.22854063]]
rpn: max max_overlap 0.599651879096
rpn: num_positive 7
rpn: num_negative 121
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (5, 5)
rpn: gt_boxes [[  96.80000305  120.          174.3999939   299.20001221   15.        ]
 [ 172.80000305  104.80000305  240.          298.3999939    15.        ]
 [  32.          127.19999695   99.19999695  299.20001221   15.        ]
 [  70.40000153  103.19999695  104.80000305  173.6000061    15.        ]
 [ 247.19999695  132.80000305  333.6000061   236.80000305   15.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00556525  0.00772473  0.00268763  0.05326822]]
stdevs:
[[ 0.07843001  0.08774769  0.21388573  0.22770892]]
rpn: max max_overlap 0.856851901989
rpn: num_positive 11
rpn: num_negative 117
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  35.20000076  124.80000305  190.3999939   263.20001221    8.        ]
 [ 248.80000305  122.40000153  383.I0215 10:13:43.064885 12693 solver.cpp:229] Iteration 100, loss = 0.207031
I0215 10:13:43.064931 12693 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.173387 (* 1 = 0.173387 loss)
I0215 10:13:43.064942 12693 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0336439 (* 1 = 0.0336439 loss)
I0215 10:13:43.064950 12693 sgd_solver.cpp:106] Iteration 100, lr = 0.001
20001221  170.3999939     8.        ]
 [   3.20000005    1.60000002  399.20001221  296.           18.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00530727  0.00756889  0.00344922  0.05810303]]
stdevs:
[[ 0.07766763  0.08936417  0.21160455  0.2304939 ]]
rpn: max max_overlap 0.849946094665
rpn: num_positive 24
rpn: num_negative 104
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (450.0, 300.0)
scale: 0.900900900364
height, width: (29, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  60.36035919   70.2702713   262.16217041  431.53152466   13.        ]
 [ 143.24324036   28.82882881  248.64865112  312.61260986   15.        ]]
total_anchors 13775
inds_inside 4390
anchors.shape (4390, 4)
means:
[[ 0.004669    0.00797945  0.0052343   0.06022644]]
stdevs:
[[ 0.07751872  0.08890418  0.20879076  0.22788188]]
rpn: max max_overlap 0.843260226203
rpn: num_positive 32
rpn: num_negative 96
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   3.20000005  150.3999939   299.20001221  399.20001221   11.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.00463324  0.00839217  0.00611178  0.0597076 ]]
stdevs:
[[ 0.07738548  0.08882113  0.20845064  0.22727937]]
rpn: max max_overlap 0.794195925268
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  81.59999847   50.40000153  273.6000061   399.20001221   13.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.00497772  0.00916738  0.00599069  0.05947264]]
stdevs:
[[ 0.07727782  0.08844503  0.20686481  0.22531689]]
rpn: max max_overlap 0.867032517145
rpn: num_positive 18
rpn: num_negative 110
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (450.0, 300.0)
scale: 0.900900900364
height, width: (29, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[ 163.96395874  202.70269775  196.39639282  238.73873901   20.        ]
 [ 140.5405426   219.81982422  185.58558655  263.06304932    9.        ]]
total_anchors 13775
inds_inside 4390
anchors.shape (4390, 4)
means:
[[ 0.00480616  0.00913159  0.00557546  0.05901289]]
stdevs:
[[ 0.07729731  0.08851625  0.2069897   0.22536071]]
rpn: max max_overlap 0.647461603563
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  64.80000305  189.6000061   287.20001221  397.6000061     9.        ]
 [ 192.80000305  220.80000305  248.80000305  277.6000061     8.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.0049399   0.00926605  0.00559989  0.05887808]]
stdevs:
[[ 0.07729726  0.088498    0.20689821  0.22532317]]
rpn: max max_overlap 0.679747865497
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (400.0, 300.0)
scale: 0.800000011921
height, width: (25, 19)
rpn: gt_boxes.shape (4, 5)
rpn: gt_boxes [[ 125.59999847  192.          265.6000061   399.20001221    2.        ]
 [ 137.6000061     6.4000001   257.6000061   303.20001221   15.        ]
 [  23.20000076   99.19999695  146.3999939   146.3999939     7.        ]
 [ 265.6000061    96.          299.20001221  116.80000305    7.        ]]
total_anchors 11875
inds_inside 3749
anchors.shape (3749, 4)
means:
[[ 0.00509321  0.00962021  0.00602583  0.05866632]]
stdevs:
[[ 0.07698419  0.08996006  0.20583484  0.23453248]]
rpn: max max_overlap 0.717395222327
rpn: num_positive 15
rpn: num_negative 113
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 112.            2.4000001   311.20001221  276.79998779   12.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00517846  0.00942622  0.00405989  0.05877856]]
stdevs:
[[ 0.07693726  0.0896775   0.20622472  0.23361886]]
rpn: max max_overlap 0.738278498443
rpn: num_positive 8
rpn: num_negative 120
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 419.0)
scale: 0.837988853455
height, width: (19, 27)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  10.05586624   74.58100891  379.60894775  279.88827515    4.        ]]
total_anchors 12825
inds_inside 3998
anchors.shape (3998, 4)
means:
[[ 0.00464383  0.00929172  0.00545613  0.05830214]]
stdevs:
[[ 0.07665537  0.0892612   0.20441992  0.23113773]]
rpn: max max_overlap 0.907988835385
rpn: num_positive 23
rpn: num_negative 105
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 136.93693542   72.07207489  379.27926636  222.52252197    8.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00468912  0.0090173   0.00634585  0.05989507]]
stdevs:
[[ 0.07649481  0.08954693  0.20417613  0.23143839]]
rpn: max max_overlap 0.606604848821
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 400.0)
scale: 0.800000011921
height, width: (19, 25)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[   0.80000001   84.80000305  399.20001221  299.20001221    7.        ]]
total_anchors 11875
inds_inside 3773
anchors.shape (3773, 4)
means:
[[ 0.00464641  0.00946348  0.00799032  0.0602427 ]]
stdevs:
[[ 0.07621625  0.08926128  0.20347925  0.22997818]]
rpn: max max_overlap 0.821287581785
rpn: num_positive 14
rpn: num_negative 114
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  89.1891861    21.62162209  145.04504395   66.66666412    1.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00455422  0.00964751  0.00808332  0.06024644]]
stdevs:
[[ 0.07617509  0.08928045  0.20335757  0.229807  ]]
rpn: max max_overlap 0.748807456459
rpn: num_positive 2
rpn: num_negative 126
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 444.0)
scale: 0.887573957443
height, width: (19, 28)
rpn: gt_boxes.shape (3, 5)
rpn: gt_boxes [[  13.31360912   94.97041321  132.2485199   229.88165283   12.        ]
 [ 157.10058594   94.97041321  363.01776123  254.73373413   12.        ]
 [ 310.65087891   76.33135986  419.82247925  236.09468079   12.        ]]
total_anchors 13300
inds_inside 4325
anchors.shape (4325, 4)
means:
[[ 0.00453653  0.00951868  0.00727787  0.05873149]]
stdevs:
[[ 0.07578754  0.08940425  0.2021182   0.23022605]]
rpn: max max_overlap 0.808934072848
rpn: num_positive 19
rpn: num_negative 109
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 450.0)
scale: 0.900900900364
height, width: (19, 29)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[ 223.42341614  132.43243408  349.54956055  299.09909058    8.        ]]
total_anchors 13775
inds_inside 4414
anchors.shape (4414, 4)
means:
[[ 0.00447014  0.00984126  0.00693431  0.05883589]]
stdevs:
[[ 0.07571589  0.08933536  0.20174697  0.22973074]]
rpn: max max_overlap 0.770959529198
rpn: num_positive 6
rpn: num_negative 122
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (300.0, 452.0)
scale: 0.903614461422
height, width: (19, 29)
rpn: gt_boxes.shape (2, 5)
rpn: gt_boxes [[  87.65060425   96.68674469  429.21685791  299.09637451   14.        ]
 [ 172.59036255   29.81927681  329.8192749   273.79516602   15.        ]]
total_anchors 13775
inds_inside 4428
anchors.shape (4428, 4)
means:
[[ 0.00461766  0.01036241  0.00414166  0.05783786]]
stdevs:
[[ 0.07571573  0.08890116  0.20332684  0.2279639 ]]
rpn: max max_overlap 0.882090980875
rpn: num_positive 19
rpn: num_negative 109
rpn: num_positive avg 9
rpn: num_negative avg 118

im_size: (397.0, 300.0)
scale: 0.79365080595
height, width: (25, 19)
rpn: gt_boxes.shape (1, 5)
rpn: gt_boxes [[  34.92063522   69.84127045  196.03175354  224.60317993   12.        ]]
total_a